<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>How a Large Language Model (LLM) Works | </title><meta name=keywords content="llm,machine-learning,ai,transformer,nlp,dev"><meta name=description content="A clear overview of how LLMs work: Transformer architecture, data processing, training, alignment (SFT, RLHF, RLAIF), and efficiency techniques like MoE, RAG, and FlashAttention."><meta name=author content="Digenaldo Neto"><link rel=canonical href=https://digenaldo.com/posts/how-a-large-language-model-llm-works/><link crossorigin=anonymous href=/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn+yY=" rel="preload stylesheet" as=style><link rel=icon href=https://digenaldo.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://digenaldo.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://digenaldo.com/favicon-32x32.png><link rel=apple-touch-icon href=https://digenaldo.com/apple-touch-icon.png><link rel=mask-icon href=https://digenaldo.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://digenaldo.com/posts/how-a-large-language-model-llm-works/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://digenaldo.com/posts/how-a-large-language-model-llm-works/"><meta property="og:title" content="How a Large Language Model (LLM) Works"><meta property="og:description" content="A clear overview of how LLMs work: Transformer architecture, data processing, training, alignment (SFT, RLHF, RLAIF), and efficiency techniques like MoE, RAG, and FlashAttention."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-12T00:00:00+00:00"><meta property="article:modified_time" content="2026-02-12T00:00:00+00:00"><meta property="article:tag" content="Llm"><meta property="article:tag" content="Machine-Learning"><meta property="article:tag" content="Ai"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="Nlp"><meta property="article:tag" content="Dev"><meta name=twitter:card content="summary"><meta name=twitter:title content="How a Large Language Model (LLM) Works"><meta name=twitter:description content="A clear overview of how LLMs work: Transformer architecture, data processing, training, alignment (SFT, RLHF, RLAIF), and efficiency techniques like MoE, RAG, and FlashAttention."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://digenaldo.com/posts/"},{"@type":"ListItem","position":2,"name":"How a Large Language Model (LLM) Works","item":"https://digenaldo.com/posts/how-a-large-language-model-llm-works/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"How a Large Language Model (LLM) Works","name":"How a Large Language Model (LLM) Works","description":"A clear overview of how LLMs work: Transformer architecture, data processing, training, alignment (SFT, RLHF, RLAIF), and efficiency techniques like MoE, RAG, and FlashAttention.","keywords":["llm","machine-learning","ai","transformer","nlp","dev"],"articleBody":"1. Fundamental Architecture: The Transformer The foundation of modern LLMs (Large Language Models) is the Transformer architecture [1].\nUnlike RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term Memory networks), which process text one step after another, the Transformer processes the entire sequence in parallel. This allows better modeling of long-range dependencies and faster training [1].\nFigure: Flow of the Transformer architecture (attention, encoder/decoder, feed-forward). Source: Vaswani et al. [1].\nHow it works in simple terms. The diagram above shows the path the data follows:\nThe input text is turned into vectors (embeddings). Those vectors enter a stack of layers. In each layer, three things happen in order: Self-attention: looks at all the tokens together and decides how much each word should “attend to” the others, so the model captures relationships in the sentence. Encoder (if present) or decoder: processes this attended representation. Feed-forward: refines each token position on its own. The output of one layer becomes the input of the next. After several such layers, the model has a rich representation of the whole sequence and uses it to predict the next token. In short: input → attention (who relates to whom) → encoder/decoder → feed-forward → repeat over layers → final representation for prediction.\nMain components:\nSelf-attention mechanism. Self-attention is the core of the Transformer [1]. It lets the model assign different importance weights to words in a sequence when making predictions. For example, in “The cat sat on the mat because it was tired,” the model can learn that “it” refers to “the cat.” In mathematical form, attention is:\nAttention(Q, K, V) = softmax(QKᵀ / √d_k) V\nA more detailed mathematical explanation is given in [2].\nEncoder and decoder. The original Transformer has both encoder and decoder stacks [1]. The encoder processes the input; the decoder produces the output. Many generative models, such as GPT (Generative Pre-trained Transformer), use only the decoder [3].\nFeed-forward networks. After the attention block, each token is processed by a position-wise fully connected feed-forward network [1].\n2. How the Model Processes Information Before the model can compute anything, text must be turned into numbers.\nFigure: From user input to next-token prediction (tokens, embeddings, positional encoding, self-attention, context vector).\nHow it works in simple terms. The diagram above shows the path from the user’s question to the first word of the answer:\nUser input. The user types a question (e.g. “What are the symptoms of dengue fever?”). The text is split into input tokens: each word or punctuation becomes a token, e.g. [What] [are] [the] [symptoms] [of] [dengue] [fever] [?]. Token embeddings. Each token is turned into a list of numbers (a vector). For example [What] becomes a vector like [0.1, 0.5, …]. This is the step where words become a form the model can process [1]. Positional encoding. The model needs to know the order of the words. Positional encoding adds numbers that tell the model that “What” is the 1st word, “are” is the 2nd, and so on. So the model knows that “dengue fever” and “symptoms” are related in the right order [1]. Self-attention. Inside the self-attention layers, each token “looks at” the others to see how they relate. For this question, “symptoms” pays more attention to “dengue” and “fever” than to “what” or “are.” The model uses Query (Q), Key (K), and Value (V) to decide how much attention each word gives to the others. The result is a contextual vector representation: one representation that encodes the meaning of the whole question [1]. Context vector. The output of attention is a single high-dimensional vector that holds the model’s “understanding” of the question. This is the context representation used for the next step. Next token prediction. With the full context, the model predicts the next token. It assigns probabilities to many possible next words (e.g. high for [Common], [Symptoms], lower for others) and then picks one (e.g. [Common]) according to its sampling strategy. This is where the answer starts. The model always generates one token at a time [1]. Embeddings. Tokens are converted into high-dimensional vectors. In this space, words with similar meanings sit closer together [1].\nPositional encoding. The Transformer does not process tokens in order. So it needs extra information about the position of each token. Positional encodings add that order information [1].\nQueries, keys, and values (Q, K, V). For each token, the model builds three vectors: Query, Key, and Value [1]. The Query of one token is compared to the Keys of other tokens to get attention weights. The Values are then combined using these weights. This is how the model learns relationships between words.\n3. Training Phases Training turns a generic model into one that can predict text well. It happens in several phases.\nPre-training. Large language models are trained on very large text corpora using next-token prediction [3]. The goal is to model the probability of the sequence:\nP(x) = ∏ P(x_t | x_","wordCount":"2849","inLanguage":"en","datePublished":"2026-02-12T00:00:00Z","dateModified":"2026-02-12T00:00:00Z","author":{"@type":"Person","name":"Digenaldo Neto"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://digenaldo.com/posts/how-a-large-language-model-llm-works/"},"publisher":{"@type":"Organization","name":"","logo":{"@type":"ImageObject","url":"https://digenaldo.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://digenaldo.com/ title=Home><span>Home</span></a></li><li><a href=https://digenaldo.com/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://digenaldo.com/sobre/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://digenaldo.com/>Home</a>&nbsp;»&nbsp;<a href=https://digenaldo.com/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">How a Large Language Model (LLM) Works</h1><div class=post-description>A clear overview of how LLMs work: Transformer architecture, data processing, training, alignment (SFT, RLHF, RLAIF), and efficiency techniques like MoE, RAG, and FlashAttention.</div><div class=post-meta><span title='2026-02-12 00:00:00 +0000 UTC'>02/12/2026</span>&nbsp;·&nbsp;<span>14 min</span>&nbsp;·&nbsp;<span>Digenaldo Neto</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-fundamental-architecture-the-transformer aria-label="1. Fundamental Architecture: The Transformer">1. Fundamental Architecture: The Transformer</a></li><li><a href=#2-how-the-model-processes-information aria-label="2. How the Model Processes Information">2. How the Model Processes Information</a></li><li><a href=#3-training-phases aria-label="3. Training Phases">3. Training Phases</a></li><li><a href=#4-alignment-and-refinement aria-label="4. Alignment and Refinement">4. Alignment and Refinement</a></li><li><a href=#5-advanced-techniques-and-efficiency aria-label="5. Advanced Techniques and Efficiency">5. Advanced Techniques and Efficiency</a></li><li><a href=#6-what-happens-when-a-user-asks-a-question aria-label="6. What Happens When a User Asks a Question?">6. What Happens When a User Asks a Question?</a></li><li><a href=#final-consideration aria-label="Final Consideration">Final Consideration</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=1-fundamental-architecture-the-transformer>1. Fundamental Architecture: The Transformer<a hidden class=anchor aria-hidden=true href=#1-fundamental-architecture-the-transformer>#</a></h2><p>The foundation of modern LLMs (Large Language Models) is the <strong>Transformer</strong> architecture [1].</p><p>Unlike RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term Memory networks), which process text one step after another, the Transformer processes the <strong>entire sequence in parallel</strong>. This allows better modeling of long-range dependencies and faster training [1].</p><p><img alt="Transformer architecture flow" loading=lazy src=/images/llm-works-flow-topic1.png></p><p><em>Figure: Flow of the Transformer architecture (attention, encoder/decoder, feed-forward). Source: Vaswani et al. [1].</em></p><p><strong>How it works in simple terms.</strong> The diagram above shows the path the data follows:</p><ol><li>The input text is turned into vectors (embeddings).</li><li>Those vectors enter a stack of layers. In each layer, three things happen in order:<ul><li><strong>Self-attention:</strong> looks at all the tokens together and decides how much each word should “attend to” the others, so the model captures relationships in the sentence.</li><li><strong>Encoder</strong> (if present) or <strong>decoder:</strong> processes this attended representation.</li><li><strong>Feed-forward:</strong> refines each token position on its own.</li></ul></li><li>The output of one layer becomes the input of the next.</li><li>After several such layers, the model has a rich representation of the whole sequence and uses it to predict the next token.</li></ol><p>In short: input → attention (who relates to whom) → encoder/decoder → feed-forward → repeat over layers → final representation for prediction.</p><p><strong>Main components:</strong></p><p><strong>Self-attention mechanism.</strong> Self-attention is the core of the Transformer [1]. It lets the model assign different importance weights to words in a sequence when making predictions. For example, in “The cat sat on the mat because it was tired,” the model can learn that “it” refers to “the cat.” In mathematical form, attention is:</p><blockquote><p><strong>Attention(Q, K, V) = softmax(QKᵀ / √d_k) V</strong></p></blockquote><p>A more detailed mathematical explanation is given in [2].</p><p><strong>Encoder and decoder.</strong> The original Transformer has both encoder and decoder stacks [1]. The encoder processes the input; the decoder produces the output. Many generative models, such as GPT (Generative Pre-trained Transformer), use only the <strong>decoder</strong> [3].</p><p><strong>Feed-forward networks.</strong> After the attention block, each token is processed by a position-wise fully connected feed-forward network [1].</p><hr><h2 id=2-how-the-model-processes-information>2. How the Model Processes Information<a hidden class=anchor aria-hidden=true href=#2-how-the-model-processes-information>#</a></h2><p>Before the model can compute anything, text must be turned into numbers.</p><p><img alt="How the model processes information" loading=lazy src=/images/llm-works-flow-topic2.png></p><p><em>Figure: From user input to next-token prediction (tokens, embeddings, positional encoding, self-attention, context vector).</em></p><p><strong>How it works in simple terms.</strong> The diagram above shows the path from the user’s question to the first word of the answer:</p><ol><li><strong>User input.</strong> The user types a question (e.g. &ldquo;What are the symptoms of dengue fever?&rdquo;). The text is split into <strong>input tokens</strong>: each word or punctuation becomes a token, e.g. [What] [are] [the] [symptoms] [of] [dengue] [fever] [?].</li><li><strong>Token embeddings.</strong> Each token is turned into a list of numbers (a vector). For example [What] becomes a vector like [0.1, 0.5, &mldr;]. This is the step where words become a form the model can process [1].</li><li><strong>Positional encoding.</strong> The model needs to know the <strong>order</strong> of the words. Positional encoding adds numbers that tell the model that &ldquo;What&rdquo; is the 1st word, &ldquo;are&rdquo; is the 2nd, and so on. So the model knows that &ldquo;dengue fever&rdquo; and &ldquo;symptoms&rdquo; are related in the right order [1].</li><li><strong>Self-attention.</strong> Inside the self-attention layers, each token &ldquo;looks at&rdquo; the others to see how they relate. For this question, &ldquo;symptoms&rdquo; pays more attention to &ldquo;dengue&rdquo; and &ldquo;fever&rdquo; than to &ldquo;what&rdquo; or &ldquo;are.&rdquo; The model uses <strong>Query (Q), Key (K), and Value (V)</strong> to decide how much attention each word gives to the others. The result is a <strong>contextual vector representation</strong>: one representation that encodes the meaning of the whole question [1].</li><li><strong>Context vector.</strong> The output of attention is a single high-dimensional vector that holds the model’s &ldquo;understanding&rdquo; of the question. This is the <strong>context representation</strong> used for the next step.</li><li><strong>Next token prediction.</strong> With the full context, the model predicts the <strong>next token</strong>. It assigns probabilities to many possible next words (e.g. high for [Common], [Symptoms], lower for others) and then picks one (e.g. [Common]) according to its sampling strategy. This is where the answer starts. The model always generates <strong>one token at a time</strong> [1].</li></ol><p><strong>Embeddings.</strong> Tokens are converted into high-dimensional vectors. In this space, words with similar meanings sit closer together [1].</p><p><strong>Positional encoding.</strong> The Transformer does not process tokens in order. So it needs extra information about the <strong>position</strong> of each token. Positional encodings add that order information [1].</p><p><strong>Queries, keys, and values (Q, K, V).</strong> For each token, the model builds three vectors: <strong>Query</strong>, <strong>Key</strong>, and <strong>Value</strong> [1]. The Query of one token is compared to the Keys of other tokens to get attention weights. The Values are then combined using these weights. This is how the model learns relationships between words.</p><hr><h2 id=3-training-phases>3. Training Phases<a hidden class=anchor aria-hidden=true href=#3-training-phases>#</a></h2><p>Training turns a generic model into one that can predict text well. It happens in several phases.</p><p><strong>Pre-training.</strong> Large language models are trained on very large text corpora using <strong>next-token prediction</strong> [3]. The goal is to model the probability of the sequence:</p><blockquote><p><strong>P(x) = ∏ P(x_t | x_&lt;t)</strong></p></blockquote><p>So the model learns to predict each token given all previous tokens. From this, it learns grammar, structure, and factual associations. The training loss is usually cross-entropy.</p><p><img alt="Training phases of a Large Language Model" loading=lazy src=/images/llm-works-flow-topic3.png></p><p><em>Figure: Training phases from data to a trained model (dataset, pre-training, loss, scaling laws, in-context learning).</em></p><p><strong>How it works in simple terms.</strong> The diagram above shows how the model learns from data and gets better over time:</p><ol><li><strong>Large text dataset.</strong> Training starts with a huge amount of text: books, websites, articles, and code. This is the raw material the model will learn from [3].</li><li><strong>Pre-training.</strong> The model is trained to <strong>predict the next token</strong>. For example, given &ldquo;The cat sat on the mat&rdquo;, it learns to predict a likely next word (e.g. &ldquo;because&rdquo; or &ldquo;and&rdquo;). By doing this over and over on billions of tokens, it learns grammar, facts, and how words relate [3].</li><li><strong>Loss calculation.</strong> For each prediction, the model compares what it predicted with the real next word. When the prediction is wrong, a <strong>loss</strong> (error) is computed. The training process updates the model so that this loss <strong>decreases over time</strong>. The graph of loss vs. time goes down as the model improves [3].</li><li><strong>Scaling laws.</strong> Performance improves in a predictable way when we add <strong>more data</strong>, <strong>more parameters</strong> (bigger model), and <strong>more compute</strong> (more computing power). This follows a <strong>power-law</strong> trend: better performance comes from scaling these three factors [4].</li><li><strong>In-context learning.</strong> After pre-training, the model can learn new tasks <strong>from examples in the prompt</strong> without changing its weights. For instance, if you show &ldquo;Translate to French: Hello → Bonjour&rdquo;, the model can do more translations. No extra training step is required; the ability emerges from scale [3].</li><li><strong>Trained language model.</strong> At the end, the model is <strong>ready for inference</strong>. All the knowledge it learned is stored in the <strong>neural network weights</strong>. When a user asks a question, the model uses these weights to generate answers [3].</li></ol><p><strong>Scaling laws.</strong> Model performance improves in a predictable way when we increase parameters, dataset size, and compute [4]. This follows a <strong>power-law</strong> relationship. Recent work also studies what happens when data growth is limited (data-constrained regimes) [5].</p><p><strong>Few-shot and in-context learning.</strong> Large models such as GPT-3 can do few-shot and zero-shot learning <strong>without updating their weights</strong> [3]. By putting examples or instructions in the prompt, the model solves new tasks. This ability emerges from scale and training, not from extra task-specific fine-tuning.</p><hr><h2 id=4-alignment-and-refinement>4. Alignment and Refinement<a hidden class=anchor aria-hidden=true href=#4-alignment-and-refinement>#</a></h2><p>Raw pre-trained models can produce unsafe or unhelpful outputs. <strong>Alignment</strong> techniques are used to improve behavior.</p><p><img alt="Alignment and refinement in Large Language Models" loading=lazy src=/images/llm-works-flow-topic4.png></p><p><em>Figure: How models become safer and more helpful (human feedback, reward model, reinforcement learning).</em></p><p><strong>How it works in simple terms.</strong> The diagram above shows the main steps of alignment:</p><ol><li><strong>From the raw model to answers.</strong> The pre-trained model generates several answers to the same question. These answers are the starting point for human feedback [6].</li><li><strong>Human feedback.</strong> People (annotators) read the model&rsquo;s answers and <strong>rank</strong> them: for example, answer A is better than B, B is better than C. These <strong>ranked answers</strong> show what humans prefer: more helpful, safer, or more accurate responses [6].</li><li><strong>Reward Model.</strong> A separate model, the <strong>Reward Model</strong>, is trained on these human rankings. It learns to <strong>score</strong> each answer with a number (e.g. +0.9 for a good answer, lower for worse ones). So &ldquo;human preferences&rdquo; are turned into a score the computer can use. This model is trained from human rankings only; it does not generate text [6].</li><li><strong>Reinforcement learning (RLHF).</strong> The main LLM is then trained with <strong>reinforcement learning</strong> (often PPO, Proximal Policy Optimization) to <strong>maximize</strong> the score given by the Reward Model. When the LLM produces a good answer, the reward is high; when it produces a bad one, the reward is low. Over time, the LLM becomes more helpful and aligned with what humans want [6].</li><li><strong>Aligned model.</strong> After this process, the model is <strong>refined</strong>: it is more likely to give safe, useful answers and less likely to give harmful or unhelpful ones. Alignment does not change what the model &ldquo;knows&rdquo; from pre-training; it changes how it <strong>behaves</strong> when it answers [6].</li></ol><p><strong>SFT (Supervised Fine-Tuning).</strong> The model is fine-tuned on high-quality human-written examples that show the desired behavior [6].</p><p><strong>RLHF (Reinforcement Learning from Human Feedback).</strong> The process in [6] works as follows. Human annotators rank model outputs. A <strong>Reward Model</strong> is trained from these rankings. Then reinforcement learning (typically PPO, Proximal Policy Optimization) is used to train the LLM to maximize that reward. This makes the model more helpful and aligned with user expectations.</p><p><strong>Constitutional AI (RLAIF).</strong> Constitutional AI replaces part of human feedback with AI-generated feedback guided by explicit principles (a “constitution”) [7]. The model critiques and improves its own outputs using these rules. This is Reinforcement Learning from AI Feedback (RLAIF). It allows scalable alignment with less human supervision.</p><hr><h2 id=5-advanced-techniques-and-efficiency>5. Advanced Techniques and Efficiency<a hidden class=anchor aria-hidden=true href=#5-advanced-techniques-and-efficiency>#</a></h2><p>Beyond architecture, training, and alignment, modern LLMs rely on several techniques to scale and run efficiently.</p><p><img alt="Advanced techniques and efficiency in LLMs" loading=lazy src=/images/llm-works-flow-topic5.png></p><p><em>Figure: How modern models scale and improve performance (MoE, RAG, FlashAttention, hardware optimization).</em></p><p><strong>How it works in simple terms.</strong> The diagram above shows how scaling challenges are addressed by combining several techniques:</p><ol><li><strong>Scaling challenge.</strong> Large models need a lot of <strong>memory</strong> and <strong>compute</strong> (many GPUs and chips). So <strong>efficiency</strong> becomes very important: we need ways to make models faster and cheaper to run [8][10].</li><li><strong>Mixture of Experts (MoE).</strong> Instead of using the whole model for every token, only <strong>part of the model</strong> is active per token. A <strong>router</strong> sends each input token to a few <strong>experts</strong> (sub-networks); for example, only &ldquo;Expert 2&rdquo; might be active for one token. This is <strong>sparse computation</strong>: it allows models with <strong>trillions of parameters</strong> while keeping the cost per token manageable [8].</li><li><strong>Retrieval-Augmented Generation (RAG).</strong> The model can <strong>retrieve external documents</strong> before answering. The user query is turned into a vector; a <strong>vector search</strong> finds relevant documents; then the LLM generates an answer using both the query and those documents. This <strong>reduces hallucination</strong> and lets the system use up-to-date knowledge without retraining [9].</li><li><strong>FlashAttention.</strong> The attention mechanism uses a lot of GPU memory and compute. <strong>FlashAttention</strong> and similar methods <strong>optimize</strong> how attention runs on the GPU. They improve <strong>speed</strong> and <strong>reduce memory use</strong>, so larger models or longer sequences can run on the same hardware [10].</li><li><strong>Hardware optimization.</strong> Modern systems are built for <strong>distributed GPU</strong> setups: many chips work in parallel. <strong>Data parallel</strong> and <strong>model parallel</strong> ways of splitting the work are used. So <strong>system and hardware engineering</strong> are part of model performance, not only the algorithm [10].</li><li><strong>Modern LLM system.</strong> Together, <strong>architecture</strong> (e.g. MoE), <strong>retrieval</strong> (RAG), <strong>sparsity</strong>, and <strong>hardware optimization</strong> form a modern LLM system. <strong>Performance depends on both algorithms and infrastructure</strong> [8][9][10].</li></ol><p><strong>Mixture-of-Experts (MoE).</strong> In models such as Switch Transformers, only a <strong>subset of parameters</strong> is used per token [8]. This is called sparse computation. Models can scale to trillions of parameters while keeping computation per token manageable. The trade-off is more complex systems and the need for good load balancing.</p><p><strong>RAG (Retrieval-Augmented Generation).</strong> RAG combines a retrieval system with a language model to improve factual grounding [9]. The model retrieves relevant documents and conditions its generation on them. This can reduce hallucination and allow knowledge updates without retraining. It also introduces new risks, such as prompt injection and manipulation of retrieved data.</p><p><strong>FlashAttention and kernel optimization.</strong> Attention is very demanding in memory and compute. <strong>FlashAttention-2</strong> and similar methods optimize attention for modern hardware [10]. For example, implementations on NVIDIA Hopper use CUDA kernel fusion and hardware-aware optimization to get large speedups. Scaling LLMs is therefore not only an algorithmic challenge but also a systems and hardware engineering problem [10].</p><hr><h2 id=6-what-happens-when-a-user-asks-a-question>6. What Happens When a User Asks a Question?<a hidden class=anchor aria-hidden=true href=#6-what-happens-when-a-user-asks-a-question>#</a></h2><p>Putting everything together: what matters in practice is what happens at <strong>inference time</strong>, when a user sends a question. Take for example: <em>&ldquo;What are the symptoms of dengue fever?&rdquo;</em> The diagram below shows the path from that question to the answer; the paragraphs that follow explain each step in one coherent flow.</p><p><img alt="What happens when a user asks a question? LLM inference flow" loading=lazy src=/images/llm-works-flow-topic6.png></p><p><em>Figure: LLM inference flow step by step (tokenization, self-attention, next-token prediction, generation, knowledge source, RAG).</em></p><p><strong>Step 1: Tokenization.</strong> The question is turned into <strong>tokens</strong> (e.g. [What] [are] [the] [symptoms] [of] [dengue] [fever] [?]) and then into <strong>embedding vectors</strong>. At this stage the model does not search the internet or any database; it only converts text into numbers the network can process [1].</p><p><strong>Step 2: Self-attention and context.</strong> The embeddings go through the Transformer layers. In each layer, <strong>self-attention</strong> uses <strong>Query (Q), Key (K), and Value (V)</strong> to compute how tokens relate (e.g. &ldquo;symptoms&rdquo; to &ldquo;dengue&rdquo; and &ldquo;fever&rdquo;) and builds a single <strong>contextual representation</strong> of the question. That representation is a high-dimensional vector that encodes the meaning of the whole prompt. The model is not retrieving a stored paragraph; it is preparing to compute a probability distribution over possible next tokens [1].</p><p><strong>Step 3: Next-token prediction.</strong> The model predicts the most likely next token. It computes:</p><blockquote><p><strong>P(next token | previous tokens)</strong></p></blockquote><p>It may assign high probability to tokens such as [Common], [Symptoms], [Include], and then picks one according to its sampling strategy. <strong>The response starts here</strong> [1].</p><p><strong>Step 4: Token-by-token generation.</strong> After the first token is generated, the process repeats: each new token is added to the context (e.g. &ldquo;What are the symptoms of dengue fever? Common symptoms include&mldr;&rdquo;), the model updates the context, recomputes attention, and predicts the next token again. This continues until a stop condition. The model never produces the full answer in one go; it generates <strong>one token at a time</strong> [1].</p><p><strong>Step 5: Where does the information come from?</strong> The model does not &ldquo;look up&rdquo; dengue fever in a database. The knowledge comes from <strong>patterns learned during pre-training</strong> [3]. If the training data included medical texts and web content about dengue, the model learned statistical links between &ldquo;dengue fever,&rdquo; &ldquo;mosquito-borne disease,&rdquo; &ldquo;high fever,&rdquo; &ldquo;joint pain,&rdquo; &ldquo;rash,&rdquo; and similar phrases. At inference it recombines these patterns. It does not check facts or guarantee correctness; it predicts what is statistically likely to follow.</p><p><strong>Step 6: What changes with RAG?</strong> If the system uses <strong>RAG (Retrieval-Augmented Generation)</strong> [9], the flow changes. Before generation, the question is turned into an embedding, a <strong>vector search</strong> is run, and <strong>relevant documents</strong> (e.g. medical) are retrieved and added to the prompt. The model then generates text conditioned on both the question and those documents, so the system can use <strong>external knowledge</strong>. Without RAG, the model relies only on what was learned in training.</p><p><strong>Important clarification.</strong> An LLM does not store structured medical knowledge like a database. It stores <strong>distributed representations</strong> in its weights. When it answers about a disease, it is activating learned patterns across billions of parameters. This is <strong>pattern completion</strong>, not structured reasoning or medical diagnosis. That distinction matters for safety, hallucination, and reliability.</p><hr><h2 id=final-consideration>Final Consideration<a hidden class=anchor aria-hidden=true href=#final-consideration>#</a></h2><p>An LLM (Large Language Model) is a probabilistic system trained to predict tokens. Its capabilities come from:</p><ul><li>The Transformer architecture [1]</li><li>Massive scale (parameters, data, compute) and scaling laws [4], [5]</li><li>Alignment (SFT, RLHF, RLAIF) [6], [7]</li><li>Efficiency techniques (MoE, RAG, FlashAttention) [8], [9], [10]</li></ul><p>It does not have symbolic reasoning, persistent memory across sessions, or guaranteed correctness. Its behavior is statistical and learned from data.</p><hr><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Vaswani, A., et al. (2017). Attention Is All You Need. Advances in Neural Information Processing Systems (NIPS 30).</p><p>[2] Reflections. (2024, June 10). Mathematical details behind self-attention.</p><p>[3] Brown, T. B., et al. (2020). Language Models are Few-Shot Learners. NeurIPS 33, 1877–1901.</p><p>[4] Kaplan, J., et al. (2020). Scaling Laws for Neural Language Models. arXiv:2001.08361.</p><p>[5] Muennighoff, N., et al. (2025). Scaling Data-Constrained Language Models. Journal of Machine Learning Research, 26, 1–66.</p><p>[6] Ouyang, L., et al. (2022). Training language models to follow instructions with human feedback. NeurIPS 36.</p><p>[7] Bai, Y., et al. (2022). Constitutional AI: Harmlessness from AI Feedback. arXiv preprint.</p><p>[8] Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. Journal of Machine Learning Research, 23.</p><p>[9] Lewis, P., et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. NeurIPS 33.</p><p>[10] Bikshandi, G., & Shah, J. (2023). A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on NVIDIA Hopper Architecture using the CUTLASS Library. Colfax Research.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://digenaldo.com/tags/llm/>Llm</a></li><li><a href=https://digenaldo.com/tags/machine-learning/>Machine-Learning</a></li><li><a href=https://digenaldo.com/tags/ai/>Ai</a></li><li><a href=https://digenaldo.com/tags/transformer/>Transformer</a></li><li><a href=https://digenaldo.com/tags/nlp/>Nlp</a></li><li><a href=https://digenaldo.com/tags/dev/>Dev</a></li></ul><nav class=paginav><a class=prev href=https://digenaldo.com/posts/llm-security-scientific-taxonomy-attack-vectors-2021-2025/><span class=title>« Prev</span><br><span>LLM Security: A Scientific Taxonomy of Attack Vectors</span>
</a><a class=next href=https://digenaldo.com/posts/argusscan-ethical-pentest-automation-with-shodan/><span class=title>Next »</span><br><span>ArgusScan: Automating Ethical Pentest with Shodan API</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share How a Large Language Model (LLM) Works on x" href="https://x.com/intent/tweet/?text=How%20a%20Large%20Language%20Model%20%28LLM%29%20Works&amp;url=https%3a%2f%2fdigenaldo.com%2fposts%2fhow-a-large-language-model-llm-works%2f&amp;hashtags=llm%2cmachine-learning%2cai%2ctransformer%2cnlp%2cdev"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share How a Large Language Model (LLM) Works on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdigenaldo.com%2fposts%2fhow-a-large-language-model-llm-works%2f&amp;title=How%20a%20Large%20Language%20Model%20%28LLM%29%20Works&amp;summary=How%20a%20Large%20Language%20Model%20%28LLM%29%20Works&amp;source=https%3a%2f%2fdigenaldo.com%2fposts%2fhow-a-large-language-model-llm-works%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share How a Large Language Model (LLM) Works on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdigenaldo.com%2fposts%2fhow-a-large-language-model-llm-works%2f&title=How%20a%20Large%20Language%20Model%20%28LLM%29%20Works"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share How a Large Language Model (LLM) Works on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdigenaldo.com%2fposts%2fhow-a-large-language-model-llm-works%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share How a Large Language Model (LLM) Works on whatsapp" href="https://api.whatsapp.com/send?text=How%20a%20Large%20Language%20Model%20%28LLM%29%20Works%20-%20https%3a%2f%2fdigenaldo.com%2fposts%2fhow-a-large-language-model-llm-works%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share How a Large Language Model (LLM) Works on telegram" href="https://telegram.me/share/url?text=How%20a%20Large%20Language%20Model%20%28LLM%29%20Works&amp;url=https%3a%2f%2fdigenaldo.com%2fposts%2fhow-a-large-language-model-llm-works%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share How a Large Language Model (LLM) Works on ycombinator" href="https://news.ycombinator.com/submitlink?t=How%20a%20Large%20Language%20Model%20%28LLM%29%20Works&u=https%3a%2f%2fdigenaldo.com%2fposts%2fhow-a-large-language-model-llm-works%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://digenaldo.com/></a></span>·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>