[{"content":"ArgusScan: Automating Ethical Pentest with Shodan API GitHub Repository: https://github.com/digenaldo/argusscan\nIntroduction Security professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\nWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\nThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\nWhat is ArgusScan? ArgusScan is a professional pentest automation tool that uses Shodan API to search for vulnerable hosts and exposed services. It\u0026rsquo;s designed for security professionals who need to perform authorized security assessments quickly and generate professional reports.\nThe tool is built with Python 3.8+ and provides a rich command-line interface that makes it easy to search for specific services, filter results, and export data in multiple formats.\nKey Characteristics:\nProfessional CLI with colorful output using the Rich library Multiple export formats: Markdown, JSON, CSV Advanced filtering options Professional report generation following PTES/OWASP standards Rate limiting to respect API limits Integration with popular security tools Key Features Shodan API Integration ArgusScan fully integrates with Shodan API, allowing you to search for vulnerable hosts using Shodan search queries (also known as \u0026ldquo;dorks\u0026rdquo;). You can search by service, port, country, organization, and many other criteria.\nThe tool handles API authentication, rate limiting, and error handling automatically, so you can focus on your security assessment.\nProfessional CLI Interface The command-line interface uses the Rich library to provide colorful, organized output. This makes it easy to read results and understand what the tool is doing. The interface shows progress, results, and any errors clearly.\nMultiple Export Formats ArgusScan supports three export formats:\nMarkdown: Perfect for documentation and reports JSON: Ideal for integration with other tools and automation CSV: Great for spreadsheet analysis and data processing This flexibility allows you to use ArgusScan results in different workflows and integrate them with other security tools.\nAdvanced Filtering You can filter Shodan results by:\nCountry code (e.g., country:BR for Brazil) Port number (e.g., port:8080) Organization name And many other Shodan search parameters This helps you narrow down results to specific targets or regions, making your assessment more focused and efficient.\nProfessional Reports ArgusScan generates reports that follow industry standards like PTES (Penetration Testing Execution Standard) and OWASP methodologies. These reports are suitable for professional pentest engagements and can be included in your final deliverables.\nRate Limiting The tool respects Shodan API rate limits (1 request per second) automatically. This prevents API errors and ensures you stay within your plan\u0026rsquo;s limits, whether you\u0026rsquo;re using a free or paid Shodan account.\nTool Integration ArgusScan is designed to work with popular security tools:\nNuclei: Use results to run vulnerability scans OWASP ZAP: Import targets for automated security testing Metasploit: Use discovered services as targets for exploitation The JSON export format makes it easy to integrate with these and other tools.\nInstallation and Setup Installing ArgusScan is straightforward:\n# Clone the repository git clone https://github.com/digenaldo/argusscan.git cd argusscan # Install dependencies pip install -r requirements.txt # Run the tool python argus_scan.py --help You\u0026rsquo;ll need a Shodan API key, which you can get from Shodan.io. Free accounts have some limitations, but they work with ArgusScan for basic searches.\nUsage Examples Here are practical examples of how to use ArgusScan:\nSearch for Jenkins Instances python argus_scan.py \u0026#34;jenkins port:8080\u0026#34; --token YOUR_API_KEY This searches for Jenkins instances running on port 8080. Jenkins is often misconfigured and can be a security risk if exposed publicly.\nThe output shows detailed information about discovered hosts, including IP addresses, ports, organizations, products, and identified vulnerabilities (CVEs). This makes it easy to identify potential targets and understand their security posture.\nSearch Apache Servers in Brazil python argus_scan.py \u0026#34;apache country:BR\u0026#34; --token YOUR_API_KEY This finds Apache web servers located in Brazil. You can use country filters to focus on specific regions.\nExport Results to JSON python argus_scan.py \u0026#34;jenkins\u0026#34; --token YOUR_API_KEY --export json This exports results in JSON format, which you can use with other tools or scripts for further analysis.\nGenerate Professional Report python argus_scan.py \u0026#34;apache\u0026#34; --token YOUR_API_KEY --export markdown --report This generates a professional markdown report following PTES/OWASP standards, suitable for client deliverables.\nTechnical Excellence ArgusScan is built with quality in mind:\nComprehensive Testing:\n25 unit tests covering all major functionality 96% code coverage ensuring reliability Automated testing with GitHub Actions CI/CD pipeline Code Quality:\nPre-commit hooks for quality assurance Modern Python 3.8+ implementation Clean, maintainable code structure Professional Development Practices:\nVersion control with Git Issue tracking and pull request workflow Documentation and examples This level of quality ensures the tool is reliable and suitable for professional use.\nIntegration with Pentest Workflows ArgusScan fits naturally into professional pentest workflows:\nReconnaissance Phase: Use ArgusScan to identify exposed services and potential targets before starting detailed testing.\nTool Integration: Export results to JSON and import them into Nuclei, OWASP ZAP, or Metasploit for automated vulnerability scanning and exploitation.\nReporting: Generate professional reports that can be included in your final pentest deliverables, following PTES and OWASP standards.\nDocumentation: Use Markdown exports to document findings and create detailed reports for clients.\nLegal and Ethical Considerations This is extremely important: ArgusScan should only be used with proper authorization.\nMandatory Requirements:\nWritten authorization (Rules of Engagement - RoE) before any scanning Only scan systems you own or have explicit permission to test Follow responsible disclosure practices Comply with all applicable laws and regulations Authorized Use Cases:\nBug bounty programs with written authorization Professional pentest engagements with signed RoE Academic security research with proper approvals Defensive security assessments of your own infrastructure Red team exercises with proper authorization Unauthorized Use:\nNever scan systems without permission Never use for malicious purposes Never violate terms of service Never break laws or regulations Security tools are powerful, and with that power comes responsibility. Always use ArgusScan ethically and legally.\nUse Cases ArgusScan is useful in several scenarios:\nAuthorized Bug Bounty Programs: When you have written authorization to test specific targets, ArgusScan helps you find exposed services quickly.\nProfessional Pentest Engagements: Use it during the reconnaissance phase to identify potential targets and attack surfaces before detailed testing.\nAcademic Security Research: Researchers can use it to study internet exposure and security trends, always with proper authorization.\nDefensive Security Assessments: Organizations can use it to assess their own infrastructure and identify misconfigurations or exposed services.\nRed Team Exercises: Red teams can use it to find targets within authorized scope during exercises.\nFuture Roadmap The project is actively maintained, and future enhancements may include:\nSupport for additional export formats Integration with more security tools Enhanced filtering options Web interface for easier use Additional report templates Performance improvements Contributions and feedback are welcome on the GitHub repository.\nConclusion ArgusScan is a powerful tool for ethical security professionals who need to automate reconnaissance during authorized pentest engagements. With its professional CLI, multiple export formats, and high code quality, it\u0026rsquo;s designed to make security assessments more efficient while maintaining professional standards.\nThe tool demonstrates that open-source security tools can be both powerful and well-built, with comprehensive testing and professional development practices.\nKey Takeaways:\nAutomates Shodan searches for pentest reconnaissance Professional reports following PTES/OWASP standards 96% test coverage and CI/CD pipeline Integrates with popular security tools Always use with proper authorization If you\u0026rsquo;re a security professional looking to improve your pentest workflow, consider trying ArgusScan. Star the repository on GitHub, try it out (with proper authorization), and contribute improvements if you find it useful.\nRemember: Security tools are powerful, but they must be used responsibly and ethically. Always get written authorization before scanning any systems.\nGet Started:\nGitHub Repository: https://github.com/digenaldo/argusscan Read the documentation Try it with your Shodan API key Contribute and share feedback Use responsibly, and happy ethical hacking!\n","permalink":"https://digenaldo.com/posts/argusscan-ethical-pentest-automation-with-shodan/","summary":"\u003ch1 id=\"argusscan-automating-ethical-pentest-with-shodan-api\"\u003eArgusScan: Automating Ethical Pentest with Shodan API\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eGitHub Repository:\u003c/strong\u003e \u003ca href=\"https://github.com/digenaldo/argusscan\"\u003ehttps://github.com/digenaldo/argusscan\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\u003c/p\u003e\n\u003cp\u003eWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\u003c/p\u003e\n\u003cp\u003eThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\u003c/p\u003e","title":"ArgusScan: Automating Ethical Pentest with Shodan API"},{"content":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus Introduction Observability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\nI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\nThis article focuses on the actual results and what the metrics tell us about the performance differences between these two technologies.\nThe Setup The Monitoring Lab includes two backend applications running the same workload:\nA Go application that performs MongoDB operations every 5 seconds A Spring Boot application performing the same operations at the same interval Prometheus collecting metrics from both applications Grafana dashboards showing real-time performance data Both applications execute similar operations: they ping MongoDB, insert documents, and record metrics about their performance. The key difference is the technology stack each one uses.\nPerformance Metrics Analysis Go Application Performance The Go application shows consistent and efficient performance across all metrics:\nCPU Usage: The Go app maintains very low CPU usage at around 0.22%. This shows the application is lightweight and doesn\u0026rsquo;t require much processing power for these operations.\nMemory Usage: Memory consumption stays stable at approximately 21.6 MiB. This is a small amount of memory, showing that Go applications can be very memory-efficient.\nMongoDB Latency: The latency metrics show excellent performance:\nAverage latency stays between 0.5 ms and 1.2 ms P95 latency (95th percentile) remains consistently around 4.5 ms P99 latency (99th percentile) stays around 5 ms These numbers mean that most operations complete very quickly, and even the slowest operations are still fast.\nOperations Rate: The application maintains a steady rate of 0.3 operations per second, which matches the expected interval of one operation every 5 seconds.\nSpring Boot Application Performance The Spring Boot application shows different characteristics:\nCPU Usage: Similar to Go, CPU usage is low at around 0.28%. Both applications handle the workload without stressing the CPU.\nMemory Usage: This is where we see a significant difference. The Spring Boot application uses 112 MiB of memory, which is more than 5 times the memory used by the Go application. This higher memory usage is typical for Java applications due to the JVM overhead.\nMongoDB Latency: The latency metrics show more variation:\nAverage latency fluctuates between 10 ms and 15 ms P95 latency follows similar patterns to the average P99 latency ranges from 15 ms to 25 ms, with spikes reaching nearly 30 ms The latency is consistently higher than the Go application, which suggests the Spring Boot application takes longer to complete the same operations.\nOperations Rate: The application maintains 0.4 operations per second, slightly higher than the Go application.\nJVM Heap Memory: The dashboard shows how the Java Virtual Machine manages memory:\nG1 Eden Space fills up and gets cleared during garbage collection cycles G1 Old Gen gradually increases over time as objects are promoted G1 Survivor Space remains minimal This memory management pattern is normal for Java applications but adds overhead that Go applications don\u0026rsquo;t have.\nDirect Comparison The comparison dashboard makes the differences clear:\nLatency Comparison:\nGo maintains average latency below 2.5 ms throughout the entire period Spring Boot shows average latency ranging from 6 ms to 15 ms Go\u0026rsquo;s latency is consistently 3 to 6 times lower than Spring Boot\u0026rsquo;s Memory Comparison:\nGo uses between 16 MiB and 24 MiB consistently Spring Boot starts at 112 MiB and fluctuates between 80 MiB and 104 MiB Spring Boot uses approximately 4 to 7 times more memory than Go These results show that for this specific workload, the Go application performs better in terms of both latency and memory usage.\nKey Metrics Explained Understanding these metrics helps you make informed decisions about which technology to use:\nCPU Usage: Shows how much processing power the application needs. Lower is better, as it leaves more resources for other processes.\nMemory Usage: Indicates how much RAM the application consumes. Lower memory usage means you can run more instances on the same server.\nAverage Latency: The typical time it takes for an operation to complete. Lower latency means faster responses to users.\nP95 and P99 Latency: These percentiles show how the slowest operations perform. P95 means 95% of operations are faster than this value. P99 means 99% of operations are faster. These metrics help you understand worst-case performance.\nOperations Rate: How many operations the application completes per second. This helps you understand throughput.\nWhat These Results Mean The metrics show clear patterns:\nGo excels at low latency: The Go application consistently completes operations faster, with latency staying under 2.5 ms.\nGo uses less memory: With memory usage around 21 MiB, Go applications can run more instances on the same hardware.\nSpring Boot has higher overhead: The JVM and framework layers add overhead, resulting in higher latency and memory usage.\nBoth handle the workload: Neither application struggles with the workload, showing that both technologies can handle this type of operation effectively.\nIt\u0026rsquo;s important to remember that these results are specific to this test scenario. Different workloads, different operations, or different configurations might produce different results.\nReal-World Implications These metrics matter in production environments:\nLower latency means users get faster responses, which improves user experience. For applications handling many requests, even small latency differences can add up significantly.\nLower memory usage means you can run more application instances on the same server, which can reduce infrastructure costs. It also means applications start faster and use fewer resources.\nConsistent performance is important for reliability. When latency varies widely, it\u0026rsquo;s harder to predict how the application will behave under load.\nConclusion The Monitoring Lab provides real data about how Go and Spring Boot applications perform. The results show that for this specific workload:\nGo offers better latency performance and lower memory usage Spring Boot provides easier development through frameworks and tools, but with higher resource usage Both technologies can handle the workload effectively The choice between technologies depends on your specific needs. If low latency and low memory usage are priorities, Go might be a better fit. If rapid development and a rich ecosystem are more important, Spring Boot might be the right choice.\nObservability tools like Prometheus and Grafana make it possible to see these differences clearly. Without metrics, you\u0026rsquo;re making decisions based on assumptions. With metrics, you can make decisions based on data.\nThe code and dashboards are available on GitHub: https://github.com/digenaldo/monitoring-lab\n","permalink":"https://digenaldo.com/posts/monitoring-lab-complete-observability-with-go-spring-boot-prometheus/","summary":"\u003ch1 id=\"monitoring-lab-a-complete-observability-lab-with-go-spring-boot-and-prometheus\"\u003eMonitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eObservability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\u003c/p\u003e\n\u003cp\u003eI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\u003c/p\u003e","title":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus"}]