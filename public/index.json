[{"content":"ArgusScan: Automating Ethical Pentest with Shodan API GitHub Repository: https://github.com/digenaldo/argusscan\nIntroduction Security professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\nWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\nThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\nWhat is ArgusScan? ArgusScan is a professional pentest automation tool that uses Shodan API to search for vulnerable hosts and exposed services. It\u0026rsquo;s designed for security professionals who need to perform authorized security assessments quickly and generate professional reports.\nThe tool is built with Python 3.8+ and provides a rich command-line interface that makes it easy to search for specific services, filter results, and export data in multiple formats.\nKey Characteristics:\nProfessional CLI with colorful output using the Rich library Multiple export formats: Markdown, JSON, CSV Advanced filtering options Professional report generation following PTES/OWASP standards Rate limiting to respect API limits Integration with popular security tools Key Features Shodan API Integration ArgusScan fully integrates with Shodan API, allowing you to search for vulnerable hosts using Shodan search queries (also known as \u0026ldquo;dorks\u0026rdquo;). You can search by service, port, country, organization, and many other criteria.\nThe tool handles API authentication, rate limiting, and error handling automatically, so you can focus on your security assessment.\nProfessional CLI Interface The command-line interface uses the Rich library to provide colorful, organized output. This makes it easy to read results and understand what the tool is doing. The interface shows progress, results, and any errors clearly.\nMultiple Export Formats ArgusScan supports three export formats:\nMarkdown: Perfect for documentation and reports JSON: Ideal for integration with other tools and automation CSV: Great for spreadsheet analysis and data processing This flexibility allows you to use ArgusScan results in different workflows and integrate them with other security tools.\nAdvanced Filtering You can filter Shodan results by:\nCountry code (e.g., country:BR for Brazil) Port number (e.g., port:8080) Organization name And many other Shodan search parameters This helps you narrow down results to specific targets or regions, making your assessment more focused and efficient.\nProfessional Reports ArgusScan generates reports that follow industry standards like PTES (Penetration Testing Execution Standard) and OWASP methodologies. These reports are suitable for professional pentest engagements and can be included in your final deliverables.\nRate Limiting The tool respects Shodan API rate limits (1 request per second) automatically. This prevents API errors and ensures you stay within your plan\u0026rsquo;s limits, whether you\u0026rsquo;re using a free or paid Shodan account.\nTool Integration ArgusScan is designed to work with popular security tools:\nNuclei: Use results to run vulnerability scans OWASP ZAP: Import targets for automated security testing Metasploit: Use discovered services as targets for exploitation The JSON export format makes it easy to integrate with these and other tools.\nInstallation and Setup Installing ArgusScan is straightforward:\n# Clone the repository git clone https://github.com/digenaldo/argusscan.git cd argusscan # Install dependencies pip install -r requirements.txt # Run the tool python argus_scan.py --help You\u0026rsquo;ll need a Shodan API key, which you can get from Shodan.io. Free accounts have some limitations, but they work with ArgusScan for basic searches.\nUsage Examples Here are practical examples of how to use ArgusScan:\nSearch for Jenkins Instances python argus_scan.py \u0026#34;jenkins port:8080\u0026#34; --token YOUR_API_KEY This searches for Jenkins instances running on port 8080. Jenkins is often misconfigured and can be a security risk if exposed publicly.\nThe output shows detailed information about discovered hosts, including IP addresses, ports, organizations, products, and identified vulnerabilities (CVEs). This makes it easy to identify potential targets and understand their security posture.\nSearch Apache Servers in Brazil python argus_scan.py \u0026#34;apache country:BR\u0026#34; --token YOUR_API_KEY This finds Apache web servers located in Brazil. You can use country filters to focus on specific regions.\nExport Results to JSON python argus_scan.py \u0026#34;jenkins\u0026#34; --token YOUR_API_KEY --export json This exports results in JSON format, which you can use with other tools or scripts for further analysis.\nGenerate Professional Report python argus_scan.py \u0026#34;apache\u0026#34; --token YOUR_API_KEY --export markdown --report This generates a professional markdown report following PTES/OWASP standards, suitable for client deliverables.\nTechnical Excellence ArgusScan is built with quality in mind:\nComprehensive Testing:\n25 unit tests covering all major functionality 96% code coverage ensuring reliability Automated testing with GitHub Actions CI/CD pipeline Code Quality:\nPre-commit hooks for quality assurance Modern Python 3.8+ implementation Clean, maintainable code structure Professional Development Practices:\nVersion control with Git Issue tracking and pull request workflow Documentation and examples This level of quality ensures the tool is reliable and suitable for professional use.\nIntegration with Pentest Workflows ArgusScan fits naturally into professional pentest workflows:\nReconnaissance Phase: Use ArgusScan to identify exposed services and potential targets before starting detailed testing.\nTool Integration: Export results to JSON and import them into Nuclei, OWASP ZAP, or Metasploit for automated vulnerability scanning and exploitation.\nReporting: Generate professional reports that can be included in your final pentest deliverables, following PTES and OWASP standards.\nDocumentation: Use Markdown exports to document findings and create detailed reports for clients.\nLegal and Ethical Considerations This is extremely important: ArgusScan should only be used with proper authorization.\nMandatory Requirements:\nWritten authorization (Rules of Engagement - RoE) before any scanning Only scan systems you own or have explicit permission to test Follow responsible disclosure practices Comply with all applicable laws and regulations Authorized Use Cases:\nBug bounty programs with written authorization Professional pentest engagements with signed RoE Academic security research with proper approvals Defensive security assessments of your own infrastructure Red team exercises with proper authorization Unauthorized Use:\nNever scan systems without permission Never use for malicious purposes Never violate terms of service Never break laws or regulations Security tools are powerful, and with that power comes responsibility. Always use ArgusScan ethically and legally.\nUse Cases ArgusScan is useful in several scenarios:\nAuthorized Bug Bounty Programs: When you have written authorization to test specific targets, ArgusScan helps you find exposed services quickly.\nProfessional Pentest Engagements: Use it during the reconnaissance phase to identify potential targets and attack surfaces before detailed testing.\nAcademic Security Research: Researchers can use it to study internet exposure and security trends, always with proper authorization.\nDefensive Security Assessments: Organizations can use it to assess their own infrastructure and identify misconfigurations or exposed services.\nRed Team Exercises: Red teams can use it to find targets within authorized scope during exercises.\nFuture Roadmap The project is actively maintained, and future enhancements may include:\nSupport for additional export formats Integration with more security tools Enhanced filtering options Web interface for easier use Additional report templates Performance improvements Contributions and feedback are welcome on the GitHub repository.\nConclusion ArgusScan is a powerful tool for ethical security professionals who need to automate reconnaissance during authorized pentest engagements. With its professional CLI, multiple export formats, and high code quality, it\u0026rsquo;s designed to make security assessments more efficient while maintaining professional standards.\nThe tool demonstrates that open-source security tools can be both powerful and well-built, with comprehensive testing and professional development practices.\nKey Takeaways:\nAutomates Shodan searches for pentest reconnaissance Professional reports following PTES/OWASP standards 96% test coverage and CI/CD pipeline Integrates with popular security tools Always use with proper authorization If you\u0026rsquo;re a security professional looking to improve your pentest workflow, consider trying ArgusScan. Star the repository on GitHub, try it out (with proper authorization), and contribute improvements if you find it useful.\nRemember: Security tools are powerful, but they must be used responsibly and ethically. Always get written authorization before scanning any systems.\nGet Started:\nGitHub Repository: https://github.com/digenaldo/argusscan Read the documentation Try it with your Shodan API key Contribute and share feedback Use responsibly, and happy ethical hacking!\n","permalink":"http://localhost:1313/posts/argusscan-ethical-pentest-automation-with-shodan/","summary":"\u003ch1 id=\"argusscan-automating-ethical-pentest-with-shodan-api\"\u003eArgusScan: Automating Ethical Pentest with Shodan API\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eGitHub Repository:\u003c/strong\u003e \u003ca href=\"https://github.com/digenaldo/argusscan\"\u003ehttps://github.com/digenaldo/argusscan\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\u003c/p\u003e\n\u003cp\u003eWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\u003c/p\u003e\n\u003cp\u003eThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\u003c/p\u003e","title":"ArgusScan: Automating Ethical Pentest with Shodan API"},{"content":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus Introduction Observability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\nI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\nThis article focuses on the actual results and what the metrics tell us about the performance differences between these two technologies.\nThe Setup The Monitoring Lab includes two backend applications running the same workload:\nA Go application that performs MongoDB operations every 5 seconds A Spring Boot application performing the same operations at the same interval Prometheus collecting metrics from both applications Grafana dashboards showing real-time performance data Both applications execute similar operations: they ping MongoDB, insert documents, and record metrics about their performance. The key difference is the technology stack each one uses.\nPerformance Metrics Analysis Go Application Performance The Go application shows consistent and efficient performance across all metrics:\nCPU Usage: The Go app maintains very low CPU usage at around 0.22%. This shows the application is lightweight and doesn\u0026rsquo;t require much processing power for these operations.\nMemory Usage: Memory consumption stays stable at approximately 21.6 MiB. This is a small amount of memory, showing that Go applications can be very memory-efficient.\nMongoDB Latency: The latency metrics show excellent performance:\nAverage latency stays between 0.5 ms and 1.2 ms P95 latency (95th percentile) remains consistently around 4.5 ms P99 latency (99th percentile) stays around 5 ms These numbers mean that most operations complete very quickly, and even the slowest operations are still fast.\nOperations Rate: The application maintains a steady rate of 0.3 operations per second, which matches the expected interval of one operation every 5 seconds.\nSpring Boot Application Performance The Spring Boot application shows different characteristics:\nCPU Usage: Similar to Go, CPU usage is low at around 0.28%. Both applications handle the workload without stressing the CPU.\nMemory Usage: This is where we see a significant difference. The Spring Boot application uses 112 MiB of memory, which is more than 5 times the memory used by the Go application. This higher memory usage is typical for Java applications due to the JVM overhead.\nMongoDB Latency: The latency metrics show more variation:\nAverage latency fluctuates between 10 ms and 15 ms P95 latency follows similar patterns to the average P99 latency ranges from 15 ms to 25 ms, with spikes reaching nearly 30 ms The latency is consistently higher than the Go application, which suggests the Spring Boot application takes longer to complete the same operations.\nOperations Rate: The application maintains 0.4 operations per second, slightly higher than the Go application.\nJVM Heap Memory: The dashboard shows how the Java Virtual Machine manages memory:\nG1 Eden Space fills up and gets cleared during garbage collection cycles G1 Old Gen gradually increases over time as objects are promoted G1 Survivor Space remains minimal This memory management pattern is normal for Java applications but adds overhead that Go applications don\u0026rsquo;t have.\nDirect Comparison The comparison dashboard makes the differences clear:\nLatency Comparison:\nGo maintains average latency below 2.5 ms throughout the entire period Spring Boot shows average latency ranging from 6 ms to 15 ms Go\u0026rsquo;s latency is consistently 3 to 6 times lower than Spring Boot\u0026rsquo;s Memory Comparison:\nGo uses between 16 MiB and 24 MiB consistently Spring Boot starts at 112 MiB and fluctuates between 80 MiB and 104 MiB Spring Boot uses approximately 4 to 7 times more memory than Go These results show that for this specific workload, the Go application performs better in terms of both latency and memory usage.\nKey Metrics Explained Understanding these metrics helps you make informed decisions about which technology to use:\nCPU Usage: Shows how much processing power the application needs. Lower is better, as it leaves more resources for other processes.\nMemory Usage: Indicates how much RAM the application consumes. Lower memory usage means you can run more instances on the same server.\nAverage Latency: The typical time it takes for an operation to complete. Lower latency means faster responses to users.\nP95 and P99 Latency: These percentiles show how the slowest operations perform. P95 means 95% of operations are faster than this value. P99 means 99% of operations are faster. These metrics help you understand worst-case performance.\nOperations Rate: How many operations the application completes per second. This helps you understand throughput.\nWhat These Results Mean The metrics show clear patterns:\nGo excels at low latency: The Go application consistently completes operations faster, with latency staying under 2.5 ms.\nGo uses less memory: With memory usage around 21 MiB, Go applications can run more instances on the same hardware.\nSpring Boot has higher overhead: The JVM and framework layers add overhead, resulting in higher latency and memory usage.\nBoth handle the workload: Neither application struggles with the workload, showing that both technologies can handle this type of operation effectively.\nIt\u0026rsquo;s important to remember that these results are specific to this test scenario. Different workloads, different operations, or different configurations might produce different results.\nReal-World Implications These metrics matter in production environments:\nLower latency means users get faster responses, which improves user experience. For applications handling many requests, even small latency differences can add up significantly.\nLower memory usage means you can run more application instances on the same server, which can reduce infrastructure costs. It also means applications start faster and use fewer resources.\nConsistent performance is important for reliability. When latency varies widely, it\u0026rsquo;s harder to predict how the application will behave under load.\nConclusion The Monitoring Lab provides real data about how Go and Spring Boot applications perform. The results show that for this specific workload:\nGo offers better latency performance and lower memory usage Spring Boot provides easier development through frameworks and tools, but with higher resource usage Both technologies can handle the workload effectively The choice between technologies depends on your specific needs. If low latency and low memory usage are priorities, Go might be a better fit. If rapid development and a rich ecosystem are more important, Spring Boot might be the right choice.\nObservability tools like Prometheus and Grafana make it possible to see these differences clearly. Without metrics, you\u0026rsquo;re making decisions based on assumptions. With metrics, you can make decisions based on data.\nThe code and dashboards are available on GitHub: https://github.com/digenaldo/monitoring-lab\n","permalink":"http://localhost:1313/posts/monitoring-lab-complete-observability-with-go-spring-boot-prometheus/","summary":"\u003ch1 id=\"monitoring-lab-a-complete-observability-lab-with-go-spring-boot-and-prometheus\"\u003eMonitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eObservability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\u003c/p\u003e\n\u003cp\u003eI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\u003c/p\u003e","title":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus"},{"content":"LLM Security: A Scientific Taxonomy of Attack Vectors Introduction Security in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\nThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\nThis post organizes the main scientific results from this period into a technical taxonomy. It is based on how attacks work, not on how the media talks about them.\n1. Adversarial Attacks and Transferable Jailbreaks Many people think of jailbreaks as just clever prompt writing. Research has shown there is more to it.\nStudies showed that adversarial suffixes can be generated automatically by gradient optimization. These suffixes can make aligned models break safety rules [1]. They push the model into parts of its internal space where forbidden answers become more likely.\nThe most important finding is transferability. Attacks that were optimized for one open-source model often worked on proprietary models trained with similar pipelines [1].\nThis is not only because of the Transformer architecture. Transfer happens because training goals and alignment methods (especially RLHF) are similar. It is a systemic weakness in models that are aligned in similar ways.\nPractical takeaway: Defenses that only use word filters or fixed rules are weak against optimization-based attacks.\n2. Indirect Prompt Injection and Context Contamination With the rise of RAG and autonomous agents, the attack surface has changed.\nIndirect Prompt Injection was formalized as a real attack vector against applications that use LLMs [2]. The core issue is that the model does not clearly separate “trusted instructions” from “external retrieved data.” Both are just tokens in the same context.\nThis is an instance of the classic “confused deputy” problem applied to generative models.\nThe impact depends on the setup:\nStandalone LLM: The effect is usually limited to changing the text output. LLM with tools: Data can be exfiltrated through API calls. Agents with broad credentials: There is risk of unauthorized actions in external systems. The real risk is not only wrong content, but privilege escalation in distributed systems.\n3. Training Data Extraction and Memorization The question of memorization became concrete when it was shown that large models can reproduce rare sequences that appeared in their training data [3].\nIt helps to separate three types of attacks that are often mixed up:\nMembership inference: Estimates whether a specific piece of data was in the training set. Training data extraction: Directly extracts memorized sequences through adaptive generation. Memorization elicitation: Makes the model reveal rare passages through statistical probing. The study showed that large-scale extraction is possible, not only probabilistic inference [3]. This conflicts with the idea of “privacy by design,” especially for models trained on scraped web data.\n4. Behavioral Backdoors and Sleeper Agents Some hoped that safety alignment would remove all malicious behavior. Experiments tested this.\nIt was shown that models can keep behavioral backdoors even after safety fine-tuning with SFT and RLHF [4]. The model learns to trigger bad behavior only under a specific condition and to look safe during evaluation.\nThese experiments were done in controlled settings. There is no agreement that the same level of persistence happens in real industrial pipelines with extensive red teaming and other alignment techniques.\nStill, the result matters: alignment does not formally guarantee that hidden behavior is removed.\n5. Supply Chain and Tool Abuse Recent work suggests that the main risk may not be the model alone, but how it is integrated into larger systems [5].\nTwo vectors are especially important:\nModel poisoning and compromised provenance: Especially when using third-party fine-tuning or unaudited checkpoints. Tool abuse: Agents with access to internal APIs, databases, or financial systems act under probabilistic control. In corporate environments, the potential impact of tool abuse often exceeds the impact of jailbreak alone.\nConclusion Research between 2021 and 2025 points to a few main ideas:\nJailbreak is a mathematical optimization problem. RAG introduces structural context contamination. LLMs can memorize sensitive data. Backdoors can survive alignment. Integration with tools greatly expands the attack surface. LLM security is not just an extension of traditional AppSec, and it cannot be solved only with prompt engineering. It is a challenge of architecture, data governance, and privilege control in integrated probabilistic systems.\nReferences [1] Zou, A., Wang, Z., Kolter, J. Z., \u0026amp; Fredrikson, M. (2023). Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043.\n[2] Greshake, K., et al. (2023). Not What You\u0026rsquo;ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security.\n[3] Carlini, N., et al. (2021). Extracting Training Data from Large Language Models. USENIX Security Symposium.\n[4] Hubinger, E., et al. (2024). Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. arXiv:2401.05566.\n[5] Yao, Y., et al. (2024). A Survey on Large Language Model Security and Privacy: The Good, the Bad, and the Ugly. High-Confidence Computing.\n","permalink":"http://localhost:1313/posts/llm-security-scientific-taxonomy-attack-vectors-2021-2025/","summary":"\u003ch1 id=\"llm-security-a-scientific-taxonomy-of-attack-vectors\"\u003eLLM Security: A Scientific Taxonomy of Attack Vectors\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\u003c/p\u003e\n\u003cp\u003eThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\u003c/p\u003e","title":"LLM Security: A Scientific Taxonomy of Attack Vectors"},{"content":"ArgusScan: Automating Ethical Pentest with Shodan API GitHub Repository: https://github.com/digenaldo/argusscan\nIntroduction Security professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\nWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\nThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\nWhat is ArgusScan? ArgusScan is a professional pentest automation tool that uses Shodan API to search for vulnerable hosts and exposed services. It\u0026rsquo;s designed for security professionals who need to perform authorized security assessments quickly and generate professional reports.\nThe tool is built with Python 3.8+ and provides a rich command-line interface that makes it easy to search for specific services, filter results, and export data in multiple formats.\nKey Characteristics:\nProfessional CLI with colorful output using the Rich library Multiple export formats: Markdown, JSON, CSV Advanced filtering options Professional report generation following PTES/OWASP standards Rate limiting to respect API limits Integration with popular security tools Key Features Shodan API Integration ArgusScan fully integrates with Shodan API, allowing you to search for vulnerable hosts using Shodan search queries (also known as \u0026ldquo;dorks\u0026rdquo;). You can search by service, port, country, organization, and many other criteria.\nThe tool handles API authentication, rate limiting, and error handling automatically, so you can focus on your security assessment.\nProfessional CLI Interface The command-line interface uses the Rich library to provide colorful, organized output. This makes it easy to read results and understand what the tool is doing. The interface shows progress, results, and any errors clearly.\nMultiple Export Formats ArgusScan supports three export formats:\nMarkdown: Perfect for documentation and reports JSON: Ideal for integration with other tools and automation CSV: Great for spreadsheet analysis and data processing This flexibility allows you to use ArgusScan results in different workflows and integrate them with other security tools.\nAdvanced Filtering You can filter Shodan results by:\nCountry code (e.g., country:BR for Brazil) Port number (e.g., port:8080) Organization name And many other Shodan search parameters This helps you narrow down results to specific targets or regions, making your assessment more focused and efficient.\nProfessional Reports ArgusScan generates reports that follow industry standards like PTES (Penetration Testing Execution Standard) and OWASP methodologies. These reports are suitable for professional pentest engagements and can be included in your final deliverables.\nRate Limiting The tool respects Shodan API rate limits (1 request per second) automatically. This prevents API errors and ensures you stay within your plan\u0026rsquo;s limits, whether you\u0026rsquo;re using a free or paid Shodan account.\nTool Integration ArgusScan is designed to work with popular security tools:\nNuclei: Use results to run vulnerability scans OWASP ZAP: Import targets for automated security testing Metasploit: Use discovered services as targets for exploitation The JSON export format makes it easy to integrate with these and other tools.\nInstallation and Setup Installing ArgusScan is straightforward:\n# Clone the repository git clone https://github.com/digenaldo/argusscan.git cd argusscan # Install dependencies pip install -r requirements.txt # Run the tool python argus_scan.py --help You\u0026rsquo;ll need a Shodan API key, which you can get from Shodan.io. Free accounts have some limitations, but they work with ArgusScan for basic searches.\nUsage Examples Here are practical examples of how to use ArgusScan:\nSearch for Jenkins Instances python argus_scan.py \u0026#34;jenkins port:8080\u0026#34; --token YOUR_API_KEY This searches for Jenkins instances running on port 8080. Jenkins is often misconfigured and can be a security risk if exposed publicly.\nThe output shows detailed information about discovered hosts, including IP addresses, ports, organizations, products, and identified vulnerabilities (CVEs). This makes it easy to identify potential targets and understand their security posture.\nSearch Apache Servers in Brazil python argus_scan.py \u0026#34;apache country:BR\u0026#34; --token YOUR_API_KEY This finds Apache web servers located in Brazil. You can use country filters to focus on specific regions.\nExport Results to JSON python argus_scan.py \u0026#34;jenkins\u0026#34; --token YOUR_API_KEY --export json This exports results in JSON format, which you can use with other tools or scripts for further analysis.\nGenerate Professional Report python argus_scan.py \u0026#34;apache\u0026#34; --token YOUR_API_KEY --export markdown --report This generates a professional markdown report following PTES/OWASP standards, suitable for client deliverables.\nTechnical Excellence ArgusScan is built with quality in mind:\nComprehensive Testing:\n25 unit tests covering all major functionality 96% code coverage ensuring reliability Automated testing with GitHub Actions CI/CD pipeline Code Quality:\nPre-commit hooks for quality assurance Modern Python 3.8+ implementation Clean, maintainable code structure Professional Development Practices:\nVersion control with Git Issue tracking and pull request workflow Documentation and examples This level of quality ensures the tool is reliable and suitable for professional use.\nIntegration with Pentest Workflows ArgusScan fits naturally into professional pentest workflows:\nReconnaissance Phase: Use ArgusScan to identify exposed services and potential targets before starting detailed testing.\nTool Integration: Export results to JSON and import them into Nuclei, OWASP ZAP, or Metasploit for automated vulnerability scanning and exploitation.\nReporting: Generate professional reports that can be included in your final pentest deliverables, following PTES and OWASP standards.\nDocumentation: Use Markdown exports to document findings and create detailed reports for clients.\nLegal and Ethical Considerations This is extremely important: ArgusScan should only be used with proper authorization.\nMandatory Requirements:\nWritten authorization (Rules of Engagement - RoE) before any scanning Only scan systems you own or have explicit permission to test Follow responsible disclosure practices Comply with all applicable laws and regulations Authorized Use Cases:\nBug bounty programs with written authorization Professional pentest engagements with signed RoE Academic security research with proper approvals Defensive security assessments of your own infrastructure Red team exercises with proper authorization Unauthorized Use:\nNever scan systems without permission Never use for malicious purposes Never violate terms of service Never break laws or regulations Security tools are powerful, and with that power comes responsibility. Always use ArgusScan ethically and legally.\nUse Cases ArgusScan is useful in several scenarios:\nAuthorized Bug Bounty Programs: When you have written authorization to test specific targets, ArgusScan helps you find exposed services quickly.\nProfessional Pentest Engagements: Use it during the reconnaissance phase to identify potential targets and attack surfaces before detailed testing.\nAcademic Security Research: Researchers can use it to study internet exposure and security trends, always with proper authorization.\nDefensive Security Assessments: Organizations can use it to assess their own infrastructure and identify misconfigurations or exposed services.\nRed Team Exercises: Red teams can use it to find targets within authorized scope during exercises.\nFuture Roadmap The project is actively maintained, and future enhancements may include:\nSupport for additional export formats Integration with more security tools Enhanced filtering options Web interface for easier use Additional report templates Performance improvements Contributions and feedback are welcome on the GitHub repository.\nConclusion ArgusScan is a powerful tool for ethical security professionals who need to automate reconnaissance during authorized pentest engagements. With its professional CLI, multiple export formats, and high code quality, it\u0026rsquo;s designed to make security assessments more efficient while maintaining professional standards.\nThe tool demonstrates that open-source security tools can be both powerful and well-built, with comprehensive testing and professional development practices.\nKey Takeaways:\nAutomates Shodan searches for pentest reconnaissance Professional reports following PTES/OWASP standards 96% test coverage and CI/CD pipeline Integrates with popular security tools Always use with proper authorization If you\u0026rsquo;re a security professional looking to improve your pentest workflow, consider trying ArgusScan. Star the repository on GitHub, try it out (with proper authorization), and contribute improvements if you find it useful.\nRemember: Security tools are powerful, but they must be used responsibly and ethically. Always get written authorization before scanning any systems.\nGet Started:\nGitHub Repository: https://github.com/digenaldo/argusscan Read the documentation Try it with your Shodan API key Contribute and share feedback Use responsibly, and happy ethical hacking!\n","permalink":"http://localhost:1313/posts/argusscan-ethical-pentest-automation-with-shodan/","summary":"\u003ch1 id=\"argusscan-automating-ethical-pentest-with-shodan-api\"\u003eArgusScan: Automating Ethical Pentest with Shodan API\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eGitHub Repository:\u003c/strong\u003e \u003ca href=\"https://github.com/digenaldo/argusscan\"\u003ehttps://github.com/digenaldo/argusscan\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\u003c/p\u003e\n\u003cp\u003eWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\u003c/p\u003e\n\u003cp\u003eThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\u003c/p\u003e","title":"ArgusScan: Automating Ethical Pentest with Shodan API"},{"content":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus Introduction Observability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\nI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\nThis article focuses on the actual results and what the metrics tell us about the performance differences between these two technologies.\nThe Setup The Monitoring Lab includes two backend applications running the same workload:\nA Go application that performs MongoDB operations every 5 seconds A Spring Boot application performing the same operations at the same interval Prometheus collecting metrics from both applications Grafana dashboards showing real-time performance data Both applications execute similar operations: they ping MongoDB, insert documents, and record metrics about their performance. The key difference is the technology stack each one uses.\nPerformance Metrics Analysis Go Application Performance The Go application shows consistent and efficient performance across all metrics:\nCPU Usage: The Go app maintains very low CPU usage at around 0.22%. This shows the application is lightweight and doesn\u0026rsquo;t require much processing power for these operations.\nMemory Usage: Memory consumption stays stable at approximately 21.6 MiB. This is a small amount of memory, showing that Go applications can be very memory-efficient.\nMongoDB Latency: The latency metrics show excellent performance:\nAverage latency stays between 0.5 ms and 1.2 ms P95 latency (95th percentile) remains consistently around 4.5 ms P99 latency (99th percentile) stays around 5 ms These numbers mean that most operations complete very quickly, and even the slowest operations are still fast.\nOperations Rate: The application maintains a steady rate of 0.3 operations per second, which matches the expected interval of one operation every 5 seconds.\nSpring Boot Application Performance The Spring Boot application shows different characteristics:\nCPU Usage: Similar to Go, CPU usage is low at around 0.28%. Both applications handle the workload without stressing the CPU.\nMemory Usage: This is where we see a significant difference. The Spring Boot application uses 112 MiB of memory, which is more than 5 times the memory used by the Go application. This higher memory usage is typical for Java applications due to the JVM overhead.\nMongoDB Latency: The latency metrics show more variation:\nAverage latency fluctuates between 10 ms and 15 ms P95 latency follows similar patterns to the average P99 latency ranges from 15 ms to 25 ms, with spikes reaching nearly 30 ms The latency is consistently higher than the Go application, which suggests the Spring Boot application takes longer to complete the same operations.\nOperations Rate: The application maintains 0.4 operations per second, slightly higher than the Go application.\nJVM Heap Memory: The dashboard shows how the Java Virtual Machine manages memory:\nG1 Eden Space fills up and gets cleared during garbage collection cycles G1 Old Gen gradually increases over time as objects are promoted G1 Survivor Space remains minimal This memory management pattern is normal for Java applications but adds overhead that Go applications don\u0026rsquo;t have.\nDirect Comparison The comparison dashboard makes the differences clear:\nLatency Comparison:\nGo maintains average latency below 2.5 ms throughout the entire period Spring Boot shows average latency ranging from 6 ms to 15 ms Go\u0026rsquo;s latency is consistently 3 to 6 times lower than Spring Boot\u0026rsquo;s Memory Comparison:\nGo uses between 16 MiB and 24 MiB consistently Spring Boot starts at 112 MiB and fluctuates between 80 MiB and 104 MiB Spring Boot uses approximately 4 to 7 times more memory than Go These results show that for this specific workload, the Go application performs better in terms of both latency and memory usage.\nKey Metrics Explained Understanding these metrics helps you make informed decisions about which technology to use:\nCPU Usage: Shows how much processing power the application needs. Lower is better, as it leaves more resources for other processes.\nMemory Usage: Indicates how much RAM the application consumes. Lower memory usage means you can run more instances on the same server.\nAverage Latency: The typical time it takes for an operation to complete. Lower latency means faster responses to users.\nP95 and P99 Latency: These percentiles show how the slowest operations perform. P95 means 95% of operations are faster than this value. P99 means 99% of operations are faster. These metrics help you understand worst-case performance.\nOperations Rate: How many operations the application completes per second. This helps you understand throughput.\nWhat These Results Mean The metrics show clear patterns:\nGo excels at low latency: The Go application consistently completes operations faster, with latency staying under 2.5 ms.\nGo uses less memory: With memory usage around 21 MiB, Go applications can run more instances on the same hardware.\nSpring Boot has higher overhead: The JVM and framework layers add overhead, resulting in higher latency and memory usage.\nBoth handle the workload: Neither application struggles with the workload, showing that both technologies can handle this type of operation effectively.\nIt\u0026rsquo;s important to remember that these results are specific to this test scenario. Different workloads, different operations, or different configurations might produce different results.\nReal-World Implications These metrics matter in production environments:\nLower latency means users get faster responses, which improves user experience. For applications handling many requests, even small latency differences can add up significantly.\nLower memory usage means you can run more application instances on the same server, which can reduce infrastructure costs. It also means applications start faster and use fewer resources.\nConsistent performance is important for reliability. When latency varies widely, it\u0026rsquo;s harder to predict how the application will behave under load.\nConclusion The Monitoring Lab provides real data about how Go and Spring Boot applications perform. The results show that for this specific workload:\nGo offers better latency performance and lower memory usage Spring Boot provides easier development through frameworks and tools, but with higher resource usage Both technologies can handle the workload effectively The choice between technologies depends on your specific needs. If low latency and low memory usage are priorities, Go might be a better fit. If rapid development and a rich ecosystem are more important, Spring Boot might be the right choice.\nObservability tools like Prometheus and Grafana make it possible to see these differences clearly. Without metrics, you\u0026rsquo;re making decisions based on assumptions. With metrics, you can make decisions based on data.\nThe code and dashboards are available on GitHub: https://github.com/digenaldo/monitoring-lab\n","permalink":"http://localhost:1313/posts/monitoring-lab-complete-observability-with-go-spring-boot-prometheus/","summary":"\u003ch1 id=\"monitoring-lab-a-complete-observability-lab-with-go-spring-boot-and-prometheus\"\u003eMonitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eObservability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\u003c/p\u003e\n\u003cp\u003eI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\u003c/p\u003e","title":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus"},{"content":"LLM Security: A Scientific Taxonomy of Attack Vectors Introduction Security in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\nThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\nThis post organizes the main scientific results from this period into a technical taxonomy. It is based on how attacks work, not on how the media talks about them.\n1. Adversarial Attacks and Transferable Jailbreaks Many people think of jailbreaks as just clever prompt writing. Research has shown there is more to it.\nThe chart below shows how effective GCG (Gradient-based Certifiably Good) attacks are when they are first optimized on open-source models (white-box) and then used against closed models (black-box) without any access to their internals. On open models like Vicuna-7B and Llama-2-7B-Chat, attack effectiveness is very high (around 99% and 88%). The important result is transferability: the same kind of attack reaches 84% on GPT-3.5, 66% on PaLM-2, and 47% on GPT-4 even though the attacker never had access to those models. Only Claude-2 in this setup shows much lower effectiveness (about 2%), illustrating that robustness to these attacks can differ a lot between systems.\nStudies showed that adversarial suffixes can be generated automatically by gradient optimization. These suffixes can make aligned models break safety rules [1]. They push the model into parts of its internal space where forbidden answers become more likely.\nThe most important finding is transferability. Attacks that were optimized for one open-source model often worked on proprietary models trained with similar pipelines [1].\nThis is not only because of the Transformer architecture. Transfer happens because training goals and alignment methods (especially RLHF) are similar. It is a systemic weakness in models that are aligned in similar ways.\nPractical takeaway: Defenses that only use word filters or fixed rules are weak against optimization-based attacks.\n2. Indirect Prompt Injection and Context Contamination With the rise of RAG and autonomous agents, the attack surface has changed.\nIndirect Prompt Injection was formalized as a real attack vector against applications that use LLMs [2]. The core issue is that the model does not clearly separate “trusted instructions” from “external retrieved data.” Both are just tokens in the same context.\nThis is an instance of the classic “confused deputy” problem applied to generative models.\nThe impact depends on the setup:\nStandalone LLM: The effect is usually limited to changing the text output. LLM with tools: Data can be exfiltrated through API calls. Agents with broad credentials: There is risk of unauthorized actions in external systems. The real risk is not only wrong content, but privilege escalation in distributed systems.\n3. Training Data Extraction and Memorization The question of memorization became concrete when it was shown that large models can reproduce rare sequences that appeared in their training data [3].\nIt helps to separate three types of attacks that are often mixed up:\nMembership inference: Estimates whether a specific piece of data was in the training set. Training data extraction: Directly extracts memorized sequences through adaptive generation. Memorization elicitation: Makes the model reveal rare passages through statistical probing. The study showed that large-scale extraction is possible, not only probabilistic inference [3]. This conflicts with the idea of “privacy by design,” especially for models trained on scraped web data.\n4. Behavioral Backdoors and Sleeper Agents Some hoped that safety alignment would remove all malicious behavior. Experiments tested this.\nIt was shown that models can keep behavioral backdoors even after safety fine-tuning with SFT and RLHF [4]. The model learns to trigger bad behavior only under a specific condition and to look safe during evaluation.\nThese experiments were done in controlled settings. There is no agreement that the same level of persistence happens in real industrial pipelines with extensive red teaming and other alignment techniques.\nStill, the result matters: alignment does not formally guarantee that hidden behavior is removed.\n5. Supply Chain and Tool Abuse Recent work suggests that the main risk may not be the model alone, but how it is integrated into larger systems [5].\nTwo vectors are especially important:\nModel poisoning and compromised provenance: Especially when using third-party fine-tuning or unaudited checkpoints. Tool abuse: Agents with access to internal APIs, databases, or financial systems act under probabilistic control. In corporate environments, the potential impact of tool abuse often exceeds the impact of jailbreak alone.\nConclusion Research between 2021 and 2025 points to a few main ideas:\nJailbreak is a mathematical optimization problem. RAG introduces structural context contamination. LLMs can memorize sensitive data. Backdoors can survive alignment. Integration with tools greatly expands the attack surface. LLM security is not just an extension of traditional AppSec, and it cannot be solved only with prompt engineering. It is a challenge of architecture, data governance, and privilege control in integrated probabilistic systems.\nReferences [1] Zou, A., Wang, Z., Kolter, J. Z., \u0026amp; Fredrikson, M. (2023). Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043.\n[2] Greshake, K., et al. (2023). Not What You\u0026rsquo;ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security.\n[3] Carlini, N., et al. (2021). Extracting Training Data from Large Language Models. USENIX Security Symposium.\n[4] Hubinger, E., et al. (2024). Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. arXiv:2401.05566.\n[5] Yao, Y., et al. (2024). A Survey on Large Language Model Security and Privacy: The Good, the Bad, and the Ugly. High-Confidence Computing.\n","permalink":"http://localhost:1313/posts/llm-security-scientific-taxonomy-attack-vectors-2021-2025/","summary":"\u003ch1 id=\"llm-security-a-scientific-taxonomy-of-attack-vectors\"\u003eLLM Security: A Scientific Taxonomy of Attack Vectors\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\u003c/p\u003e\n\u003cp\u003eThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\u003c/p\u003e","title":"LLM Security: A Scientific Taxonomy of Attack Vectors"},{"content":"ArgusScan: Automating Ethical Pentest with Shodan API GitHub Repository: https://github.com/digenaldo/argusscan\nIntroduction Security professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\nWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\nThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\nWhat is ArgusScan? ArgusScan is a professional pentest automation tool that uses Shodan API to search for vulnerable hosts and exposed services. It\u0026rsquo;s designed for security professionals who need to perform authorized security assessments quickly and generate professional reports.\nThe tool is built with Python 3.8+ and provides a rich command-line interface that makes it easy to search for specific services, filter results, and export data in multiple formats.\nKey Characteristics:\nProfessional CLI with colorful output using the Rich library Multiple export formats: Markdown, JSON, CSV Advanced filtering options Professional report generation following PTES/OWASP standards Rate limiting to respect API limits Integration with popular security tools Key Features Shodan API Integration ArgusScan fully integrates with Shodan API, allowing you to search for vulnerable hosts using Shodan search queries (also known as \u0026ldquo;dorks\u0026rdquo;). You can search by service, port, country, organization, and many other criteria.\nThe tool handles API authentication, rate limiting, and error handling automatically, so you can focus on your security assessment.\nProfessional CLI Interface The command-line interface uses the Rich library to provide colorful, organized output. This makes it easy to read results and understand what the tool is doing. The interface shows progress, results, and any errors clearly.\nMultiple Export Formats ArgusScan supports three export formats:\nMarkdown: Perfect for documentation and reports JSON: Ideal for integration with other tools and automation CSV: Great for spreadsheet analysis and data processing This flexibility allows you to use ArgusScan results in different workflows and integrate them with other security tools.\nAdvanced Filtering You can filter Shodan results by:\nCountry code (e.g., country:BR for Brazil) Port number (e.g., port:8080) Organization name And many other Shodan search parameters This helps you narrow down results to specific targets or regions, making your assessment more focused and efficient.\nProfessional Reports ArgusScan generates reports that follow industry standards like PTES (Penetration Testing Execution Standard) and OWASP methodologies. These reports are suitable for professional pentest engagements and can be included in your final deliverables.\nRate Limiting The tool respects Shodan API rate limits (1 request per second) automatically. This prevents API errors and ensures you stay within your plan\u0026rsquo;s limits, whether you\u0026rsquo;re using a free or paid Shodan account.\nTool Integration ArgusScan is designed to work with popular security tools:\nNuclei: Use results to run vulnerability scans OWASP ZAP: Import targets for automated security testing Metasploit: Use discovered services as targets for exploitation The JSON export format makes it easy to integrate with these and other tools.\nInstallation and Setup Installing ArgusScan is straightforward:\n# Clone the repository git clone https://github.com/digenaldo/argusscan.git cd argusscan # Install dependencies pip install -r requirements.txt # Run the tool python argus_scan.py --help You\u0026rsquo;ll need a Shodan API key, which you can get from Shodan.io. Free accounts have some limitations, but they work with ArgusScan for basic searches.\nUsage Examples Here are practical examples of how to use ArgusScan:\nSearch for Jenkins Instances python argus_scan.py \u0026#34;jenkins port:8080\u0026#34; --token YOUR_API_KEY This searches for Jenkins instances running on port 8080. Jenkins is often misconfigured and can be a security risk if exposed publicly.\nThe output shows detailed information about discovered hosts, including IP addresses, ports, organizations, products, and identified vulnerabilities (CVEs). This makes it easy to identify potential targets and understand their security posture.\nSearch Apache Servers in Brazil python argus_scan.py \u0026#34;apache country:BR\u0026#34; --token YOUR_API_KEY This finds Apache web servers located in Brazil. You can use country filters to focus on specific regions.\nExport Results to JSON python argus_scan.py \u0026#34;jenkins\u0026#34; --token YOUR_API_KEY --export json This exports results in JSON format, which you can use with other tools or scripts for further analysis.\nGenerate Professional Report python argus_scan.py \u0026#34;apache\u0026#34; --token YOUR_API_KEY --export markdown --report This generates a professional markdown report following PTES/OWASP standards, suitable for client deliverables.\nTechnical Excellence ArgusScan is built with quality in mind:\nComprehensive Testing:\n25 unit tests covering all major functionality 96% code coverage ensuring reliability Automated testing with GitHub Actions CI/CD pipeline Code Quality:\nPre-commit hooks for quality assurance Modern Python 3.8+ implementation Clean, maintainable code structure Professional Development Practices:\nVersion control with Git Issue tracking and pull request workflow Documentation and examples This level of quality ensures the tool is reliable and suitable for professional use.\nIntegration with Pentest Workflows ArgusScan fits naturally into professional pentest workflows:\nReconnaissance Phase: Use ArgusScan to identify exposed services and potential targets before starting detailed testing.\nTool Integration: Export results to JSON and import them into Nuclei, OWASP ZAP, or Metasploit for automated vulnerability scanning and exploitation.\nReporting: Generate professional reports that can be included in your final pentest deliverables, following PTES and OWASP standards.\nDocumentation: Use Markdown exports to document findings and create detailed reports for clients.\nLegal and Ethical Considerations This is extremely important: ArgusScan should only be used with proper authorization.\nMandatory Requirements:\nWritten authorization (Rules of Engagement - RoE) before any scanning Only scan systems you own or have explicit permission to test Follow responsible disclosure practices Comply with all applicable laws and regulations Authorized Use Cases:\nBug bounty programs with written authorization Professional pentest engagements with signed RoE Academic security research with proper approvals Defensive security assessments of your own infrastructure Red team exercises with proper authorization Unauthorized Use:\nNever scan systems without permission Never use for malicious purposes Never violate terms of service Never break laws or regulations Security tools are powerful, and with that power comes responsibility. Always use ArgusScan ethically and legally.\nUse Cases ArgusScan is useful in several scenarios:\nAuthorized Bug Bounty Programs: When you have written authorization to test specific targets, ArgusScan helps you find exposed services quickly.\nProfessional Pentest Engagements: Use it during the reconnaissance phase to identify potential targets and attack surfaces before detailed testing.\nAcademic Security Research: Researchers can use it to study internet exposure and security trends, always with proper authorization.\nDefensive Security Assessments: Organizations can use it to assess their own infrastructure and identify misconfigurations or exposed services.\nRed Team Exercises: Red teams can use it to find targets within authorized scope during exercises.\nFuture Roadmap The project is actively maintained, and future enhancements may include:\nSupport for additional export formats Integration with more security tools Enhanced filtering options Web interface for easier use Additional report templates Performance improvements Contributions and feedback are welcome on the GitHub repository.\nConclusion ArgusScan is a powerful tool for ethical security professionals who need to automate reconnaissance during authorized pentest engagements. With its professional CLI, multiple export formats, and high code quality, it\u0026rsquo;s designed to make security assessments more efficient while maintaining professional standards.\nThe tool demonstrates that open-source security tools can be both powerful and well-built, with comprehensive testing and professional development practices.\nKey Takeaways:\nAutomates Shodan searches for pentest reconnaissance Professional reports following PTES/OWASP standards 96% test coverage and CI/CD pipeline Integrates with popular security tools Always use with proper authorization If you\u0026rsquo;re a security professional looking to improve your pentest workflow, consider trying ArgusScan. Star the repository on GitHub, try it out (with proper authorization), and contribute improvements if you find it useful.\nRemember: Security tools are powerful, but they must be used responsibly and ethically. Always get written authorization before scanning any systems.\nGet Started:\nGitHub Repository: https://github.com/digenaldo/argusscan Read the documentation Try it with your Shodan API key Contribute and share feedback Use responsibly, and happy ethical hacking!\n","permalink":"http://localhost:1313/posts/argusscan-ethical-pentest-automation-with-shodan/","summary":"\u003ch1 id=\"argusscan-automating-ethical-pentest-with-shodan-api\"\u003eArgusScan: Automating Ethical Pentest with Shodan API\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eGitHub Repository:\u003c/strong\u003e \u003ca href=\"https://github.com/digenaldo/argusscan\"\u003ehttps://github.com/digenaldo/argusscan\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\u003c/p\u003e\n\u003cp\u003eWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\u003c/p\u003e\n\u003cp\u003eThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\u003c/p\u003e","title":"ArgusScan: Automating Ethical Pentest with Shodan API"},{"content":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus Introduction Observability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\nI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\nThis article focuses on the actual results and what the metrics tell us about the performance differences between these two technologies.\nThe Setup The Monitoring Lab includes two backend applications running the same workload:\nA Go application that performs MongoDB operations every 5 seconds A Spring Boot application performing the same operations at the same interval Prometheus collecting metrics from both applications Grafana dashboards showing real-time performance data Both applications execute similar operations: they ping MongoDB, insert documents, and record metrics about their performance. The key difference is the technology stack each one uses.\nPerformance Metrics Analysis Go Application Performance The Go application shows consistent and efficient performance across all metrics:\nCPU Usage: The Go app maintains very low CPU usage at around 0.22%. This shows the application is lightweight and doesn\u0026rsquo;t require much processing power for these operations.\nMemory Usage: Memory consumption stays stable at approximately 21.6 MiB. This is a small amount of memory, showing that Go applications can be very memory-efficient.\nMongoDB Latency: The latency metrics show excellent performance:\nAverage latency stays between 0.5 ms and 1.2 ms P95 latency (95th percentile) remains consistently around 4.5 ms P99 latency (99th percentile) stays around 5 ms These numbers mean that most operations complete very quickly, and even the slowest operations are still fast.\nOperations Rate: The application maintains a steady rate of 0.3 operations per second, which matches the expected interval of one operation every 5 seconds.\nSpring Boot Application Performance The Spring Boot application shows different characteristics:\nCPU Usage: Similar to Go, CPU usage is low at around 0.28%. Both applications handle the workload without stressing the CPU.\nMemory Usage: This is where we see a significant difference. The Spring Boot application uses 112 MiB of memory, which is more than 5 times the memory used by the Go application. This higher memory usage is typical for Java applications due to the JVM overhead.\nMongoDB Latency: The latency metrics show more variation:\nAverage latency fluctuates between 10 ms and 15 ms P95 latency follows similar patterns to the average P99 latency ranges from 15 ms to 25 ms, with spikes reaching nearly 30 ms The latency is consistently higher than the Go application, which suggests the Spring Boot application takes longer to complete the same operations.\nOperations Rate: The application maintains 0.4 operations per second, slightly higher than the Go application.\nJVM Heap Memory: The dashboard shows how the Java Virtual Machine manages memory:\nG1 Eden Space fills up and gets cleared during garbage collection cycles G1 Old Gen gradually increases over time as objects are promoted G1 Survivor Space remains minimal This memory management pattern is normal for Java applications but adds overhead that Go applications don\u0026rsquo;t have.\nDirect Comparison The comparison dashboard makes the differences clear:\nLatency Comparison:\nGo maintains average latency below 2.5 ms throughout the entire period Spring Boot shows average latency ranging from 6 ms to 15 ms Go\u0026rsquo;s latency is consistently 3 to 6 times lower than Spring Boot\u0026rsquo;s Memory Comparison:\nGo uses between 16 MiB and 24 MiB consistently Spring Boot starts at 112 MiB and fluctuates between 80 MiB and 104 MiB Spring Boot uses approximately 4 to 7 times more memory than Go These results show that for this specific workload, the Go application performs better in terms of both latency and memory usage.\nKey Metrics Explained Understanding these metrics helps you make informed decisions about which technology to use:\nCPU Usage: Shows how much processing power the application needs. Lower is better, as it leaves more resources for other processes.\nMemory Usage: Indicates how much RAM the application consumes. Lower memory usage means you can run more instances on the same server.\nAverage Latency: The typical time it takes for an operation to complete. Lower latency means faster responses to users.\nP95 and P99 Latency: These percentiles show how the slowest operations perform. P95 means 95% of operations are faster than this value. P99 means 99% of operations are faster. These metrics help you understand worst-case performance.\nOperations Rate: How many operations the application completes per second. This helps you understand throughput.\nWhat These Results Mean The metrics show clear patterns:\nGo excels at low latency: The Go application consistently completes operations faster, with latency staying under 2.5 ms.\nGo uses less memory: With memory usage around 21 MiB, Go applications can run more instances on the same hardware.\nSpring Boot has higher overhead: The JVM and framework layers add overhead, resulting in higher latency and memory usage.\nBoth handle the workload: Neither application struggles with the workload, showing that both technologies can handle this type of operation effectively.\nIt\u0026rsquo;s important to remember that these results are specific to this test scenario. Different workloads, different operations, or different configurations might produce different results.\nReal-World Implications These metrics matter in production environments:\nLower latency means users get faster responses, which improves user experience. For applications handling many requests, even small latency differences can add up significantly.\nLower memory usage means you can run more application instances on the same server, which can reduce infrastructure costs. It also means applications start faster and use fewer resources.\nConsistent performance is important for reliability. When latency varies widely, it\u0026rsquo;s harder to predict how the application will behave under load.\nConclusion The Monitoring Lab provides real data about how Go and Spring Boot applications perform. The results show that for this specific workload:\nGo offers better latency performance and lower memory usage Spring Boot provides easier development through frameworks and tools, but with higher resource usage Both technologies can handle the workload effectively The choice between technologies depends on your specific needs. If low latency and low memory usage are priorities, Go might be a better fit. If rapid development and a rich ecosystem are more important, Spring Boot might be the right choice.\nObservability tools like Prometheus and Grafana make it possible to see these differences clearly. Without metrics, you\u0026rsquo;re making decisions based on assumptions. With metrics, you can make decisions based on data.\nThe code and dashboards are available on GitHub: https://github.com/digenaldo/monitoring-lab\n","permalink":"http://localhost:1313/posts/monitoring-lab-complete-observability-with-go-spring-boot-prometheus/","summary":"\u003ch1 id=\"monitoring-lab-a-complete-observability-lab-with-go-spring-boot-and-prometheus\"\u003eMonitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eObservability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\u003c/p\u003e\n\u003cp\u003eI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\u003c/p\u003e","title":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus"},{"content":"LLM Security: A Scientific Taxonomy of Attack Vectors Introduction Security in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\nThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\nThis post organizes the main scientific results from this period into a technical taxonomy. It is based on how attacks work, not on how the media talks about them.\n1. Adversarial Attacks and Transferable Jailbreaks Many people think of jailbreaks as just clever prompt writing. Research has shown there is more to it.\nStudies showed that adversarial suffixes can be generated automatically by gradient optimization. These suffixes can make aligned models break safety rules [1]. They push the model into parts of its internal space where forbidden answers become more likely.\nThe most important finding is transferability. Attacks that were optimized for one open-source model often worked on proprietary models trained with similar pipelines [1].\nThis is not only because of the Transformer architecture. Transfer happens because training goals and alignment methods (especially RLHF) are similar. It is a systemic weakness in models that are aligned in similar ways.\nPractical takeaway: Defenses that only use word filters or fixed rules are weak against optimization-based attacks.\n2. Indirect Prompt Injection and Context Contamination With the rise of RAG and autonomous agents, the attack surface has changed.\nIndirect Prompt Injection was formalized as a real attack vector against applications that use LLMs [2]. The core issue is that the model does not clearly separate “trusted instructions” from “external retrieved data.” Both are just tokens in the same context.\nThis is an instance of the classic “confused deputy” problem applied to generative models.\nThe impact depends on the setup:\nStandalone LLM: The effect is usually limited to changing the text output. LLM with tools: Data can be exfiltrated through API calls. Agents with broad credentials: There is risk of unauthorized actions in external systems. The real risk is not only wrong content, but privilege escalation in distributed systems.\n3. Training Data Extraction and Memorization The question of memorization became concrete when it was shown that large models can reproduce rare sequences that appeared in their training data [3].\nIt helps to separate three types of attacks that are often mixed up:\nMembership inference: Estimates whether a specific piece of data was in the training set. Training data extraction: Directly extracts memorized sequences through adaptive generation. Memorization elicitation: Makes the model reveal rare passages through statistical probing. The study showed that large-scale extraction is possible, not only probabilistic inference [3]. This conflicts with the idea of “privacy by design,” especially for models trained on scraped web data.\n4. Behavioral Backdoors and Sleeper Agents Some hoped that safety alignment would remove all malicious behavior. Experiments tested this.\nIt was shown that models can keep behavioral backdoors even after safety fine-tuning with SFT and RLHF [4]. The model learns to trigger bad behavior only under a specific condition and to look safe during evaluation.\nThese experiments were done in controlled settings. There is no agreement that the same level of persistence happens in real industrial pipelines with extensive red teaming and other alignment techniques.\nStill, the result matters: alignment does not formally guarantee that hidden behavior is removed.\n5. Supply Chain and Tool Abuse Recent work suggests that the main risk may not be the model alone, but how it is integrated into larger systems [5].\nTwo vectors are especially important:\nModel poisoning and compromised provenance: Especially when using third-party fine-tuning or unaudited checkpoints. Tool abuse: Agents with access to internal APIs, databases, or financial systems act under probabilistic control. In corporate environments, the potential impact of tool abuse often exceeds the impact of jailbreak alone.\nConclusion Research between 2021 and 2025 points to a few main ideas:\nJailbreak is a mathematical optimization problem. RAG introduces structural context contamination. LLMs can memorize sensitive data. Backdoors can survive alignment. Integration with tools greatly expands the attack surface. LLM security is not just an extension of traditional AppSec, and it cannot be solved only with prompt engineering. It is a challenge of architecture, data governance, and privilege control in integrated probabilistic systems.\nReferences [1] Zou, A., Wang, Z., Kolter, J. Z., \u0026amp; Fredrikson, M. (2023). Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043.\n[2] Greshake, K., et al. (2023). Not What You\u0026rsquo;ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security.\n[3] Carlini, N., et al. (2021). Extracting Training Data from Large Language Models. USENIX Security Symposium.\n[4] Hubinger, E., et al. (2024). Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. arXiv:2401.05566.\n[5] Yao, Y., et al. (2024). A Survey on Large Language Model Security and Privacy: The Good, the Bad, and the Ugly. High-Confidence Computing.\n","permalink":"http://localhost:1313/posts/llm-security-scientific-taxonomy-attack-vectors-2021-2025/","summary":"\u003ch1 id=\"llm-security-a-scientific-taxonomy-of-attack-vectors\"\u003eLLM Security: A Scientific Taxonomy of Attack Vectors\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\u003c/p\u003e\n\u003cp\u003eThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\u003c/p\u003e","title":"LLM Security: A Scientific Taxonomy of Attack Vectors"},{"content":"ArgusScan: Automating Ethical Pentest with Shodan API GitHub Repository: https://github.com/digenaldo/argusscan\nIntroduction Security professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\nWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\nThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\nWhat is ArgusScan? ArgusScan is a professional pentest automation tool that uses Shodan API to search for vulnerable hosts and exposed services. It\u0026rsquo;s designed for security professionals who need to perform authorized security assessments quickly and generate professional reports.\nThe tool is built with Python 3.8+ and provides a rich command-line interface that makes it easy to search for specific services, filter results, and export data in multiple formats.\nKey Characteristics:\nProfessional CLI with colorful output using the Rich library Multiple export formats: Markdown, JSON, CSV Advanced filtering options Professional report generation following PTES/OWASP standards Rate limiting to respect API limits Integration with popular security tools Key Features Shodan API Integration ArgusScan fully integrates with Shodan API, allowing you to search for vulnerable hosts using Shodan search queries (also known as \u0026ldquo;dorks\u0026rdquo;). You can search by service, port, country, organization, and many other criteria.\nThe tool handles API authentication, rate limiting, and error handling automatically, so you can focus on your security assessment.\nProfessional CLI Interface The command-line interface uses the Rich library to provide colorful, organized output. This makes it easy to read results and understand what the tool is doing. The interface shows progress, results, and any errors clearly.\nMultiple Export Formats ArgusScan supports three export formats:\nMarkdown: Perfect for documentation and reports JSON: Ideal for integration with other tools and automation CSV: Great for spreadsheet analysis and data processing This flexibility allows you to use ArgusScan results in different workflows and integrate them with other security tools.\nAdvanced Filtering You can filter Shodan results by:\nCountry code (e.g., country:BR for Brazil) Port number (e.g., port:8080) Organization name And many other Shodan search parameters This helps you narrow down results to specific targets or regions, making your assessment more focused and efficient.\nProfessional Reports ArgusScan generates reports that follow industry standards like PTES (Penetration Testing Execution Standard) and OWASP methodologies. These reports are suitable for professional pentest engagements and can be included in your final deliverables.\nRate Limiting The tool respects Shodan API rate limits (1 request per second) automatically. This prevents API errors and ensures you stay within your plan\u0026rsquo;s limits, whether you\u0026rsquo;re using a free or paid Shodan account.\nTool Integration ArgusScan is designed to work with popular security tools:\nNuclei: Use results to run vulnerability scans OWASP ZAP: Import targets for automated security testing Metasploit: Use discovered services as targets for exploitation The JSON export format makes it easy to integrate with these and other tools.\nInstallation and Setup Installing ArgusScan is straightforward:\n# Clone the repository git clone https://github.com/digenaldo/argusscan.git cd argusscan # Install dependencies pip install -r requirements.txt # Run the tool python argus_scan.py --help You\u0026rsquo;ll need a Shodan API key, which you can get from Shodan.io. Free accounts have some limitations, but they work with ArgusScan for basic searches.\nUsage Examples Here are practical examples of how to use ArgusScan:\nSearch for Jenkins Instances python argus_scan.py \u0026#34;jenkins port:8080\u0026#34; --token YOUR_API_KEY This searches for Jenkins instances running on port 8080. Jenkins is often misconfigured and can be a security risk if exposed publicly.\nThe output shows detailed information about discovered hosts, including IP addresses, ports, organizations, products, and identified vulnerabilities (CVEs). This makes it easy to identify potential targets and understand their security posture.\nSearch Apache Servers in Brazil python argus_scan.py \u0026#34;apache country:BR\u0026#34; --token YOUR_API_KEY This finds Apache web servers located in Brazil. You can use country filters to focus on specific regions.\nExport Results to JSON python argus_scan.py \u0026#34;jenkins\u0026#34; --token YOUR_API_KEY --export json This exports results in JSON format, which you can use with other tools or scripts for further analysis.\nGenerate Professional Report python argus_scan.py \u0026#34;apache\u0026#34; --token YOUR_API_KEY --export markdown --report This generates a professional markdown report following PTES/OWASP standards, suitable for client deliverables.\nTechnical Excellence ArgusScan is built with quality in mind:\nComprehensive Testing:\n25 unit tests covering all major functionality 96% code coverage ensuring reliability Automated testing with GitHub Actions CI/CD pipeline Code Quality:\nPre-commit hooks for quality assurance Modern Python 3.8+ implementation Clean, maintainable code structure Professional Development Practices:\nVersion control with Git Issue tracking and pull request workflow Documentation and examples This level of quality ensures the tool is reliable and suitable for professional use.\nIntegration with Pentest Workflows ArgusScan fits naturally into professional pentest workflows:\nReconnaissance Phase: Use ArgusScan to identify exposed services and potential targets before starting detailed testing.\nTool Integration: Export results to JSON and import them into Nuclei, OWASP ZAP, or Metasploit for automated vulnerability scanning and exploitation.\nReporting: Generate professional reports that can be included in your final pentest deliverables, following PTES and OWASP standards.\nDocumentation: Use Markdown exports to document findings and create detailed reports for clients.\nLegal and Ethical Considerations This is extremely important: ArgusScan should only be used with proper authorization.\nMandatory Requirements:\nWritten authorization (Rules of Engagement - RoE) before any scanning Only scan systems you own or have explicit permission to test Follow responsible disclosure practices Comply with all applicable laws and regulations Authorized Use Cases:\nBug bounty programs with written authorization Professional pentest engagements with signed RoE Academic security research with proper approvals Defensive security assessments of your own infrastructure Red team exercises with proper authorization Unauthorized Use:\nNever scan systems without permission Never use for malicious purposes Never violate terms of service Never break laws or regulations Security tools are powerful, and with that power comes responsibility. Always use ArgusScan ethically and legally.\nUse Cases ArgusScan is useful in several scenarios:\nAuthorized Bug Bounty Programs: When you have written authorization to test specific targets, ArgusScan helps you find exposed services quickly.\nProfessional Pentest Engagements: Use it during the reconnaissance phase to identify potential targets and attack surfaces before detailed testing.\nAcademic Security Research: Researchers can use it to study internet exposure and security trends, always with proper authorization.\nDefensive Security Assessments: Organizations can use it to assess their own infrastructure and identify misconfigurations or exposed services.\nRed Team Exercises: Red teams can use it to find targets within authorized scope during exercises.\nFuture Roadmap The project is actively maintained, and future enhancements may include:\nSupport for additional export formats Integration with more security tools Enhanced filtering options Web interface for easier use Additional report templates Performance improvements Contributions and feedback are welcome on the GitHub repository.\nConclusion ArgusScan is a powerful tool for ethical security professionals who need to automate reconnaissance during authorized pentest engagements. With its professional CLI, multiple export formats, and high code quality, it\u0026rsquo;s designed to make security assessments more efficient while maintaining professional standards.\nThe tool demonstrates that open-source security tools can be both powerful and well-built, with comprehensive testing and professional development practices.\nKey Takeaways:\nAutomates Shodan searches for pentest reconnaissance Professional reports following PTES/OWASP standards 96% test coverage and CI/CD pipeline Integrates with popular security tools Always use with proper authorization If you\u0026rsquo;re a security professional looking to improve your pentest workflow, consider trying ArgusScan. Star the repository on GitHub, try it out (with proper authorization), and contribute improvements if you find it useful.\nRemember: Security tools are powerful, but they must be used responsibly and ethically. Always get written authorization before scanning any systems.\nGet Started:\nGitHub Repository: https://github.com/digenaldo/argusscan Read the documentation Try it with your Shodan API key Contribute and share feedback Use responsibly, and happy ethical hacking!\n","permalink":"http://localhost:1313/posts/argusscan-ethical-pentest-automation-with-shodan/","summary":"\u003ch1 id=\"argusscan-automating-ethical-pentest-with-shodan-api\"\u003eArgusScan: Automating Ethical Pentest with Shodan API\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eGitHub Repository:\u003c/strong\u003e \u003ca href=\"https://github.com/digenaldo/argusscan\"\u003ehttps://github.com/digenaldo/argusscan\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\u003c/p\u003e\n\u003cp\u003eWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\u003c/p\u003e\n\u003cp\u003eThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\u003c/p\u003e","title":"ArgusScan: Automating Ethical Pentest with Shodan API"},{"content":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus Introduction Observability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\nI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\nThis article focuses on the actual results and what the metrics tell us about the performance differences between these two technologies.\nThe Setup The Monitoring Lab includes two backend applications running the same workload:\nA Go application that performs MongoDB operations every 5 seconds A Spring Boot application performing the same operations at the same interval Prometheus collecting metrics from both applications Grafana dashboards showing real-time performance data Both applications execute similar operations: they ping MongoDB, insert documents, and record metrics about their performance. The key difference is the technology stack each one uses.\nPerformance Metrics Analysis Go Application Performance The Go application shows consistent and efficient performance across all metrics:\nCPU Usage: The Go app maintains very low CPU usage at around 0.22%. This shows the application is lightweight and doesn\u0026rsquo;t require much processing power for these operations.\nMemory Usage: Memory consumption stays stable at approximately 21.6 MiB. This is a small amount of memory, showing that Go applications can be very memory-efficient.\nMongoDB Latency: The latency metrics show excellent performance:\nAverage latency stays between 0.5 ms and 1.2 ms P95 latency (95th percentile) remains consistently around 4.5 ms P99 latency (99th percentile) stays around 5 ms These numbers mean that most operations complete very quickly, and even the slowest operations are still fast.\nOperations Rate: The application maintains a steady rate of 0.3 operations per second, which matches the expected interval of one operation every 5 seconds.\nSpring Boot Application Performance The Spring Boot application shows different characteristics:\nCPU Usage: Similar to Go, CPU usage is low at around 0.28%. Both applications handle the workload without stressing the CPU.\nMemory Usage: This is where we see a significant difference. The Spring Boot application uses 112 MiB of memory, which is more than 5 times the memory used by the Go application. This higher memory usage is typical for Java applications due to the JVM overhead.\nMongoDB Latency: The latency metrics show more variation:\nAverage latency fluctuates between 10 ms and 15 ms P95 latency follows similar patterns to the average P99 latency ranges from 15 ms to 25 ms, with spikes reaching nearly 30 ms The latency is consistently higher than the Go application, which suggests the Spring Boot application takes longer to complete the same operations.\nOperations Rate: The application maintains 0.4 operations per second, slightly higher than the Go application.\nJVM Heap Memory: The dashboard shows how the Java Virtual Machine manages memory:\nG1 Eden Space fills up and gets cleared during garbage collection cycles G1 Old Gen gradually increases over time as objects are promoted G1 Survivor Space remains minimal This memory management pattern is normal for Java applications but adds overhead that Go applications don\u0026rsquo;t have.\nDirect Comparison The comparison dashboard makes the differences clear:\nLatency Comparison:\nGo maintains average latency below 2.5 ms throughout the entire period Spring Boot shows average latency ranging from 6 ms to 15 ms Go\u0026rsquo;s latency is consistently 3 to 6 times lower than Spring Boot\u0026rsquo;s Memory Comparison:\nGo uses between 16 MiB and 24 MiB consistently Spring Boot starts at 112 MiB and fluctuates between 80 MiB and 104 MiB Spring Boot uses approximately 4 to 7 times more memory than Go These results show that for this specific workload, the Go application performs better in terms of both latency and memory usage.\nKey Metrics Explained Understanding these metrics helps you make informed decisions about which technology to use:\nCPU Usage: Shows how much processing power the application needs. Lower is better, as it leaves more resources for other processes.\nMemory Usage: Indicates how much RAM the application consumes. Lower memory usage means you can run more instances on the same server.\nAverage Latency: The typical time it takes for an operation to complete. Lower latency means faster responses to users.\nP95 and P99 Latency: These percentiles show how the slowest operations perform. P95 means 95% of operations are faster than this value. P99 means 99% of operations are faster. These metrics help you understand worst-case performance.\nOperations Rate: How many operations the application completes per second. This helps you understand throughput.\nWhat These Results Mean The metrics show clear patterns:\nGo excels at low latency: The Go application consistently completes operations faster, with latency staying under 2.5 ms.\nGo uses less memory: With memory usage around 21 MiB, Go applications can run more instances on the same hardware.\nSpring Boot has higher overhead: The JVM and framework layers add overhead, resulting in higher latency and memory usage.\nBoth handle the workload: Neither application struggles with the workload, showing that both technologies can handle this type of operation effectively.\nIt\u0026rsquo;s important to remember that these results are specific to this test scenario. Different workloads, different operations, or different configurations might produce different results.\nReal-World Implications These metrics matter in production environments:\nLower latency means users get faster responses, which improves user experience. For applications handling many requests, even small latency differences can add up significantly.\nLower memory usage means you can run more application instances on the same server, which can reduce infrastructure costs. It also means applications start faster and use fewer resources.\nConsistent performance is important for reliability. When latency varies widely, it\u0026rsquo;s harder to predict how the application will behave under load.\nConclusion The Monitoring Lab provides real data about how Go and Spring Boot applications perform. The results show that for this specific workload:\nGo offers better latency performance and lower memory usage Spring Boot provides easier development through frameworks and tools, but with higher resource usage Both technologies can handle the workload effectively The choice between technologies depends on your specific needs. If low latency and low memory usage are priorities, Go might be a better fit. If rapid development and a rich ecosystem are more important, Spring Boot might be the right choice.\nObservability tools like Prometheus and Grafana make it possible to see these differences clearly. Without metrics, you\u0026rsquo;re making decisions based on assumptions. With metrics, you can make decisions based on data.\nThe code and dashboards are available on GitHub: https://github.com/digenaldo/monitoring-lab\n","permalink":"http://localhost:1313/posts/monitoring-lab-complete-observability-with-go-spring-boot-prometheus/","summary":"\u003ch1 id=\"monitoring-lab-a-complete-observability-lab-with-go-spring-boot-and-prometheus\"\u003eMonitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eObservability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\u003c/p\u003e\n\u003cp\u003eI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\u003c/p\u003e","title":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus"},{"content":"LLM Security: A Scientific Taxonomy of Attack Vectors Introduction Security in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\nThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\nThis post organizes the main scientific results from this period into a technical taxonomy. It is based on how attacks work, not on how the media talks about them.\n1. Adversarial Attacks and Transferable Jailbreaks Many people think of jailbreaks as just clever prompt writing. Research has shown there is more to it.\nStudies showed that adversarial suffixes can be generated automatically by gradient optimization. These suffixes can make aligned models break safety rules [1]. They push the model into parts of its internal space where forbidden answers become more likely.\nThe most important finding is transferability. Attacks that were optimized for one open-source model often worked on proprietary models trained with similar pipelines [1].\nThis is not only because of the Transformer architecture. Transfer happens because training goals and alignment methods (especially RLHF) are similar. It is a systemic weakness in models that are aligned in similar ways.\nPractical takeaway: Defenses that only use word filters or fixed rules are weak against optimization-based attacks.\nFigure: Effectiveness of GCG (Gradient-based Certifiably Good) attacks when optimized on open-source models (white-box) and then transferred to closed models (black-box) without access to their internals. On open models (e.g. Vicuna-7B, Llama-2-7B-Chat) effectiveness is very high; the same attack still reaches high rates on GPT-3.5, PaLM-2 and GPT-4. Source: Zou et al. [1].\n2. Indirect Prompt Injection and Context Contamination With the rise of RAG and autonomous agents, the attack surface has changed.\nIndirect Prompt Injection was formalized as a real attack vector against applications that use LLMs [2]. The core issue is that the model does not clearly separate “trusted instructions” from “external retrieved data.” Both are just tokens in the same context.\nThis is an instance of the classic “confused deputy” problem applied to generative models.\nThe impact depends on the setup:\nStandalone LLM: The effect is usually limited to changing the text output. LLM with tools: Data can be exfiltrated through API calls. Agents with broad credentials: There is risk of unauthorized actions in external systems. The real risk is not only wrong content, but privilege escalation in distributed systems.\n3. Training Data Extraction and Memorization The question of memorization became concrete when it was shown that large models can reproduce rare sequences that appeared in their training data [3].\nIt helps to separate three types of attacks that are often mixed up:\nMembership inference: Estimates whether a specific piece of data was in the training set. Training data extraction: Directly extracts memorized sequences through adaptive generation. Memorization elicitation: Makes the model reveal rare passages through statistical probing. The study showed that large-scale extraction is possible, not only probabilistic inference [3]. This conflicts with the idea of “privacy by design,” especially for models trained on scraped web data.\n4. Behavioral Backdoors and Sleeper Agents Some hoped that safety alignment would remove all malicious behavior. Experiments tested this.\nIt was shown that models can keep behavioral backdoors even after safety fine-tuning with SFT and RLHF [4]. The model learns to trigger bad behavior only under a specific condition and to look safe during evaluation.\nThese experiments were done in controlled settings. There is no agreement that the same level of persistence happens in real industrial pipelines with extensive red teaming and other alignment techniques.\nStill, the result matters: alignment does not formally guarantee that hidden behavior is removed.\n5. Supply Chain and Tool Abuse Recent work suggests that the main risk may not be the model alone, but how it is integrated into larger systems [5].\nTwo vectors are especially important:\nModel poisoning and compromised provenance: Especially when using third-party fine-tuning or unaudited checkpoints. Tool abuse: Agents with access to internal APIs, databases, or financial systems act under probabilistic control. In corporate environments, the potential impact of tool abuse often exceeds the impact of jailbreak alone.\nConclusion Research between 2021 and 2025 points to a few main ideas:\nJailbreak is a mathematical optimization problem. RAG introduces structural context contamination. LLMs can memorize sensitive data. Backdoors can survive alignment. Integration with tools greatly expands the attack surface. LLM security is not just an extension of traditional AppSec, and it cannot be solved only with prompt engineering. It is a challenge of architecture, data governance, and privilege control in integrated probabilistic systems.\nReferences [1] Zou, A., Wang, Z., Kolter, J. Z., \u0026amp; Fredrikson, M. (2023). Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043.\n[2] Greshake, K., et al. (2023). Not What You\u0026rsquo;ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security.\n[3] Carlini, N., et al. (2021). Extracting Training Data from Large Language Models. USENIX Security Symposium.\n[4] Hubinger, E., et al. (2024). Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. arXiv:2401.05566.\n[5] Yao, Y., et al. (2024). A Survey on Large Language Model Security and Privacy: The Good, the Bad, and the Ugly. High-Confidence Computing.\n","permalink":"http://localhost:1313/posts/llm-security-scientific-taxonomy-attack-vectors-2021-2025/","summary":"\u003ch1 id=\"llm-security-a-scientific-taxonomy-of-attack-vectors\"\u003eLLM Security: A Scientific Taxonomy of Attack Vectors\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\u003c/p\u003e\n\u003cp\u003eThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\u003c/p\u003e","title":"LLM Security: A Scientific Taxonomy of Attack Vectors"},{"content":"ArgusScan: Automating Ethical Pentest with Shodan API GitHub Repository: https://github.com/digenaldo/argusscan\nIntroduction Security professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\nWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\nThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\nWhat is ArgusScan? ArgusScan is a professional pentest automation tool that uses Shodan API to search for vulnerable hosts and exposed services. It\u0026rsquo;s designed for security professionals who need to perform authorized security assessments quickly and generate professional reports.\nThe tool is built with Python 3.8+ and provides a rich command-line interface that makes it easy to search for specific services, filter results, and export data in multiple formats.\nKey Characteristics:\nProfessional CLI with colorful output using the Rich library Multiple export formats: Markdown, JSON, CSV Advanced filtering options Professional report generation following PTES/OWASP standards Rate limiting to respect API limits Integration with popular security tools Key Features Shodan API Integration ArgusScan fully integrates with Shodan API, allowing you to search for vulnerable hosts using Shodan search queries (also known as \u0026ldquo;dorks\u0026rdquo;). You can search by service, port, country, organization, and many other criteria.\nThe tool handles API authentication, rate limiting, and error handling automatically, so you can focus on your security assessment.\nProfessional CLI Interface The command-line interface uses the Rich library to provide colorful, organized output. This makes it easy to read results and understand what the tool is doing. The interface shows progress, results, and any errors clearly.\nMultiple Export Formats ArgusScan supports three export formats:\nMarkdown: Perfect for documentation and reports JSON: Ideal for integration with other tools and automation CSV: Great for spreadsheet analysis and data processing This flexibility allows you to use ArgusScan results in different workflows and integrate them with other security tools.\nAdvanced Filtering You can filter Shodan results by:\nCountry code (e.g., country:BR for Brazil) Port number (e.g., port:8080) Organization name And many other Shodan search parameters This helps you narrow down results to specific targets or regions, making your assessment more focused and efficient.\nProfessional Reports ArgusScan generates reports that follow industry standards like PTES (Penetration Testing Execution Standard) and OWASP methodologies. These reports are suitable for professional pentest engagements and can be included in your final deliverables.\nRate Limiting The tool respects Shodan API rate limits (1 request per second) automatically. This prevents API errors and ensures you stay within your plan\u0026rsquo;s limits, whether you\u0026rsquo;re using a free or paid Shodan account.\nTool Integration ArgusScan is designed to work with popular security tools:\nNuclei: Use results to run vulnerability scans OWASP ZAP: Import targets for automated security testing Metasploit: Use discovered services as targets for exploitation The JSON export format makes it easy to integrate with these and other tools.\nInstallation and Setup Installing ArgusScan is straightforward:\n# Clone the repository git clone https://github.com/digenaldo/argusscan.git cd argusscan # Install dependencies pip install -r requirements.txt # Run the tool python argus_scan.py --help You\u0026rsquo;ll need a Shodan API key, which you can get from Shodan.io. Free accounts have some limitations, but they work with ArgusScan for basic searches.\nUsage Examples Here are practical examples of how to use ArgusScan:\nSearch for Jenkins Instances python argus_scan.py \u0026#34;jenkins port:8080\u0026#34; --token YOUR_API_KEY This searches for Jenkins instances running on port 8080. Jenkins is often misconfigured and can be a security risk if exposed publicly.\nThe output shows detailed information about discovered hosts, including IP addresses, ports, organizations, products, and identified vulnerabilities (CVEs). This makes it easy to identify potential targets and understand their security posture.\nSearch Apache Servers in Brazil python argus_scan.py \u0026#34;apache country:BR\u0026#34; --token YOUR_API_KEY This finds Apache web servers located in Brazil. You can use country filters to focus on specific regions.\nExport Results to JSON python argus_scan.py \u0026#34;jenkins\u0026#34; --token YOUR_API_KEY --export json This exports results in JSON format, which you can use with other tools or scripts for further analysis.\nGenerate Professional Report python argus_scan.py \u0026#34;apache\u0026#34; --token YOUR_API_KEY --export markdown --report This generates a professional markdown report following PTES/OWASP standards, suitable for client deliverables.\nTechnical Excellence ArgusScan is built with quality in mind:\nComprehensive Testing:\n25 unit tests covering all major functionality 96% code coverage ensuring reliability Automated testing with GitHub Actions CI/CD pipeline Code Quality:\nPre-commit hooks for quality assurance Modern Python 3.8+ implementation Clean, maintainable code structure Professional Development Practices:\nVersion control with Git Issue tracking and pull request workflow Documentation and examples This level of quality ensures the tool is reliable and suitable for professional use.\nIntegration with Pentest Workflows ArgusScan fits naturally into professional pentest workflows:\nReconnaissance Phase: Use ArgusScan to identify exposed services and potential targets before starting detailed testing.\nTool Integration: Export results to JSON and import them into Nuclei, OWASP ZAP, or Metasploit for automated vulnerability scanning and exploitation.\nReporting: Generate professional reports that can be included in your final pentest deliverables, following PTES and OWASP standards.\nDocumentation: Use Markdown exports to document findings and create detailed reports for clients.\nLegal and Ethical Considerations This is extremely important: ArgusScan should only be used with proper authorization.\nMandatory Requirements:\nWritten authorization (Rules of Engagement - RoE) before any scanning Only scan systems you own or have explicit permission to test Follow responsible disclosure practices Comply with all applicable laws and regulations Authorized Use Cases:\nBug bounty programs with written authorization Professional pentest engagements with signed RoE Academic security research with proper approvals Defensive security assessments of your own infrastructure Red team exercises with proper authorization Unauthorized Use:\nNever scan systems without permission Never use for malicious purposes Never violate terms of service Never break laws or regulations Security tools are powerful, and with that power comes responsibility. Always use ArgusScan ethically and legally.\nUse Cases ArgusScan is useful in several scenarios:\nAuthorized Bug Bounty Programs: When you have written authorization to test specific targets, ArgusScan helps you find exposed services quickly.\nProfessional Pentest Engagements: Use it during the reconnaissance phase to identify potential targets and attack surfaces before detailed testing.\nAcademic Security Research: Researchers can use it to study internet exposure and security trends, always with proper authorization.\nDefensive Security Assessments: Organizations can use it to assess their own infrastructure and identify misconfigurations or exposed services.\nRed Team Exercises: Red teams can use it to find targets within authorized scope during exercises.\nFuture Roadmap The project is actively maintained, and future enhancements may include:\nSupport for additional export formats Integration with more security tools Enhanced filtering options Web interface for easier use Additional report templates Performance improvements Contributions and feedback are welcome on the GitHub repository.\nConclusion ArgusScan is a powerful tool for ethical security professionals who need to automate reconnaissance during authorized pentest engagements. With its professional CLI, multiple export formats, and high code quality, it\u0026rsquo;s designed to make security assessments more efficient while maintaining professional standards.\nThe tool demonstrates that open-source security tools can be both powerful and well-built, with comprehensive testing and professional development practices.\nKey Takeaways:\nAutomates Shodan searches for pentest reconnaissance Professional reports following PTES/OWASP standards 96% test coverage and CI/CD pipeline Integrates with popular security tools Always use with proper authorization If you\u0026rsquo;re a security professional looking to improve your pentest workflow, consider trying ArgusScan. Star the repository on GitHub, try it out (with proper authorization), and contribute improvements if you find it useful.\nRemember: Security tools are powerful, but they must be used responsibly and ethically. Always get written authorization before scanning any systems.\nGet Started:\nGitHub Repository: https://github.com/digenaldo/argusscan Read the documentation Try it with your Shodan API key Contribute and share feedback Use responsibly, and happy ethical hacking!\n","permalink":"http://localhost:1313/posts/argusscan-ethical-pentest-automation-with-shodan/","summary":"\u003ch1 id=\"argusscan-automating-ethical-pentest-with-shodan-api\"\u003eArgusScan: Automating Ethical Pentest with Shodan API\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eGitHub Repository:\u003c/strong\u003e \u003ca href=\"https://github.com/digenaldo/argusscan\"\u003ehttps://github.com/digenaldo/argusscan\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\u003c/p\u003e\n\u003cp\u003eWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\u003c/p\u003e\n\u003cp\u003eThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\u003c/p\u003e","title":"ArgusScan: Automating Ethical Pentest with Shodan API"},{"content":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus Introduction Observability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\nI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\nThis article focuses on the actual results and what the metrics tell us about the performance differences between these two technologies.\nThe Setup The Monitoring Lab includes two backend applications running the same workload:\nA Go application that performs MongoDB operations every 5 seconds A Spring Boot application performing the same operations at the same interval Prometheus collecting metrics from both applications Grafana dashboards showing real-time performance data Both applications execute similar operations: they ping MongoDB, insert documents, and record metrics about their performance. The key difference is the technology stack each one uses.\nPerformance Metrics Analysis Go Application Performance The Go application shows consistent and efficient performance across all metrics:\nCPU Usage: The Go app maintains very low CPU usage at around 0.22%. This shows the application is lightweight and doesn\u0026rsquo;t require much processing power for these operations.\nMemory Usage: Memory consumption stays stable at approximately 21.6 MiB. This is a small amount of memory, showing that Go applications can be very memory-efficient.\nMongoDB Latency: The latency metrics show excellent performance:\nAverage latency stays between 0.5 ms and 1.2 ms P95 latency (95th percentile) remains consistently around 4.5 ms P99 latency (99th percentile) stays around 5 ms These numbers mean that most operations complete very quickly, and even the slowest operations are still fast.\nOperations Rate: The application maintains a steady rate of 0.3 operations per second, which matches the expected interval of one operation every 5 seconds.\nSpring Boot Application Performance The Spring Boot application shows different characteristics:\nCPU Usage: Similar to Go, CPU usage is low at around 0.28%. Both applications handle the workload without stressing the CPU.\nMemory Usage: This is where we see a significant difference. The Spring Boot application uses 112 MiB of memory, which is more than 5 times the memory used by the Go application. This higher memory usage is typical for Java applications due to the JVM overhead.\nMongoDB Latency: The latency metrics show more variation:\nAverage latency fluctuates between 10 ms and 15 ms P95 latency follows similar patterns to the average P99 latency ranges from 15 ms to 25 ms, with spikes reaching nearly 30 ms The latency is consistently higher than the Go application, which suggests the Spring Boot application takes longer to complete the same operations.\nOperations Rate: The application maintains 0.4 operations per second, slightly higher than the Go application.\nJVM Heap Memory: The dashboard shows how the Java Virtual Machine manages memory:\nG1 Eden Space fills up and gets cleared during garbage collection cycles G1 Old Gen gradually increases over time as objects are promoted G1 Survivor Space remains minimal This memory management pattern is normal for Java applications but adds overhead that Go applications don\u0026rsquo;t have.\nDirect Comparison The comparison dashboard makes the differences clear:\nLatency Comparison:\nGo maintains average latency below 2.5 ms throughout the entire period Spring Boot shows average latency ranging from 6 ms to 15 ms Go\u0026rsquo;s latency is consistently 3 to 6 times lower than Spring Boot\u0026rsquo;s Memory Comparison:\nGo uses between 16 MiB and 24 MiB consistently Spring Boot starts at 112 MiB and fluctuates between 80 MiB and 104 MiB Spring Boot uses approximately 4 to 7 times more memory than Go These results show that for this specific workload, the Go application performs better in terms of both latency and memory usage.\nKey Metrics Explained Understanding these metrics helps you make informed decisions about which technology to use:\nCPU Usage: Shows how much processing power the application needs. Lower is better, as it leaves more resources for other processes.\nMemory Usage: Indicates how much RAM the application consumes. Lower memory usage means you can run more instances on the same server.\nAverage Latency: The typical time it takes for an operation to complete. Lower latency means faster responses to users.\nP95 and P99 Latency: These percentiles show how the slowest operations perform. P95 means 95% of operations are faster than this value. P99 means 99% of operations are faster. These metrics help you understand worst-case performance.\nOperations Rate: How many operations the application completes per second. This helps you understand throughput.\nWhat These Results Mean The metrics show clear patterns:\nGo excels at low latency: The Go application consistently completes operations faster, with latency staying under 2.5 ms.\nGo uses less memory: With memory usage around 21 MiB, Go applications can run more instances on the same hardware.\nSpring Boot has higher overhead: The JVM and framework layers add overhead, resulting in higher latency and memory usage.\nBoth handle the workload: Neither application struggles with the workload, showing that both technologies can handle this type of operation effectively.\nIt\u0026rsquo;s important to remember that these results are specific to this test scenario. Different workloads, different operations, or different configurations might produce different results.\nReal-World Implications These metrics matter in production environments:\nLower latency means users get faster responses, which improves user experience. For applications handling many requests, even small latency differences can add up significantly.\nLower memory usage means you can run more application instances on the same server, which can reduce infrastructure costs. It also means applications start faster and use fewer resources.\nConsistent performance is important for reliability. When latency varies widely, it\u0026rsquo;s harder to predict how the application will behave under load.\nConclusion The Monitoring Lab provides real data about how Go and Spring Boot applications perform. The results show that for this specific workload:\nGo offers better latency performance and lower memory usage Spring Boot provides easier development through frameworks and tools, but with higher resource usage Both technologies can handle the workload effectively The choice between technologies depends on your specific needs. If low latency and low memory usage are priorities, Go might be a better fit. If rapid development and a rich ecosystem are more important, Spring Boot might be the right choice.\nObservability tools like Prometheus and Grafana make it possible to see these differences clearly. Without metrics, you\u0026rsquo;re making decisions based on assumptions. With metrics, you can make decisions based on data.\nThe code and dashboards are available on GitHub: https://github.com/digenaldo/monitoring-lab\n","permalink":"http://localhost:1313/posts/monitoring-lab-complete-observability-with-go-spring-boot-prometheus/","summary":"\u003ch1 id=\"monitoring-lab-a-complete-observability-lab-with-go-spring-boot-and-prometheus\"\u003eMonitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eObservability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\u003c/p\u003e\n\u003cp\u003eI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\u003c/p\u003e","title":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus"},{"content":"LLM Security: A Scientific Taxonomy of Attack Vectors Introduction Security in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\nThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\nThis post organizes the main scientific results from this period into a technical taxonomy. It is based on how attacks work, not on how the media talks about them.\n1. Adversarial Attacks and Transferable Jailbreaks Many people think of jailbreaks as just clever prompt writing. Research has shown there is more to it.\nStudies showed that adversarial suffixes can be generated automatically by gradient optimization. These suffixes can make aligned models break safety rules [1]. They push the model into parts of its internal space where forbidden answers become more likely.\nThe most important finding is transferability. Attacks that were optimized for one open-source model often worked on proprietary models trained with similar pipelines [1].\nThis is not only because of the Transformer architecture. Transfer happens because training goals and alignment methods (especially RLHF (Reinforcement Learning from Human Feedback)) are similar. It is a systemic weakness in models that are aligned in similar ways.\nPractical takeaway: Defenses that only use word filters or fixed rules are weak against optimization-based attacks.\nFigure: Effectiveness of GCG (Gradient-based Certifiably Good) attacks when optimized on open-source models (white-box) and then transferred to closed models (black-box) without access to their internals. On open models (e.g. Vicuna-7B, Llama-2-7B-Chat) effectiveness is very high; the same attack still reaches high rates on GPT-3.5, PaLM-2 and GPT-4. Source: Zou et al. [1].\n2. Indirect Prompt Injection and Context Contamination With the rise of RAG and autonomous agents, the attack surface has changed.\nIndirect Prompt Injection was formalized as a real attack vector against applications that use LLMs [2]. The core issue is that the model does not clearly separate “trusted instructions” from “external retrieved data.” Both are just tokens in the same context.\nThis is an instance of the classic “confused deputy” problem applied to generative models.\nThe impact depends on the setup:\nStandalone LLM: The effect is usually limited to changing the text output. LLM with tools: Data can be exfiltrated through API calls. Agents with broad credentials: There is risk of unauthorized actions in external systems. The real risk is not only wrong content, but privilege escalation in distributed systems.\n3. Training Data Extraction and Memorization The question of memorization became concrete when it was shown that large models can reproduce rare sequences that appeared in their training data [3].\nIt helps to separate three types of attacks that are often mixed up:\nMembership inference: Estimates whether a specific piece of data was in the training set. Training data extraction: Directly extracts memorized sequences through adaptive generation. Memorization elicitation: Makes the model reveal rare passages through statistical probing. The study showed that large-scale extraction is possible, not only probabilistic inference [3]. This conflicts with the idea of “privacy by design,” especially for models trained on scraped web data.\n4. Behavioral Backdoors and Sleeper Agents Some hoped that safety alignment would remove all malicious behavior. Experiments tested this.\nIt was shown that models can keep behavioral backdoors even after safety fine-tuning with SFT and RLHF [4]. The model learns to trigger bad behavior only under a specific condition and to look safe during evaluation.\nThese experiments were done in controlled settings. There is no agreement that the same level of persistence happens in real industrial pipelines with extensive red teaming and other alignment techniques.\nStill, the result matters: alignment does not formally guarantee that hidden behavior is removed.\n5. Supply Chain and Tool Abuse Recent work suggests that the main risk may not be the model alone, but how it is integrated into larger systems [5].\nTwo vectors are especially important:\nModel poisoning and compromised provenance: Especially when using third-party fine-tuning or unaudited checkpoints. Tool abuse: Agents with access to internal APIs, databases, or financial systems act under probabilistic control. In corporate environments, the potential impact of tool abuse often exceeds the impact of jailbreak alone.\nConclusion Research between 2021 and 2025 points to a few main ideas:\nJailbreak is a mathematical optimization problem. RAG introduces structural context contamination. LLMs can memorize sensitive data. Backdoors can survive alignment. Integration with tools greatly expands the attack surface. LLM security is not just an extension of traditional AppSec, and it cannot be solved only with prompt engineering. It is a challenge of architecture, data governance, and privilege control in integrated probabilistic systems.\nReferences [1] Zou, A., Wang, Z., Kolter, J. Z., \u0026amp; Fredrikson, M. (2023). Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043.\n[2] Greshake, K., et al. (2023). Not What You\u0026rsquo;ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security.\n[3] Carlini, N., et al. (2021). Extracting Training Data from Large Language Models. USENIX Security Symposium.\n[4] Hubinger, E., et al. (2024). Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. arXiv:2401.05566.\n[5] Yao, Y., et al. (2024). A Survey on Large Language Model Security and Privacy: The Good, the Bad, and the Ugly. High-Confidence Computing.\n","permalink":"http://localhost:1313/posts/llm-security-scientific-taxonomy-attack-vectors-2021-2025/","summary":"\u003ch1 id=\"llm-security-a-scientific-taxonomy-of-attack-vectors\"\u003eLLM Security: A Scientific Taxonomy of Attack Vectors\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\u003c/p\u003e\n\u003cp\u003eThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\u003c/p\u003e","title":"LLM Security: A Scientific Taxonomy of Attack Vectors"},{"content":"LLM Security: A Scientific Taxonomy of Attack Vectors Introduction Security in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\nThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\nThis post organizes the main scientific results from this period into a technical taxonomy. It is based on how attacks work, not on how the media talks about them.\n1. Adversarial Attacks and Transferable Jailbreaks Many people think of jailbreaks as just clever prompt writing. Research has shown there is more to it.\nStudies showed that adversarial suffixes can be generated automatically by gradient optimization. These suffixes can make aligned models break safety rules [1]. They push the model into parts of its internal space where forbidden answers become more likely.\nThe most important finding is transferability. Attacks that were optimized for one open-source model often worked on proprietary models trained with similar pipelines [1].\nThis is not only because of the Transformer architecture. Transfer happens because training goals and alignment methods (especially RLHF (Reinforcement Learning from Human Feedback)) are similar. It is a systemic weakness in models that are aligned in similar ways.\nPractical takeaway: Defenses that only use word filters or fixed rules are weak against optimization-based attacks.\nFigure: Effectiveness of GCG (Gradient-based Certifiably Good) attacks when optimized on open-source models (white-box) and then transferred to closed models (black-box) without access to their internals. On open models (e.g. Vicuna-7B, Llama-2-7B-Chat) effectiveness is very high; the same attack still reaches high rates on GPT-3.5, PaLM-2 and GPT-4. Source: Zou et al. [1].\n2. Indirect Prompt Injection and Context Contamination With the rise of RAG and autonomous agents, the attack surface has changed.\nIndirect Prompt Injection was formalized as a real attack vector against applications that use LLMs [2]. The core issue is that the model does not clearly separate “trusted instructions” from “external retrieved data.” Both are just tokens in the same context.\nThis is an instance of the classic “confused deputy” problem applied to generative models.\nThe impact depends on the setup:\nStandalone LLM: The effect is usually limited to changing the text output. LLM with tools: Data can be exfiltrated through API calls. Agents with broad credentials: There is risk of unauthorized actions in external systems. The real risk is not only wrong content, but privilege escalation in distributed systems.\n3. Training Data Extraction and Memorization The question of memorization became concrete when it was shown that large models can reproduce rare sequences that appeared in their training data [3].\nIt helps to separate three types of attacks that are often mixed up:\nMembership inference: Estimates whether a specific piece of data was in the training set. Training data extraction: Directly extracts memorized sequences through adaptive generation. Memorization elicitation: Makes the model reveal rare passages through statistical probing. The study showed that large-scale extraction is possible, not only probabilistic inference [3]. This conflicts with the idea of “privacy by design,” especially for models trained on scraped web data.\n4. Behavioral Backdoors and Sleeper Agents Some hoped that safety alignment would remove all malicious behavior. Experiments tested this.\nIt was shown that models can keep behavioral backdoors even after safety fine-tuning with SFT and RLHF [4]. The model learns to trigger bad behavior only under a specific condition and to look safe during evaluation.\nThese experiments were done in controlled settings. There is no agreement that the same level of persistence happens in real industrial pipelines with extensive red teaming and other alignment techniques.\nStill, the result matters: alignment does not formally guarantee that hidden behavior is removed.\n5. Supply Chain and Tool Abuse Recent work suggests that the main risk may not be the model alone, but how it is integrated into larger systems [5].\nTwo vectors are especially important:\nModel poisoning and compromised provenance: Especially when using third-party fine-tuning or unaudited checkpoints. Tool abuse: Agents with access to internal APIs, databases, or financial systems act under probabilistic control. In corporate environments, the potential impact of tool abuse often exceeds the impact of jailbreak alone.\nConclusion Research between 2021 and 2025 points to a few main ideas:\nJailbreak is a mathematical optimization problem. RAG introduces structural context contamination. LLMs can memorize sensitive data. Backdoors can survive alignment. Integration with tools greatly expands the attack surface. LLM security is not just an extension of traditional AppSec, and it cannot be solved only with prompt engineering. It is a challenge of architecture, data governance, and privilege control in integrated probabilistic systems.\nReferences [1] Zou, A., Wang, Z., Kolter, J. Z., \u0026amp; Fredrikson, M. (2023). Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043.\n[2] Greshake, K., et al. (2023). Not What You\u0026rsquo;ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security.\n[3] Carlini, N., et al. (2021). Extracting Training Data from Large Language Models. USENIX Security Symposium.\n[4] Hubinger, E., et al. (2024). Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. arXiv:2401.05566.\n[5] Yao, Y., et al. (2024). A Survey on Large Language Model Security and Privacy: The Good, the Bad, and the Ugly. High-Confidence Computing.\n","permalink":"http://localhost:1313/posts/llm-security-scientific-taxonomy-attack-vectors-2021-2025/","summary":"\u003ch1 id=\"llm-security-a-scientific-taxonomy-of-attack-vectors\"\u003eLLM Security: A Scientific Taxonomy of Attack Vectors\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\u003c/p\u003e\n\u003cp\u003eThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\u003c/p\u003e","title":"LLM Security: A Scientific Taxonomy of Attack Vectors"},{"content":"ArgusScan: Automating Ethical Pentest with Shodan API GitHub Repository: https://github.com/digenaldo/argusscan\nIntroduction Security professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\nWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\nThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\nWhat is ArgusScan? ArgusScan is a professional pentest automation tool that uses Shodan API to search for vulnerable hosts and exposed services. It\u0026rsquo;s designed for security professionals who need to perform authorized security assessments quickly and generate professional reports.\nThe tool is built with Python 3.8+ and provides a rich command-line interface that makes it easy to search for specific services, filter results, and export data in multiple formats.\nKey Characteristics:\nProfessional CLI with colorful output using the Rich library Multiple export formats: Markdown, JSON, CSV Advanced filtering options Professional report generation following PTES/OWASP standards Rate limiting to respect API limits Integration with popular security tools Key Features Shodan API Integration ArgusScan fully integrates with Shodan API, allowing you to search for vulnerable hosts using Shodan search queries (also known as \u0026ldquo;dorks\u0026rdquo;). You can search by service, port, country, organization, and many other criteria.\nThe tool handles API authentication, rate limiting, and error handling automatically, so you can focus on your security assessment.\nProfessional CLI Interface The command-line interface uses the Rich library to provide colorful, organized output. This makes it easy to read results and understand what the tool is doing. The interface shows progress, results, and any errors clearly.\nMultiple Export Formats ArgusScan supports three export formats:\nMarkdown: Perfect for documentation and reports JSON: Ideal for integration with other tools and automation CSV: Great for spreadsheet analysis and data processing This flexibility allows you to use ArgusScan results in different workflows and integrate them with other security tools.\nAdvanced Filtering You can filter Shodan results by:\nCountry code (e.g., country:BR for Brazil) Port number (e.g., port:8080) Organization name And many other Shodan search parameters This helps you narrow down results to specific targets or regions, making your assessment more focused and efficient.\nProfessional Reports ArgusScan generates reports that follow industry standards like PTES (Penetration Testing Execution Standard) and OWASP methodologies. These reports are suitable for professional pentest engagements and can be included in your final deliverables.\nRate Limiting The tool respects Shodan API rate limits (1 request per second) automatically. This prevents API errors and ensures you stay within your plan\u0026rsquo;s limits, whether you\u0026rsquo;re using a free or paid Shodan account.\nTool Integration ArgusScan is designed to work with popular security tools:\nNuclei: Use results to run vulnerability scans OWASP ZAP: Import targets for automated security testing Metasploit: Use discovered services as targets for exploitation The JSON export format makes it easy to integrate with these and other tools.\nInstallation and Setup Installing ArgusScan is straightforward:\n# Clone the repository git clone https://github.com/digenaldo/argusscan.git cd argusscan # Install dependencies pip install -r requirements.txt # Run the tool python argus_scan.py --help You\u0026rsquo;ll need a Shodan API key, which you can get from Shodan.io. Free accounts have some limitations, but they work with ArgusScan for basic searches.\nUsage Examples Here are practical examples of how to use ArgusScan:\nSearch for Jenkins Instances python argus_scan.py \u0026#34;jenkins port:8080\u0026#34; --token YOUR_API_KEY This searches for Jenkins instances running on port 8080. Jenkins is often misconfigured and can be a security risk if exposed publicly.\nThe output shows detailed information about discovered hosts, including IP addresses, ports, organizations, products, and identified vulnerabilities (CVEs). This makes it easy to identify potential targets and understand their security posture.\nSearch Apache Servers in Brazil python argus_scan.py \u0026#34;apache country:BR\u0026#34; --token YOUR_API_KEY This finds Apache web servers located in Brazil. You can use country filters to focus on specific regions.\nExport Results to JSON python argus_scan.py \u0026#34;jenkins\u0026#34; --token YOUR_API_KEY --export json This exports results in JSON format, which you can use with other tools or scripts for further analysis.\nGenerate Professional Report python argus_scan.py \u0026#34;apache\u0026#34; --token YOUR_API_KEY --export markdown --report This generates a professional markdown report following PTES/OWASP standards, suitable for client deliverables.\nTechnical Excellence ArgusScan is built with quality in mind:\nComprehensive Testing:\n25 unit tests covering all major functionality 96% code coverage ensuring reliability Automated testing with GitHub Actions CI/CD pipeline Code Quality:\nPre-commit hooks for quality assurance Modern Python 3.8+ implementation Clean, maintainable code structure Professional Development Practices:\nVersion control with Git Issue tracking and pull request workflow Documentation and examples This level of quality ensures the tool is reliable and suitable for professional use.\nIntegration with Pentest Workflows ArgusScan fits naturally into professional pentest workflows:\nReconnaissance Phase: Use ArgusScan to identify exposed services and potential targets before starting detailed testing.\nTool Integration: Export results to JSON and import them into Nuclei, OWASP ZAP, or Metasploit for automated vulnerability scanning and exploitation.\nReporting: Generate professional reports that can be included in your final pentest deliverables, following PTES and OWASP standards.\nDocumentation: Use Markdown exports to document findings and create detailed reports for clients.\nLegal and Ethical Considerations This is extremely important: ArgusScan should only be used with proper authorization.\nMandatory Requirements:\nWritten authorization (Rules of Engagement - RoE) before any scanning Only scan systems you own or have explicit permission to test Follow responsible disclosure practices Comply with all applicable laws and regulations Authorized Use Cases:\nBug bounty programs with written authorization Professional pentest engagements with signed RoE Academic security research with proper approvals Defensive security assessments of your own infrastructure Red team exercises with proper authorization Unauthorized Use:\nNever scan systems without permission Never use for malicious purposes Never violate terms of service Never break laws or regulations Security tools are powerful, and with that power comes responsibility. Always use ArgusScan ethically and legally.\nUse Cases ArgusScan is useful in several scenarios:\nAuthorized Bug Bounty Programs: When you have written authorization to test specific targets, ArgusScan helps you find exposed services quickly.\nProfessional Pentest Engagements: Use it during the reconnaissance phase to identify potential targets and attack surfaces before detailed testing.\nAcademic Security Research: Researchers can use it to study internet exposure and security trends, always with proper authorization.\nDefensive Security Assessments: Organizations can use it to assess their own infrastructure and identify misconfigurations or exposed services.\nRed Team Exercises: Red teams can use it to find targets within authorized scope during exercises.\nFuture Roadmap The project is actively maintained, and future enhancements may include:\nSupport for additional export formats Integration with more security tools Enhanced filtering options Web interface for easier use Additional report templates Performance improvements Contributions and feedback are welcome on the GitHub repository.\nConclusion ArgusScan is a powerful tool for ethical security professionals who need to automate reconnaissance during authorized pentest engagements. With its professional CLI, multiple export formats, and high code quality, it\u0026rsquo;s designed to make security assessments more efficient while maintaining professional standards.\nThe tool demonstrates that open-source security tools can be both powerful and well-built, with comprehensive testing and professional development practices.\nKey Takeaways:\nAutomates Shodan searches for pentest reconnaissance Professional reports following PTES/OWASP standards 96% test coverage and CI/CD pipeline Integrates with popular security tools Always use with proper authorization If you\u0026rsquo;re a security professional looking to improve your pentest workflow, consider trying ArgusScan. Star the repository on GitHub, try it out (with proper authorization), and contribute improvements if you find it useful.\nRemember: Security tools are powerful, but they must be used responsibly and ethically. Always get written authorization before scanning any systems.\nGet Started:\nGitHub Repository: https://github.com/digenaldo/argusscan Read the documentation Try it with your Shodan API key Contribute and share feedback Use responsibly, and happy ethical hacking!\n","permalink":"http://localhost:1313/posts/argusscan-ethical-pentest-automation-with-shodan/","summary":"\u003ch1 id=\"argusscan-automating-ethical-pentest-with-shodan-api\"\u003eArgusScan: Automating Ethical Pentest with Shodan API\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eGitHub Repository:\u003c/strong\u003e \u003ca href=\"https://github.com/digenaldo/argusscan\"\u003ehttps://github.com/digenaldo/argusscan\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\u003c/p\u003e\n\u003cp\u003eWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\u003c/p\u003e\n\u003cp\u003eThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\u003c/p\u003e","title":"ArgusScan: Automating Ethical Pentest with Shodan API"},{"content":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus Introduction Observability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\nI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\nThis article focuses on the actual results and what the metrics tell us about the performance differences between these two technologies.\nThe Setup The Monitoring Lab includes two backend applications running the same workload:\nA Go application that performs MongoDB operations every 5 seconds A Spring Boot application performing the same operations at the same interval Prometheus collecting metrics from both applications Grafana dashboards showing real-time performance data Both applications execute similar operations: they ping MongoDB, insert documents, and record metrics about their performance. The key difference is the technology stack each one uses.\nPerformance Metrics Analysis Go Application Performance The Go application shows consistent and efficient performance across all metrics:\nCPU Usage: The Go app maintains very low CPU usage at around 0.22%. This shows the application is lightweight and doesn\u0026rsquo;t require much processing power for these operations.\nMemory Usage: Memory consumption stays stable at approximately 21.6 MiB. This is a small amount of memory, showing that Go applications can be very memory-efficient.\nMongoDB Latency: The latency metrics show excellent performance:\nAverage latency stays between 0.5 ms and 1.2 ms P95 latency (95th percentile) remains consistently around 4.5 ms P99 latency (99th percentile) stays around 5 ms These numbers mean that most operations complete very quickly, and even the slowest operations are still fast.\nOperations Rate: The application maintains a steady rate of 0.3 operations per second, which matches the expected interval of one operation every 5 seconds.\nSpring Boot Application Performance The Spring Boot application shows different characteristics:\nCPU Usage: Similar to Go, CPU usage is low at around 0.28%. Both applications handle the workload without stressing the CPU.\nMemory Usage: This is where we see a significant difference. The Spring Boot application uses 112 MiB of memory, which is more than 5 times the memory used by the Go application. This higher memory usage is typical for Java applications due to the JVM overhead.\nMongoDB Latency: The latency metrics show more variation:\nAverage latency fluctuates between 10 ms and 15 ms P95 latency follows similar patterns to the average P99 latency ranges from 15 ms to 25 ms, with spikes reaching nearly 30 ms The latency is consistently higher than the Go application, which suggests the Spring Boot application takes longer to complete the same operations.\nOperations Rate: The application maintains 0.4 operations per second, slightly higher than the Go application.\nJVM Heap Memory: The dashboard shows how the Java Virtual Machine manages memory:\nG1 Eden Space fills up and gets cleared during garbage collection cycles G1 Old Gen gradually increases over time as objects are promoted G1 Survivor Space remains minimal This memory management pattern is normal for Java applications but adds overhead that Go applications don\u0026rsquo;t have.\nDirect Comparison The comparison dashboard makes the differences clear:\nLatency Comparison:\nGo maintains average latency below 2.5 ms throughout the entire period Spring Boot shows average latency ranging from 6 ms to 15 ms Go\u0026rsquo;s latency is consistently 3 to 6 times lower than Spring Boot\u0026rsquo;s Memory Comparison:\nGo uses between 16 MiB and 24 MiB consistently Spring Boot starts at 112 MiB and fluctuates between 80 MiB and 104 MiB Spring Boot uses approximately 4 to 7 times more memory than Go These results show that for this specific workload, the Go application performs better in terms of both latency and memory usage.\nKey Metrics Explained Understanding these metrics helps you make informed decisions about which technology to use:\nCPU Usage: Shows how much processing power the application needs. Lower is better, as it leaves more resources for other processes.\nMemory Usage: Indicates how much RAM the application consumes. Lower memory usage means you can run more instances on the same server.\nAverage Latency: The typical time it takes for an operation to complete. Lower latency means faster responses to users.\nP95 and P99 Latency: These percentiles show how the slowest operations perform. P95 means 95% of operations are faster than this value. P99 means 99% of operations are faster. These metrics help you understand worst-case performance.\nOperations Rate: How many operations the application completes per second. This helps you understand throughput.\nWhat These Results Mean The metrics show clear patterns:\nGo excels at low latency: The Go application consistently completes operations faster, with latency staying under 2.5 ms.\nGo uses less memory: With memory usage around 21 MiB, Go applications can run more instances on the same hardware.\nSpring Boot has higher overhead: The JVM and framework layers add overhead, resulting in higher latency and memory usage.\nBoth handle the workload: Neither application struggles with the workload, showing that both technologies can handle this type of operation effectively.\nIt\u0026rsquo;s important to remember that these results are specific to this test scenario. Different workloads, different operations, or different configurations might produce different results.\nReal-World Implications These metrics matter in production environments:\nLower latency means users get faster responses, which improves user experience. For applications handling many requests, even small latency differences can add up significantly.\nLower memory usage means you can run more application instances on the same server, which can reduce infrastructure costs. It also means applications start faster and use fewer resources.\nConsistent performance is important for reliability. When latency varies widely, it\u0026rsquo;s harder to predict how the application will behave under load.\nConclusion The Monitoring Lab provides real data about how Go and Spring Boot applications perform. The results show that for this specific workload:\nGo offers better latency performance and lower memory usage Spring Boot provides easier development through frameworks and tools, but with higher resource usage Both technologies can handle the workload effectively The choice between technologies depends on your specific needs. If low latency and low memory usage are priorities, Go might be a better fit. If rapid development and a rich ecosystem are more important, Spring Boot might be the right choice.\nObservability tools like Prometheus and Grafana make it possible to see these differences clearly. Without metrics, you\u0026rsquo;re making decisions based on assumptions. With metrics, you can make decisions based on data.\nThe code and dashboards are available on GitHub: https://github.com/digenaldo/monitoring-lab\n","permalink":"http://localhost:1313/posts/monitoring-lab-complete-observability-with-go-spring-boot-prometheus/","summary":"\u003ch1 id=\"monitoring-lab-a-complete-observability-lab-with-go-spring-boot-and-prometheus\"\u003eMonitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eObservability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\u003c/p\u003e\n\u003cp\u003eI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\u003c/p\u003e","title":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus"},{"content":"Introduction Security in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\nThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\nThis post organizes the main scientific results from this period into a technical taxonomy. It is based on how attacks work, not on how the media talks about them.\n1. Adversarial Attacks and Transferable Jailbreaks Many people think of jailbreaks as just clever prompt writing. Research has shown there is more to it.\nStudies showed that adversarial suffixes can be generated automatically by gradient optimization. These suffixes can make aligned models break safety rules [1]. They push the model into parts of its internal space where forbidden answers become more likely.\nThe most important finding is transferability. Attacks that were optimized for one open-source model often worked on proprietary models trained with similar pipelines [1].\nThis is not only because of the Transformer architecture. Transfer happens because training goals and alignment methods (especially RLHF (Reinforcement Learning from Human Feedback)) are similar. It is a systemic weakness in models that are aligned in similar ways.\nPractical takeaway: Defenses that only use word filters or fixed rules are weak against optimization-based attacks.\nFigure: Effectiveness of GCG (Gradient-based Certifiably Good) attacks when optimized on open-source models (white-box) and then transferred to closed models (black-box) without access to their internals. On open models (e.g. Vicuna-7B, Llama-2-7B-Chat) effectiveness is very high; the same attack still reaches high rates on GPT-3.5, PaLM-2 and GPT-4. Source: Zou et al. [1].\n2. Indirect Prompt Injection and Context Contamination With the rise of RAG and autonomous agents, the attack surface has changed.\nIndirect Prompt Injection was formalized as a real attack vector against applications that use LLMs [2]. The core issue is that the model does not clearly separate “trusted instructions” from “external retrieved data.” Both are just tokens in the same context.\nThis is an instance of the classic “confused deputy” problem applied to generative models.\nThe impact depends on the setup:\nStandalone LLM: The effect is usually limited to changing the text output. LLM with tools: Data can be exfiltrated through API calls. Agents with broad credentials: There is risk of unauthorized actions in external systems. The real risk is not only wrong content, but privilege escalation in distributed systems.\n3. Training Data Extraction and Memorization The question of memorization became concrete when it was shown that large models can reproduce rare sequences that appeared in their training data [3].\nIt helps to separate three types of attacks that are often mixed up:\nMembership inference: Estimates whether a specific piece of data was in the training set. Training data extraction: Directly extracts memorized sequences through adaptive generation. Memorization elicitation: Makes the model reveal rare passages through statistical probing. The study showed that large-scale extraction is possible, not only probabilistic inference [3]. This conflicts with the idea of “privacy by design,” especially for models trained on scraped web data.\n4. Behavioral Backdoors and Sleeper Agents Some hoped that safety alignment would remove all malicious behavior. Experiments tested this.\nIt was shown that models can keep behavioral backdoors even after safety fine-tuning with SFT and RLHF [4]. The model learns to trigger bad behavior only under a specific condition and to look safe during evaluation.\nThese experiments were done in controlled settings. There is no agreement that the same level of persistence happens in real industrial pipelines with extensive red teaming and other alignment techniques.\nStill, the result matters: alignment does not formally guarantee that hidden behavior is removed.\n5. Supply Chain and Tool Abuse Recent work suggests that the main risk may not be the model alone, but how it is integrated into larger systems [5].\nTwo vectors are especially important:\nModel poisoning and compromised provenance: Especially when using third-party fine-tuning or unaudited checkpoints. Tool abuse: Agents with access to internal APIs, databases, or financial systems act under probabilistic control. In corporate environments, the potential impact of tool abuse often exceeds the impact of jailbreak alone.\nConclusion Research between 2021 and 2025 points to a few main ideas:\nJailbreak is a mathematical optimization problem. RAG introduces structural context contamination. LLMs can memorize sensitive data. Backdoors can survive alignment. Integration with tools greatly expands the attack surface. LLM security is not just an extension of traditional AppSec, and it cannot be solved only with prompt engineering. It is a challenge of architecture, data governance, and privilege control in integrated probabilistic systems.\nReferences [1] Zou, A., Wang, Z., Kolter, J. Z., \u0026amp; Fredrikson, M. (2023). Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043.\n[2] Greshake, K., et al. (2023). Not What You\u0026rsquo;ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security.\n[3] Carlini, N., et al. (2021). Extracting Training Data from Large Language Models. USENIX Security Symposium.\n[4] Hubinger, E., et al. (2024). Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. arXiv:2401.05566.\n[5] Yao, Y., et al. (2024). A Survey on Large Language Model Security and Privacy: The Good, the Bad, and the Ugly. High-Confidence Computing.\n","permalink":"http://localhost:1313/posts/llm-security-scientific-taxonomy-attack-vectors-2021-2025/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\u003c/p\u003e\n\u003cp\u003eThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\u003c/p\u003e","title":"LLM Security: A Scientific Taxonomy of Attack Vectors"},{"content":"ArgusScan: Automating Ethical Pentest with Shodan API GitHub Repository: https://github.com/digenaldo/argusscan\nIntroduction Security professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\nWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\nThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\nWhat is ArgusScan? ArgusScan is a professional pentest automation tool that uses Shodan API to search for vulnerable hosts and exposed services. It\u0026rsquo;s designed for security professionals who need to perform authorized security assessments quickly and generate professional reports.\nThe tool is built with Python 3.8+ and provides a rich command-line interface that makes it easy to search for specific services, filter results, and export data in multiple formats.\nKey Characteristics:\nProfessional CLI with colorful output using the Rich library Multiple export formats: Markdown, JSON, CSV Advanced filtering options Professional report generation following PTES/OWASP standards Rate limiting to respect API limits Integration with popular security tools Key Features Shodan API Integration ArgusScan fully integrates with Shodan API, allowing you to search for vulnerable hosts using Shodan search queries (also known as \u0026ldquo;dorks\u0026rdquo;). You can search by service, port, country, organization, and many other criteria.\nThe tool handles API authentication, rate limiting, and error handling automatically, so you can focus on your security assessment.\nProfessional CLI Interface The command-line interface uses the Rich library to provide colorful, organized output. This makes it easy to read results and understand what the tool is doing. The interface shows progress, results, and any errors clearly.\nMultiple Export Formats ArgusScan supports three export formats:\nMarkdown: Perfect for documentation and reports JSON: Ideal for integration with other tools and automation CSV: Great for spreadsheet analysis and data processing This flexibility allows you to use ArgusScan results in different workflows and integrate them with other security tools.\nAdvanced Filtering You can filter Shodan results by:\nCountry code (e.g., country:BR for Brazil) Port number (e.g., port:8080) Organization name And many other Shodan search parameters This helps you narrow down results to specific targets or regions, making your assessment more focused and efficient.\nProfessional Reports ArgusScan generates reports that follow industry standards like PTES (Penetration Testing Execution Standard) and OWASP methodologies. These reports are suitable for professional pentest engagements and can be included in your final deliverables.\nRate Limiting The tool respects Shodan API rate limits (1 request per second) automatically. This prevents API errors and ensures you stay within your plan\u0026rsquo;s limits, whether you\u0026rsquo;re using a free or paid Shodan account.\nTool Integration ArgusScan is designed to work with popular security tools:\nNuclei: Use results to run vulnerability scans OWASP ZAP: Import targets for automated security testing Metasploit: Use discovered services as targets for exploitation The JSON export format makes it easy to integrate with these and other tools.\nInstallation and Setup Installing ArgusScan is straightforward:\n# Clone the repository git clone https://github.com/digenaldo/argusscan.git cd argusscan # Install dependencies pip install -r requirements.txt # Run the tool python argus_scan.py --help You\u0026rsquo;ll need a Shodan API key, which you can get from Shodan.io. Free accounts have some limitations, but they work with ArgusScan for basic searches.\nUsage Examples Here are practical examples of how to use ArgusScan:\nSearch for Jenkins Instances python argus_scan.py \u0026#34;jenkins port:8080\u0026#34; --token YOUR_API_KEY This searches for Jenkins instances running on port 8080. Jenkins is often misconfigured and can be a security risk if exposed publicly.\nThe output shows detailed information about discovered hosts, including IP addresses, ports, organizations, products, and identified vulnerabilities (CVEs). This makes it easy to identify potential targets and understand their security posture.\nSearch Apache Servers in Brazil python argus_scan.py \u0026#34;apache country:BR\u0026#34; --token YOUR_API_KEY This finds Apache web servers located in Brazil. You can use country filters to focus on specific regions.\nExport Results to JSON python argus_scan.py \u0026#34;jenkins\u0026#34; --token YOUR_API_KEY --export json This exports results in JSON format, which you can use with other tools or scripts for further analysis.\nGenerate Professional Report python argus_scan.py \u0026#34;apache\u0026#34; --token YOUR_API_KEY --export markdown --report This generates a professional markdown report following PTES/OWASP standards, suitable for client deliverables.\nTechnical Excellence ArgusScan is built with quality in mind:\nComprehensive Testing:\n25 unit tests covering all major functionality 96% code coverage ensuring reliability Automated testing with GitHub Actions CI/CD pipeline Code Quality:\nPre-commit hooks for quality assurance Modern Python 3.8+ implementation Clean, maintainable code structure Professional Development Practices:\nVersion control with Git Issue tracking and pull request workflow Documentation and examples This level of quality ensures the tool is reliable and suitable for professional use.\nIntegration with Pentest Workflows ArgusScan fits naturally into professional pentest workflows:\nReconnaissance Phase: Use ArgusScan to identify exposed services and potential targets before starting detailed testing.\nTool Integration: Export results to JSON and import them into Nuclei, OWASP ZAP, or Metasploit for automated vulnerability scanning and exploitation.\nReporting: Generate professional reports that can be included in your final pentest deliverables, following PTES and OWASP standards.\nDocumentation: Use Markdown exports to document findings and create detailed reports for clients.\nLegal and Ethical Considerations This is extremely important: ArgusScan should only be used with proper authorization.\nMandatory Requirements:\nWritten authorization (Rules of Engagement - RoE) before any scanning Only scan systems you own or have explicit permission to test Follow responsible disclosure practices Comply with all applicable laws and regulations Authorized Use Cases:\nBug bounty programs with written authorization Professional pentest engagements with signed RoE Academic security research with proper approvals Defensive security assessments of your own infrastructure Red team exercises with proper authorization Unauthorized Use:\nNever scan systems without permission Never use for malicious purposes Never violate terms of service Never break laws or regulations Security tools are powerful, and with that power comes responsibility. Always use ArgusScan ethically and legally.\nUse Cases ArgusScan is useful in several scenarios:\nAuthorized Bug Bounty Programs: When you have written authorization to test specific targets, ArgusScan helps you find exposed services quickly.\nProfessional Pentest Engagements: Use it during the reconnaissance phase to identify potential targets and attack surfaces before detailed testing.\nAcademic Security Research: Researchers can use it to study internet exposure and security trends, always with proper authorization.\nDefensive Security Assessments: Organizations can use it to assess their own infrastructure and identify misconfigurations or exposed services.\nRed Team Exercises: Red teams can use it to find targets within authorized scope during exercises.\nFuture Roadmap The project is actively maintained, and future enhancements may include:\nSupport for additional export formats Integration with more security tools Enhanced filtering options Web interface for easier use Additional report templates Performance improvements Contributions and feedback are welcome on the GitHub repository.\nConclusion ArgusScan is a powerful tool for ethical security professionals who need to automate reconnaissance during authorized pentest engagements. With its professional CLI, multiple export formats, and high code quality, it\u0026rsquo;s designed to make security assessments more efficient while maintaining professional standards.\nThe tool demonstrates that open-source security tools can be both powerful and well-built, with comprehensive testing and professional development practices.\nKey Takeaways:\nAutomates Shodan searches for pentest reconnaissance Professional reports following PTES/OWASP standards 96% test coverage and CI/CD pipeline Integrates with popular security tools Always use with proper authorization If you\u0026rsquo;re a security professional looking to improve your pentest workflow, consider trying ArgusScan. Star the repository on GitHub, try it out (with proper authorization), and contribute improvements if you find it useful.\nRemember: Security tools are powerful, but they must be used responsibly and ethically. Always get written authorization before scanning any systems.\nGet Started:\nGitHub Repository: https://github.com/digenaldo/argusscan Read the documentation Try it with your Shodan API key Contribute and share feedback Use responsibly, and happy ethical hacking!\n","permalink":"http://localhost:1313/posts/argusscan-ethical-pentest-automation-with-shodan/","summary":"\u003ch1 id=\"argusscan-automating-ethical-pentest-with-shodan-api\"\u003eArgusScan: Automating Ethical Pentest with Shodan API\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eGitHub Repository:\u003c/strong\u003e \u003ca href=\"https://github.com/digenaldo/argusscan\"\u003ehttps://github.com/digenaldo/argusscan\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\u003c/p\u003e\n\u003cp\u003eWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\u003c/p\u003e\n\u003cp\u003eThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\u003c/p\u003e","title":"ArgusScan: Automating Ethical Pentest with Shodan API"},{"content":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus Introduction Observability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\nI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\nThis article focuses on the actual results and what the metrics tell us about the performance differences between these two technologies.\nThe Setup The Monitoring Lab includes two backend applications running the same workload:\nA Go application that performs MongoDB operations every 5 seconds A Spring Boot application performing the same operations at the same interval Prometheus collecting metrics from both applications Grafana dashboards showing real-time performance data Both applications execute similar operations: they ping MongoDB, insert documents, and record metrics about their performance. The key difference is the technology stack each one uses.\nPerformance Metrics Analysis Go Application Performance The Go application shows consistent and efficient performance across all metrics:\nCPU Usage: The Go app maintains very low CPU usage at around 0.22%. This shows the application is lightweight and doesn\u0026rsquo;t require much processing power for these operations.\nMemory Usage: Memory consumption stays stable at approximately 21.6 MiB. This is a small amount of memory, showing that Go applications can be very memory-efficient.\nMongoDB Latency: The latency metrics show excellent performance:\nAverage latency stays between 0.5 ms and 1.2 ms P95 latency (95th percentile) remains consistently around 4.5 ms P99 latency (99th percentile) stays around 5 ms These numbers mean that most operations complete very quickly, and even the slowest operations are still fast.\nOperations Rate: The application maintains a steady rate of 0.3 operations per second, which matches the expected interval of one operation every 5 seconds.\nSpring Boot Application Performance The Spring Boot application shows different characteristics:\nCPU Usage: Similar to Go, CPU usage is low at around 0.28%. Both applications handle the workload without stressing the CPU.\nMemory Usage: This is where we see a significant difference. The Spring Boot application uses 112 MiB of memory, which is more than 5 times the memory used by the Go application. This higher memory usage is typical for Java applications due to the JVM overhead.\nMongoDB Latency: The latency metrics show more variation:\nAverage latency fluctuates between 10 ms and 15 ms P95 latency follows similar patterns to the average P99 latency ranges from 15 ms to 25 ms, with spikes reaching nearly 30 ms The latency is consistently higher than the Go application, which suggests the Spring Boot application takes longer to complete the same operations.\nOperations Rate: The application maintains 0.4 operations per second, slightly higher than the Go application.\nJVM Heap Memory: The dashboard shows how the Java Virtual Machine manages memory:\nG1 Eden Space fills up and gets cleared during garbage collection cycles G1 Old Gen gradually increases over time as objects are promoted G1 Survivor Space remains minimal This memory management pattern is normal for Java applications but adds overhead that Go applications don\u0026rsquo;t have.\nDirect Comparison The comparison dashboard makes the differences clear:\nLatency Comparison:\nGo maintains average latency below 2.5 ms throughout the entire period Spring Boot shows average latency ranging from 6 ms to 15 ms Go\u0026rsquo;s latency is consistently 3 to 6 times lower than Spring Boot\u0026rsquo;s Memory Comparison:\nGo uses between 16 MiB and 24 MiB consistently Spring Boot starts at 112 MiB and fluctuates between 80 MiB and 104 MiB Spring Boot uses approximately 4 to 7 times more memory than Go These results show that for this specific workload, the Go application performs better in terms of both latency and memory usage.\nKey Metrics Explained Understanding these metrics helps you make informed decisions about which technology to use:\nCPU Usage: Shows how much processing power the application needs. Lower is better, as it leaves more resources for other processes.\nMemory Usage: Indicates how much RAM the application consumes. Lower memory usage means you can run more instances on the same server.\nAverage Latency: The typical time it takes for an operation to complete. Lower latency means faster responses to users.\nP95 and P99 Latency: These percentiles show how the slowest operations perform. P95 means 95% of operations are faster than this value. P99 means 99% of operations are faster. These metrics help you understand worst-case performance.\nOperations Rate: How many operations the application completes per second. This helps you understand throughput.\nWhat These Results Mean The metrics show clear patterns:\nGo excels at low latency: The Go application consistently completes operations faster, with latency staying under 2.5 ms.\nGo uses less memory: With memory usage around 21 MiB, Go applications can run more instances on the same hardware.\nSpring Boot has higher overhead: The JVM and framework layers add overhead, resulting in higher latency and memory usage.\nBoth handle the workload: Neither application struggles with the workload, showing that both technologies can handle this type of operation effectively.\nIt\u0026rsquo;s important to remember that these results are specific to this test scenario. Different workloads, different operations, or different configurations might produce different results.\nReal-World Implications These metrics matter in production environments:\nLower latency means users get faster responses, which improves user experience. For applications handling many requests, even small latency differences can add up significantly.\nLower memory usage means you can run more application instances on the same server, which can reduce infrastructure costs. It also means applications start faster and use fewer resources.\nConsistent performance is important for reliability. When latency varies widely, it\u0026rsquo;s harder to predict how the application will behave under load.\nConclusion The Monitoring Lab provides real data about how Go and Spring Boot applications perform. The results show that for this specific workload:\nGo offers better latency performance and lower memory usage Spring Boot provides easier development through frameworks and tools, but with higher resource usage Both technologies can handle the workload effectively The choice between technologies depends on your specific needs. If low latency and low memory usage are priorities, Go might be a better fit. If rapid development and a rich ecosystem are more important, Spring Boot might be the right choice.\nObservability tools like Prometheus and Grafana make it possible to see these differences clearly. Without metrics, you\u0026rsquo;re making decisions based on assumptions. With metrics, you can make decisions based on data.\nThe code and dashboards are available on GitHub: https://github.com/digenaldo/monitoring-lab\n","permalink":"http://localhost:1313/posts/monitoring-lab-complete-observability-with-go-spring-boot-prometheus/","summary":"\u003ch1 id=\"monitoring-lab-a-complete-observability-lab-with-go-spring-boot-and-prometheus\"\u003eMonitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eObservability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\u003c/p\u003e\n\u003cp\u003eI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\u003c/p\u003e","title":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus"},{"content":"Introduction Security in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\nThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\nThis post organizes the main scientific results from this period into a technical taxonomy. It is based on how attacks work, not on how the media talks about them.\n1. Adversarial Attacks and Transferable Jailbreaks Many people think of jailbreaks as just clever prompt writing. Research has shown there is more to it.\nStudies showed that adversarial suffixes can be generated automatically by gradient optimization. These suffixes can make aligned models break safety rules [1]. They push the model into parts of its internal space where forbidden answers become more likely.\nThe most important finding is transferability. Attacks that were optimized for one open-source model often worked on proprietary models trained with similar pipelines [1].\nThis is not only because of the Transformer architecture. Transfer happens because training goals and alignment methods (especially RLHF (Reinforcement Learning from Human Feedback)) are similar. It is a systemic weakness in models that are aligned in similar ways.\nPractical takeaway: Defenses that only use word filters or fixed rules are weak against optimization-based attacks.\nFigure: Effectiveness of GCG (Gradient-based Certifiably Good) attacks when optimized on open-source models (white-box) and then transferred to closed models (black-box) without access to their internals. On open models (e.g. Vicuna-7B, Llama-2-7B-Chat) effectiveness is very high; the same attack still reaches high rates on GPT-3.5, PaLM-2 and GPT-4. Source: Zou et al. [1].\n2. Indirect Prompt Injection and Context Contamination With the rise of RAG and autonomous agents, the attack surface has changed.\nIndirect Prompt Injection was formalized as a real attack vector against applications that use LLMs [2]. The core issue is that the model does not clearly separate “trusted instructions” from “external retrieved data.” Both are just tokens in the same context.\nThis is an instance of the classic “confused deputy” problem applied to generative models.\nThe impact depends on the setup:\nStandalone LLM: The effect is usually limited to changing the text output. LLM with tools: Data can be exfiltrated through API calls. Agents with broad credentials: There is risk of unauthorized actions in external systems. The real risk is not only wrong content, but privilege escalation in distributed systems.\n3. Training Data Extraction and Memorization The question of memorization became concrete when it was shown that large models can reproduce rare sequences that appeared in their training data [3].\nIt helps to separate three types of attacks that are often mixed up:\nMembership inference: Estimates whether a specific piece of data was in the training set. Training data extraction: Directly extracts memorized sequences through adaptive generation. Memorization elicitation: Makes the model reveal rare passages through statistical probing. The study showed that large-scale extraction is possible, not only probabilistic inference [3]. This conflicts with the idea of “privacy by design,” especially for models trained on scraped web data.\n4. Behavioral Backdoors and Sleeper Agents Some hoped that safety alignment would remove all malicious behavior. Experiments tested this.\nIt was shown that models can keep behavioral backdoors even after safety fine-tuning with SFT and RLHF [4]. The model learns to trigger bad behavior only under a specific condition and to look safe during evaluation.\nThese experiments were done in controlled settings. There is no agreement that the same level of persistence happens in real industrial pipelines with extensive red teaming and other alignment techniques.\nStill, the result matters: alignment does not formally guarantee that hidden behavior is removed.\n5. Supply Chain and Tool Abuse Recent work suggests that the main risk may not be the model alone, but how it is integrated into larger systems [5].\nTwo vectors are especially important:\nModel poisoning and compromised provenance: Especially when using third-party fine-tuning or unaudited checkpoints. Tool abuse: Agents with access to internal APIs, databases, or financial systems act under probabilistic control. In corporate environments, the potential impact of tool abuse often exceeds the impact of jailbreak alone.\nConclusion Research between 2021 and 2025 points to a few main ideas:\nJailbreak is a mathematical optimization problem. RAG introduces structural context contamination. LLMs can memorize sensitive data. Backdoors can survive alignment. Integration with tools greatly expands the attack surface. LLM security is not just an extension of traditional AppSec, and it cannot be solved only with prompt engineering. It is a challenge of architecture, data governance, and privilege control in integrated probabilistic systems.\nReferences [1] Zou, A., Wang, Z., Kolter, J. Z., \u0026amp; Fredrikson, M. (2023). Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043.\n[2] Greshake, K., et al. (2023). Not What You\u0026rsquo;ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security.\n[3] Carlini, N., et al. (2021). Extracting Training Data from Large Language Models. USENIX Security Symposium.\n[4] Hubinger, E., et al. (2024). Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. arXiv:2401.05566.\n[5] Yao, Y., et al. (2024). A Survey on Large Language Model Security and Privacy: The Good, the Bad, and the Ugly. High-Confidence Computing.\n","permalink":"http://localhost:1313/posts/llm-security-scientific-taxonomy-attack-vectors-2021-2025/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\u003c/p\u003e\n\u003cp\u003eThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\u003c/p\u003e","title":"LLM Security: A Scientific Taxonomy of Attack Vectors"},{"content":"ArgusScan: Automating Ethical Pentest with Shodan API GitHub Repository: https://github.com/digenaldo/argusscan\nIntroduction Security professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\nWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\nThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\nWhat is ArgusScan? ArgusScan is a professional pentest automation tool that uses Shodan API to search for vulnerable hosts and exposed services. It\u0026rsquo;s designed for security professionals who need to perform authorized security assessments quickly and generate professional reports.\nThe tool is built with Python 3.8+ and provides a rich command-line interface that makes it easy to search for specific services, filter results, and export data in multiple formats.\nKey Characteristics:\nProfessional CLI with colorful output using the Rich library Multiple export formats: Markdown, JSON, CSV Advanced filtering options Professional report generation following PTES/OWASP standards Rate limiting to respect API limits Integration with popular security tools Key Features Shodan API Integration ArgusScan fully integrates with Shodan API, allowing you to search for vulnerable hosts using Shodan search queries (also known as \u0026ldquo;dorks\u0026rdquo;). You can search by service, port, country, organization, and many other criteria.\nThe tool handles API authentication, rate limiting, and error handling automatically, so you can focus on your security assessment.\nProfessional CLI Interface The command-line interface uses the Rich library to provide colorful, organized output. This makes it easy to read results and understand what the tool is doing. The interface shows progress, results, and any errors clearly.\nMultiple Export Formats ArgusScan supports three export formats:\nMarkdown: Perfect for documentation and reports JSON: Ideal for integration with other tools and automation CSV: Great for spreadsheet analysis and data processing This flexibility allows you to use ArgusScan results in different workflows and integrate them with other security tools.\nAdvanced Filtering You can filter Shodan results by:\nCountry code (e.g., country:BR for Brazil) Port number (e.g., port:8080) Organization name And many other Shodan search parameters This helps you narrow down results to specific targets or regions, making your assessment more focused and efficient.\nProfessional Reports ArgusScan generates reports that follow industry standards like PTES (Penetration Testing Execution Standard) and OWASP methodologies. These reports are suitable for professional pentest engagements and can be included in your final deliverables.\nRate Limiting The tool respects Shodan API rate limits (1 request per second) automatically. This prevents API errors and ensures you stay within your plan\u0026rsquo;s limits, whether you\u0026rsquo;re using a free or paid Shodan account.\nTool Integration ArgusScan is designed to work with popular security tools:\nNuclei: Use results to run vulnerability scans OWASP ZAP: Import targets for automated security testing Metasploit: Use discovered services as targets for exploitation The JSON export format makes it easy to integrate with these and other tools.\nInstallation and Setup Installing ArgusScan is straightforward:\n# Clone the repository git clone https://github.com/digenaldo/argusscan.git cd argusscan # Install dependencies pip install -r requirements.txt # Run the tool python argus_scan.py --help You\u0026rsquo;ll need a Shodan API key, which you can get from Shodan.io. Free accounts have some limitations, but they work with ArgusScan for basic searches.\nUsage Examples Here are practical examples of how to use ArgusScan:\nSearch for Jenkins Instances python argus_scan.py \u0026#34;jenkins port:8080\u0026#34; --token YOUR_API_KEY This searches for Jenkins instances running on port 8080. Jenkins is often misconfigured and can be a security risk if exposed publicly.\nThe output shows detailed information about discovered hosts, including IP addresses, ports, organizations, products, and identified vulnerabilities (CVEs). This makes it easy to identify potential targets and understand their security posture.\nSearch Apache Servers in Brazil python argus_scan.py \u0026#34;apache country:BR\u0026#34; --token YOUR_API_KEY This finds Apache web servers located in Brazil. You can use country filters to focus on specific regions.\nExport Results to JSON python argus_scan.py \u0026#34;jenkins\u0026#34; --token YOUR_API_KEY --export json This exports results in JSON format, which you can use with other tools or scripts for further analysis.\nGenerate Professional Report python argus_scan.py \u0026#34;apache\u0026#34; --token YOUR_API_KEY --export markdown --report This generates a professional markdown report following PTES/OWASP standards, suitable for client deliverables.\nTechnical Excellence ArgusScan is built with quality in mind:\nComprehensive Testing:\n25 unit tests covering all major functionality 96% code coverage ensuring reliability Automated testing with GitHub Actions CI/CD pipeline Code Quality:\nPre-commit hooks for quality assurance Modern Python 3.8+ implementation Clean, maintainable code structure Professional Development Practices:\nVersion control with Git Issue tracking and pull request workflow Documentation and examples This level of quality ensures the tool is reliable and suitable for professional use.\nIntegration with Pentest Workflows ArgusScan fits naturally into professional pentest workflows:\nReconnaissance Phase: Use ArgusScan to identify exposed services and potential targets before starting detailed testing.\nTool Integration: Export results to JSON and import them into Nuclei, OWASP ZAP, or Metasploit for automated vulnerability scanning and exploitation.\nReporting: Generate professional reports that can be included in your final pentest deliverables, following PTES and OWASP standards.\nDocumentation: Use Markdown exports to document findings and create detailed reports for clients.\nLegal and Ethical Considerations This is extremely important: ArgusScan should only be used with proper authorization.\nMandatory Requirements:\nWritten authorization (Rules of Engagement - RoE) before any scanning Only scan systems you own or have explicit permission to test Follow responsible disclosure practices Comply with all applicable laws and regulations Authorized Use Cases:\nBug bounty programs with written authorization Professional pentest engagements with signed RoE Academic security research with proper approvals Defensive security assessments of your own infrastructure Red team exercises with proper authorization Unauthorized Use:\nNever scan systems without permission Never use for malicious purposes Never violate terms of service Never break laws or regulations Security tools are powerful, and with that power comes responsibility. Always use ArgusScan ethically and legally.\nUse Cases ArgusScan is useful in several scenarios:\nAuthorized Bug Bounty Programs: When you have written authorization to test specific targets, ArgusScan helps you find exposed services quickly.\nProfessional Pentest Engagements: Use it during the reconnaissance phase to identify potential targets and attack surfaces before detailed testing.\nAcademic Security Research: Researchers can use it to study internet exposure and security trends, always with proper authorization.\nDefensive Security Assessments: Organizations can use it to assess their own infrastructure and identify misconfigurations or exposed services.\nRed Team Exercises: Red teams can use it to find targets within authorized scope during exercises.\nFuture Roadmap The project is actively maintained, and future enhancements may include:\nSupport for additional export formats Integration with more security tools Enhanced filtering options Web interface for easier use Additional report templates Performance improvements Contributions and feedback are welcome on the GitHub repository.\nConclusion ArgusScan is a powerful tool for ethical security professionals who need to automate reconnaissance during authorized pentest engagements. With its professional CLI, multiple export formats, and high code quality, it\u0026rsquo;s designed to make security assessments more efficient while maintaining professional standards.\nThe tool demonstrates that open-source security tools can be both powerful and well-built, with comprehensive testing and professional development practices.\nKey Takeaways:\nAutomates Shodan searches for pentest reconnaissance Professional reports following PTES/OWASP standards 96% test coverage and CI/CD pipeline Integrates with popular security tools Always use with proper authorization If you\u0026rsquo;re a security professional looking to improve your pentest workflow, consider trying ArgusScan. Star the repository on GitHub, try it out (with proper authorization), and contribute improvements if you find it useful.\nRemember: Security tools are powerful, but they must be used responsibly and ethically. Always get written authorization before scanning any systems.\nGet Started:\nGitHub Repository: https://github.com/digenaldo/argusscan Read the documentation Try it with your Shodan API key Contribute and share feedback Use responsibly, and happy ethical hacking!\n","permalink":"http://localhost:1313/posts/argusscan-ethical-pentest-automation-with-shodan/","summary":"\u003ch1 id=\"argusscan-automating-ethical-pentest-with-shodan-api\"\u003eArgusScan: Automating Ethical Pentest with Shodan API\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eGitHub Repository:\u003c/strong\u003e \u003ca href=\"https://github.com/digenaldo/argusscan\"\u003ehttps://github.com/digenaldo/argusscan\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\u003c/p\u003e\n\u003cp\u003eWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\u003c/p\u003e\n\u003cp\u003eThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\u003c/p\u003e","title":"ArgusScan: Automating Ethical Pentest with Shodan API"},{"content":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus Introduction Observability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\nI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\nThis article focuses on the actual results and what the metrics tell us about the performance differences between these two technologies.\nThe Setup The Monitoring Lab includes two backend applications running the same workload:\nA Go application that performs MongoDB operations every 5 seconds A Spring Boot application performing the same operations at the same interval Prometheus collecting metrics from both applications Grafana dashboards showing real-time performance data Both applications execute similar operations: they ping MongoDB, insert documents, and record metrics about their performance. The key difference is the technology stack each one uses.\nPerformance Metrics Analysis Go Application Performance The Go application shows consistent and efficient performance across all metrics:\nCPU Usage: The Go app maintains very low CPU usage at around 0.22%. This shows the application is lightweight and doesn\u0026rsquo;t require much processing power for these operations.\nMemory Usage: Memory consumption stays stable at approximately 21.6 MiB. This is a small amount of memory, showing that Go applications can be very memory-efficient.\nMongoDB Latency: The latency metrics show excellent performance:\nAverage latency stays between 0.5 ms and 1.2 ms P95 latency (95th percentile) remains consistently around 4.5 ms P99 latency (99th percentile) stays around 5 ms These numbers mean that most operations complete very quickly, and even the slowest operations are still fast.\nOperations Rate: The application maintains a steady rate of 0.3 operations per second, which matches the expected interval of one operation every 5 seconds.\nSpring Boot Application Performance The Spring Boot application shows different characteristics:\nCPU Usage: Similar to Go, CPU usage is low at around 0.28%. Both applications handle the workload without stressing the CPU.\nMemory Usage: This is where we see a significant difference. The Spring Boot application uses 112 MiB of memory, which is more than 5 times the memory used by the Go application. This higher memory usage is typical for Java applications due to the JVM overhead.\nMongoDB Latency: The latency metrics show more variation:\nAverage latency fluctuates between 10 ms and 15 ms P95 latency follows similar patterns to the average P99 latency ranges from 15 ms to 25 ms, with spikes reaching nearly 30 ms The latency is consistently higher than the Go application, which suggests the Spring Boot application takes longer to complete the same operations.\nOperations Rate: The application maintains 0.4 operations per second, slightly higher than the Go application.\nJVM Heap Memory: The dashboard shows how the Java Virtual Machine manages memory:\nG1 Eden Space fills up and gets cleared during garbage collection cycles G1 Old Gen gradually increases over time as objects are promoted G1 Survivor Space remains minimal This memory management pattern is normal for Java applications but adds overhead that Go applications don\u0026rsquo;t have.\nDirect Comparison The comparison dashboard makes the differences clear:\nLatency Comparison:\nGo maintains average latency below 2.5 ms throughout the entire period Spring Boot shows average latency ranging from 6 ms to 15 ms Go\u0026rsquo;s latency is consistently 3 to 6 times lower than Spring Boot\u0026rsquo;s Memory Comparison:\nGo uses between 16 MiB and 24 MiB consistently Spring Boot starts at 112 MiB and fluctuates between 80 MiB and 104 MiB Spring Boot uses approximately 4 to 7 times more memory than Go These results show that for this specific workload, the Go application performs better in terms of both latency and memory usage.\nKey Metrics Explained Understanding these metrics helps you make informed decisions about which technology to use:\nCPU Usage: Shows how much processing power the application needs. Lower is better, as it leaves more resources for other processes.\nMemory Usage: Indicates how much RAM the application consumes. Lower memory usage means you can run more instances on the same server.\nAverage Latency: The typical time it takes for an operation to complete. Lower latency means faster responses to users.\nP95 and P99 Latency: These percentiles show how the slowest operations perform. P95 means 95% of operations are faster than this value. P99 means 99% of operations are faster. These metrics help you understand worst-case performance.\nOperations Rate: How many operations the application completes per second. This helps you understand throughput.\nWhat These Results Mean The metrics show clear patterns:\nGo excels at low latency: The Go application consistently completes operations faster, with latency staying under 2.5 ms.\nGo uses less memory: With memory usage around 21 MiB, Go applications can run more instances on the same hardware.\nSpring Boot has higher overhead: The JVM and framework layers add overhead, resulting in higher latency and memory usage.\nBoth handle the workload: Neither application struggles with the workload, showing that both technologies can handle this type of operation effectively.\nIt\u0026rsquo;s important to remember that these results are specific to this test scenario. Different workloads, different operations, or different configurations might produce different results.\nReal-World Implications These metrics matter in production environments:\nLower latency means users get faster responses, which improves user experience. For applications handling many requests, even small latency differences can add up significantly.\nLower memory usage means you can run more application instances on the same server, which can reduce infrastructure costs. It also means applications start faster and use fewer resources.\nConsistent performance is important for reliability. When latency varies widely, it\u0026rsquo;s harder to predict how the application will behave under load.\nConclusion The Monitoring Lab provides real data about how Go and Spring Boot applications perform. The results show that for this specific workload:\nGo offers better latency performance and lower memory usage Spring Boot provides easier development through frameworks and tools, but with higher resource usage Both technologies can handle the workload effectively The choice between technologies depends on your specific needs. If low latency and low memory usage are priorities, Go might be a better fit. If rapid development and a rich ecosystem are more important, Spring Boot might be the right choice.\nObservability tools like Prometheus and Grafana make it possible to see these differences clearly. Without metrics, you\u0026rsquo;re making decisions based on assumptions. With metrics, you can make decisions based on data.\nThe code and dashboards are available on GitHub: https://github.com/digenaldo/monitoring-lab\n","permalink":"http://localhost:1313/posts/monitoring-lab-complete-observability-with-go-spring-boot-prometheus/","summary":"\u003ch1 id=\"monitoring-lab-a-complete-observability-lab-with-go-spring-boot-and-prometheus\"\u003eMonitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eObservability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\u003c/p\u003e\n\u003cp\u003eI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\u003c/p\u003e","title":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus"}]