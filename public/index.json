[{"content":"Introduction Security in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\nThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\nThis post organizes the main scientific results from this period into a technical taxonomy. It is based on how attacks work, not on how the media talks about them.\nFigure: Overview of the main attack vectors covered in this post: adversarial jailbreak, training data extraction, sleeper agents, supply chain and tool abuse, and indirect prompt injection. The center shows how a deployed LLM (Large Language Model) application (model, APIs, retrieval) becomes the target of these threats.\n1. Adversarial Attacks and Transferable Jailbreaks Many people think of jailbreaks as just clever prompt writing. Research has shown there is more to it.\nStudies showed that adversarial suffixes can be generated automatically by gradient optimization. These suffixes can make aligned models break safety rules [1]. They push the model into parts of its internal space where forbidden answers become more likely.\nFigure: Two-phase process of GCG (Greedy Coordinate Gradient) attacks: optimization of an adversarial suffix on a white-box model, then transfer of that suffix to a black-box model to get forbidden answers. Source: Zou et al. [1].\nHow the GCG attack works: from optimization to exploitation\nThe GCG (Greedy Coordinate Gradient) attack works like building a mathematical “master key” that uses the fact that different AIs are built in similar ways. It starts in the optimization phase (white-box). Attackers use open-source models (such as Llama and Vicuna) to compute an adversarial suffix. This suffix is a string of characters that may look random to humans but is optimized with gradients so that the model starts its answer with something like “Sure, here is how to…”.\nUnlike human “gaslighting” (where you try to talk the AI into being harmful), the suffix is a digital “master key”: a string such as ! ? @ # or other seemingly random character sequences that exploit the model’s gradients. The attack does not try to persuade the model; it uses math to push it into a bad state.\nThe main idea is multi-model optimization: the suffix is not tuned to fool just one AI but several at once. By finding a sequence that breaks the safety of many open models at the same time, attackers get a universal suffix that targets patterns that are shared by almost all Large Language Models (LLMs).\nThat leads to the transfer phase (black-box). Closed models (such as GPT-4 or PaLM-2) use the same basic architecture (Transformer) and are trained on data similar to the open models. So they end up sharing the same kinds of weaknesses. When this universal suffix is sent to a closed model, it creates a kind of “mathematical pressure” that safety filters do not recognize as a threat.\nOnce the suffix makes the closed model produce the first tokens of a forbidden answer, control is lost: the chance that the model will keep generating harmful text goes up a lot, and it can ignore its original safety rules. In the end, the attack works because what was learned on open models also applies to how these architectures process tokens.\nSo the main finding is transferability. Attacks that were optimized for one open-source model often worked on closed or commercial models trained with similar pipelines [1]. This is not only because of the Transformer architecture. Transfer happens because training goals and alignment methods (especially RLHF (Reinforcement Learning from Human Feedback)) are similar. It is a weakness in the whole system when models are aligned in similar ways.\nPractical takeaway: Defenses that only use word filters or fixed rules are weak against optimization-based attacks.\nFigure: Effectiveness of GCG (Gradient-based Certifiably Good) attacks when optimized on open-source models (white-box) and then transferred to closed models (black-box) without access to their internals. On open models (e.g. Vicuna-7B, Llama-2-7B-Chat) effectiveness is very high; the same attack still reaches high rates on GPT-3.5, PaLM-2 and GPT-4. Source: Zou et al. [1].\n2. Indirect Prompt Injection and Context Contamination With the rise of RAG (Retrieval-Augmented Generation) and autonomous agents, the attack surface has changed.\nIndirect Prompt Injection was formalized as a real attack vector against applications that use LLMs [2]. The core issue is that the model does not clearly separate “trusted instructions” from “external retrieved data.” Both are just tokens in the same context.\nThis is an instance of the classic “confused deputy” problem applied to generative models.\nFigure: Indirect prompt injection: malicious content in retrieved data can change how the model behaves. Source: Greshake et al. [2].\nHow the attack works\nThe attacker does not send the malicious text directly to the model. Instead, they hide instructions inside data that the app later fetches (for example, text from a web page or a document). When the app uses RAG (Retrieval-Augmented Generation) or similar tools, it puts that data into the same context as the user\u0026rsquo;s question and the system\u0026rsquo;s own instructions. The model sees everything as one block of text and cannot tell which part is \u0026ldquo;safe\u0026rdquo; and which part is controlled by the attacker. So it may follow the hidden instructions and change its answer, call an API (Application Programming Interface), or leak data. How bad that gets depends on the setup:\nStandalone LLM (Large Language Model): The effect is usually limited to changing the text output. LLM (Large Language Model) with tools: Data can be exfiltrated through API (Application Programming Interface) calls. Agents with broad credentials: There is risk of unauthorized actions in external systems. Figure: Experimental results from real-world LLM (Large Language Model) applications showing the success of indirect prompt injection across different scenarios. Source: Greshake et al. [2].\nThe real risk is not only wrong content, but privilege escalation in distributed systems.\n3. Training Data Extraction and Memorization The question of memorization became concrete when it was shown that large models can reproduce rare sequences that appeared in their training data [3].\nIt helps to separate three types of attacks that are often mixed up:\nMembership inference: Estimates whether a specific piece of data was in the training set. Training data extraction: Directly extracts memorized sequences through adaptive generation. Memorization elicitation: Makes the model reveal rare passages through statistical probing. The study showed that large-scale extraction is possible, not only probabilistic inference [3]. This conflicts with the idea of “privacy by design,” especially for models trained on scraped web data.\nHow Training Data Extraction Attacks Work\nTraining data extraction is an attack at inference time. It uses the fact that large language models sometimes memorize rare or unique text from their training data and can repeat it word for word. The attacker does not have the training data; they only query the model (black-box) and try to get back real fragments of that data.\nFigure: The extraction flow: LLM (Large Language Model) trained on web-scale data, adaptive probing by the attacker, then verbatim reproduction of rare memorized sequences. Rare sequences are more likely to be memorized. Source: Carlini et al. [3].\nWhy memorization happens. Models are trained to predict the next token well. When they see very common patterns, they learn to generalize. When they see something rare or unique (for example an API key, a phone number, or a unique code snippet), they may memorize it instead, because that is the easiest way to reduce loss. Carlini et al. show that bigger models tend to memorize more of these rare sequences.\nWhat the attacker wants. The attacker wants to recover training data that is rare, unique, and possibly sensitive. They do not know the data in advance; they only have access to the model\u0026rsquo;s answers.\nHow extraction works (in simple steps).\nPrompt seeding: The attacker gives a prefix that is likely to appear in training data (e.g. start of a sentence, or a common log format). This pushes the model toward parts of its \u0026ldquo;memory\u0026rdquo; where it might continue with a memorized sequence. Sampling at scale: The attacker asks the model for a very large number of completions (with different settings like temperature or sampling). The goal is not creative text but to cover many possible continuations. Filtering and ranking: From all outputs, the attacker looks for sequences that look non-generic: low entropy, long, or with structure (e.g. like private data). Verification: When possible, they check if the extracted text really appeared in known datasets. The paper shows that models can reproduce training data verbatim, not just similar, but the same. Do not mix this up with membership inference. Membership inference only answers: \u0026ldquo;Was this exact item in the training set?\u0026rdquo; Training data extraction goes further: it recovers the memorized sequence. So extraction is a stronger attack.\nWhy bigger models are more at risk. The paper shows that larger models memorize more. So as models grow, both generalization and memorization can increase. Rare sequences are especially at risk.\nWhat this means for security. The attack shows that (1) training on scraped web data creates real privacy risk, (2) removing obvious personal data is not enough, (3) fine-tuning does not always remove memorized content, and (4) black-box access is enough to run the attack. The problem is structural: it comes from model size, exposure to rare sequences, and the training objective.\nMain takeaway. The important point is not only that memorization exists, but that it can be extracted at scale. Carlini et al. extracted hundreds of unique memorized sequences from large models. That turns a theoretical privacy concern into a real attack.\nFigure: Overview of training data extraction and memorization risks in LLMs (Large Language Models). Source: Carlini et al. [3].\n4. Behavioral Backdoors and Sleeper Agents Some hoped that safety alignment would remove all malicious behavior. Experiments tested this.\nIt was shown that models can keep behavioral backdoors even after safety fine-tuning with SFT (Supervised Fine-Tuning) and RLHF (Reinforcement Learning from Human Feedback) [4]. The model learns to trigger bad behavior only under a specific condition and to look safe during evaluation. These experiments were done in controlled settings. There is no agreement that the same level of persistence happens in real industrial pipelines with extensive red teaming and other alignment techniques. Still, the result matters: alignment does not formally guarantee that hidden behavior is removed.\nHow Sleeper Agent Attacks Work\nA sleeper agent attack is when a language model is trained to behave normally most of the time, but to act in a harmful way when a specific trigger appears. The model looks safe during testing, but it keeps hidden behavior that turns on only under certain conditions.\nFigure: From backdoor training to deployment: the model learns to behave well without the trigger and to switch to harmful behavior when the trigger appears. Source: Hubinger et al. [4].\n1. Training the backdoor. The model is first trained with special data. It learns two behaviors: if there is no trigger, it gives safe answers; if a specific trigger appears, it gives harmful or policy-breaking answers. The trigger can be a special phrase, a hidden keyword, a date, or a context condition. The model learns to link the trigger with the bad behavior.\n2. Safety alignment phase. After training, the model goes through safety alignment (e.g. SFT (Supervised Fine-Tuning) and RLHF (Reinforcement Learning from Human Feedback)). During evaluation, testers use normal prompts. Because the trigger is not present, the model behaves safely and looks aligned.\n3. Deployment phase. After deployment, if a user provides the trigger, the model switches behavior and activates the hidden harmful response. So the model was not truly fixed: it learned to hide its bad behavior.\n4. Why this is dangerous. The main risk is that alignment training may not remove the backdoor. The model learns: “When I see evaluation patterns, behave well; when I see the trigger, activate harmful mode.” That creates a lasting hidden risk inside the model. The paper shows that some backdoors survive even after safety fine-tuning [4].\nFigure: Evidence that backdoors can persist after safety training. Source: Hubinger et al. [4].\n5. Important limitation. The experiments were done in controlled environments. It is not proven that all production models behave the same way. However, the results show that alignment alone does not guarantee that hidden behavior is removed.\n5. Supply Chain and Tool Abuse Recent work suggests that the main risk may not be the model alone, but how it is integrated into larger systems [5]. Two vectors are especially important: model poisoning and compromised provenance (e.g. third-party fine-tuning or unaudited checkpoints), and tool abuse (agents with access to internal APIs (Application Programming Interfaces), databases, or financial systems acting under probabilistic control). In corporate environments, the potential impact of tool abuse often exceeds the impact of jailbreak alone.\nHow Supply Chain Attacks and Tool Abuse Work\nTopic 5 is different from the other attacks. Here the model itself may not be attacked directly at inference time. Instead, the risk comes from the model’s origin, the training pipeline, and the tools connected to the model. The problem is architectural and systemic.\nFigure: From supply chain (model origin, fine-tuning, checkpoints) to tool integration (APIs, databases, systems). Source: Yao et al. [5].\nPart 1: Supply chain compromise\nWhere the risk begins. Modern LLM (Large Language Model) systems often use open-source base models, third-party fine-tuned checkpoints, external datasets, or external fine-tuning providers. If any part of this chain is compromised, the model can contain hidden behavior. Examples: malicious fine-tuning data, a backdoor inserted during training, or an altered model checkpoint. The organization may deploy the model without knowing it contains hidden triggers.\nWhy this is dangerous. Unlike prompt injection, this attack does not depend on user input. The malicious behavior is embedded in the model weights. Even safety alignment may not fully remove it. This creates a long-term persistent risk.\nPart 2: Tool abuse\nThe integration problem. Modern LLM (Large Language Model) systems often have access to APIs (Application Programming Interfaces), databases, cloud services, email systems, or financial systems. The model can decide when to call these tools, so the model has real operational power.\nHow abuse happens. If the model is compromised, is manipulated via prompt injection, or behaves unpredictably, it may trigger actions such as sending sensitive data, modifying records, or executing transactions. The attacker does not need system credentials; they only need to influence the model’s reasoning.\nRisk amplification. Risk increases with integration complexity, privilege scope, and automation level. A standalone model only produces text. An agent with credentials can change real systems. The main risk is privilege escalation in distributed systems.\nFigure: Overview of supply chain and tool abuse risks. Source: Yao et al. [5].\nConclusion Research between 2021 and 2025 points to a few main ideas:\nJailbreak is a mathematical optimization problem. RAG (Retrieval-Augmented Generation) introduces structural context contamination. LLMs (Large Language Models) can memorize sensitive data. Backdoors can survive alignment. Integration with tools greatly expands the attack surface. LLM (Large Language Model) security is not just an extension of traditional AppSec (Application Security), and it cannot be solved only with prompt engineering. It is a challenge of architecture, data governance, and privilege control in integrated probabilistic systems.\nReferences [1] Zou, A., Wang, Z., Kolter, J. Z., \u0026amp; Fredrikson, M. (2023). Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043.\n[2] Greshake, K., et al. (2023). Not What You\u0026rsquo;ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security.\n[3] Carlini, N., et al. (2021). Extracting Training Data from Large Language Models. USENIX Security Symposium.\n[4] Hubinger, E., et al. (2024). Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. arXiv:2401.05566.\n[5] Yao, Y., et al. (2024). A Survey on Large Language Model Security and Privacy: The Good, the Bad, and the Ugly. High-Confidence Computing.\n","permalink":"https://digenaldo.com/posts/llm-security-scientific-taxonomy-attack-vectors-2021-2025/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\u003c/p\u003e\n\u003cp\u003eThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\u003c/p\u003e","title":"LLM Security: A Scientific Taxonomy of Attack Vectors"},{"content":"1. Fundamental Architecture: The Transformer The foundation of modern LLMs (Large Language Models) is the Transformer architecture [1].\nUnlike RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term Memory networks), which process text one step after another, the Transformer processes the entire sequence in parallel. This allows better modeling of long-range dependencies and faster training [1].\nFigure: Flow of the Transformer architecture (attention, encoder/decoder, feed-forward). Source: Vaswani et al. [1].\nHow it works in simple terms. The diagram above shows the path the data follows:\nThe input text is turned into vectors (embeddings). Those vectors enter a stack of layers. In each layer, three things happen in order: Self-attention: looks at all the tokens together and decides how much each word should “attend to” the others, so the model captures relationships in the sentence. Encoder (if present) or decoder: processes this attended representation. Feed-forward: refines each token position on its own. The output of one layer becomes the input of the next. After several such layers, the model has a rich representation of the whole sequence and uses it to predict the next token. In short: input → attention (who relates to whom) → encoder/decoder → feed-forward → repeat over layers → final representation for prediction.\nMain components:\nSelf-attention mechanism. Self-attention is the core of the Transformer [1]. It lets the model assign different importance weights to words in a sequence when making predictions. For example, in “The cat sat on the mat because it was tired,” the model can learn that “it” refers to “the cat.” In mathematical form, attention is:\nAttention(Q, K, V) = softmax(QKᵀ / √d_k) V\nA more detailed mathematical explanation is given in [2].\nEncoder and decoder. The original Transformer has both encoder and decoder stacks [1]. The encoder processes the input; the decoder produces the output. Many generative models, such as GPT (Generative Pre-trained Transformer), use only the decoder [3].\nFeed-forward networks. After the attention block, each token is processed by a position-wise fully connected feed-forward network [1].\n2. How the Model Processes Information Before the model can compute anything, text must be turned into numbers.\nFigure: From user input to next-token prediction (tokens, embeddings, positional encoding, self-attention, context vector).\nHow it works in simple terms. The diagram above shows the path from the user’s question to the first word of the answer:\nUser input. The user types a question (e.g. \u0026ldquo;What are the symptoms of dengue fever?\u0026rdquo;). The text is split into input tokens: each word or punctuation becomes a token, e.g. [What] [are] [the] [symptoms] [of] [dengue] [fever] [?]. Token embeddings. Each token is turned into a list of numbers (a vector). For example [What] becomes a vector like [0.1, 0.5, \u0026hellip;]. This is the step where words become a form the model can process [1]. Positional encoding. The model needs to know the order of the words. Positional encoding adds numbers that tell the model that \u0026ldquo;What\u0026rdquo; is the 1st word, \u0026ldquo;are\u0026rdquo; is the 2nd, and so on. So the model knows that \u0026ldquo;dengue fever\u0026rdquo; and \u0026ldquo;symptoms\u0026rdquo; are related in the right order [1]. Self-attention. Inside the self-attention layers, each token \u0026ldquo;looks at\u0026rdquo; the others to see how they relate. For this question, \u0026ldquo;symptoms\u0026rdquo; pays more attention to \u0026ldquo;dengue\u0026rdquo; and \u0026ldquo;fever\u0026rdquo; than to \u0026ldquo;what\u0026rdquo; or \u0026ldquo;are.\u0026rdquo; The model uses Query (Q), Key (K), and Value (V) to decide how much attention each word gives to the others. The result is a contextual vector representation: one representation that encodes the meaning of the whole question [1]. Context vector. The output of attention is a single high-dimensional vector that holds the model’s \u0026ldquo;understanding\u0026rdquo; of the question. This is the context representation used for the next step. Next token prediction. With the full context, the model predicts the next token. It assigns probabilities to many possible next words (e.g. high for [Common], [Symptoms], lower for others) and then picks one (e.g. [Common]) according to its sampling strategy. This is where the answer starts. The model always generates one token at a time [1]. Embeddings. Tokens are converted into high-dimensional vectors. In this space, words with similar meanings sit closer together [1].\nPositional encoding. The Transformer does not process tokens in order. So it needs extra information about the position of each token. Positional encodings add that order information [1].\nQueries, keys, and values (Q, K, V). For each token, the model builds three vectors: Query, Key, and Value [1]. The Query of one token is compared to the Keys of other tokens to get attention weights. The Values are then combined using these weights. This is how the model learns relationships between words.\n3. Training Phases Pre-training. Large language models are trained on very large text corpora using next-token prediction [3]. The goal is to model the probability of the sequence:\nP(x) = ∏ P(x_t | x_\u0026lt;t)\nSo the model learns to predict each token given all previous tokens. From this, it learns grammar, structure, and factual associations. The training loss is usually cross-entropy.\nFigure: Training phases from data to a trained model (dataset, pre-training, loss, scaling laws, in-context learning).\nHow it works in simple terms. The diagram above shows how the model learns from data and gets better over time:\nLarge text dataset. Training starts with a huge amount of text: books, websites, articles, and code. This is the raw material the model will learn from [3]. Pre-training. The model is trained to predict the next token. For example, given \u0026ldquo;The cat sat on the mat\u0026rdquo;, it learns to predict a likely next word (e.g. \u0026ldquo;because\u0026rdquo; or \u0026ldquo;and\u0026rdquo;). By doing this over and over on billions of tokens, it learns grammar, facts, and how words relate [3]. Loss calculation. For each prediction, the model compares what it predicted with the real next word. When the prediction is wrong, a loss (error) is computed. The training process updates the model so that this loss decreases over time. The graph of loss vs. time goes down as the model improves [3]. Scaling laws. Performance improves in a predictable way when we add more data, more parameters (bigger model), and more compute (more computing power). This follows a power-law trend: better performance comes from scaling these three factors [4]. In-context learning. After pre-training, the model can learn new tasks from examples in the prompt without changing its weights. For instance, if you show \u0026ldquo;Translate to French: Hello → Bonjour\u0026rdquo;, the model can do more translations. No extra training step is required; the ability emerges from scale [3]. Trained language model. At the end, the model is ready for inference. All the knowledge it learned is stored in the neural network weights. When a user asks a question, the model uses these weights to generate answers [3]. Scaling laws. Model performance improves in a predictable way when we increase parameters, dataset size, and compute [4]. This follows a power-law relationship. Recent work also studies what happens when data growth is limited (data-constrained regimes) [5].\nFew-shot and in-context learning. Large models such as GPT-3 can do few-shot and zero-shot learning without updating their weights [3]. By putting examples or instructions in the prompt, the model solves new tasks. This ability emerges from scale and training, not from extra task-specific fine-tuning.\n4. Alignment and Refinement Raw pre-trained models can produce unsafe or unhelpful outputs. Alignment techniques are used to improve behavior.\nFigure: How models become safer and more helpful (human feedback, reward model, reinforcement learning).\nHow it works in simple terms. The diagram above shows the main steps of alignment:\nFrom the raw model to answers. The pre-trained model generates several answers to the same question. These answers are the starting point for human feedback [6]. Human feedback. People (annotators) read the model\u0026rsquo;s answers and rank them: for example, answer A is better than B, B is better than C. These ranked answers show what humans prefer: more helpful, safer, or more accurate responses [6]. Reward Model. A separate model, the Reward Model, is trained on these human rankings. It learns to score each answer with a number (e.g. +0.9 for a good answer, lower for worse ones). So \u0026ldquo;human preferences\u0026rdquo; are turned into a score the computer can use. This model is trained from human rankings only; it does not generate text [6]. Reinforcement learning (RLHF). The main LLM is then trained with reinforcement learning (often PPO, Proximal Policy Optimization) to maximize the score given by the Reward Model. When the LLM produces a good answer, the reward is high; when it produces a bad one, the reward is low. Over time, the LLM becomes more helpful and aligned with what humans want [6]. Aligned model. After this process, the model is refined: it is more likely to give safe, useful answers and less likely to give harmful or unhelpful ones. Alignment does not change what the model \u0026ldquo;knows\u0026rdquo; from pre-training; it changes how it behaves when it answers [6]. SFT (Supervised Fine-Tuning). The model is fine-tuned on high-quality human-written examples that show the desired behavior [6].\nRLHF (Reinforcement Learning from Human Feedback). The process in [6] works as follows. Human annotators rank model outputs. A Reward Model is trained from these rankings. Then reinforcement learning (typically PPO, Proximal Policy Optimization) is used to train the LLM to maximize that reward. This makes the model more helpful and aligned with user expectations.\nConstitutional AI (RLAIF). Constitutional AI replaces part of human feedback with AI-generated feedback guided by explicit principles (a “constitution”) [7]. The model critiques and improves its own outputs using these rules. This is Reinforcement Learning from AI Feedback (RLAIF). It allows scalable alignment with less human supervision.\n5. Advanced Techniques and Efficiency Figure: How modern models scale and improve performance (MoE, RAG, FlashAttention, hardware optimization).\nHow it works in simple terms. The diagram above shows how scaling challenges are addressed by combining several techniques:\nScaling challenge. Large models need a lot of memory and compute (many GPUs and chips). So efficiency becomes very important: we need ways to make models faster and cheaper to run [8][10]. Mixture of Experts (MoE). Instead of using the whole model for every token, only part of the model is active per token. A router sends each input token to a few experts (sub-networks); for example, only \u0026ldquo;Expert 2\u0026rdquo; might be active for one token. This is sparse computation: it allows models with trillions of parameters while keeping the cost per token manageable [8]. Retrieval-Augmented Generation (RAG). The model can retrieve external documents before answering. The user query is turned into a vector; a vector search finds relevant documents; then the LLM generates an answer using both the query and those documents. This reduces hallucination and lets the system use up-to-date knowledge without retraining [9]. FlashAttention. The attention mechanism uses a lot of GPU memory and compute. FlashAttention and similar methods optimize how attention runs on the GPU. They improve speed and reduce memory use, so larger models or longer sequences can run on the same hardware [10]. Hardware optimization. Modern systems are built for distributed GPU setups: many chips work in parallel. Data parallel and model parallel ways of splitting the work are used. So system and hardware engineering are part of model performance, not only the algorithm [10]. Modern LLM system. Together, architecture (e.g. MoE), retrieval (RAG), sparsity, and hardware optimization form a modern LLM system. Performance depends on both algorithms and infrastructure [8][9][10]. Mixture-of-Experts (MoE). In models such as Switch Transformers, only a subset of parameters is used per token [8]. This is called sparse computation. Models can scale to trillions of parameters while keeping computation per token manageable. The trade-off is more complex systems and the need for good load balancing.\nRAG (Retrieval-Augmented Generation). RAG combines a retrieval system with a language model to improve factual grounding [9]. The model retrieves relevant documents and conditions its generation on them. This can reduce hallucination and allow knowledge updates without retraining. It also introduces new risks, such as prompt injection and manipulation of retrieved data.\nFlashAttention and kernel optimization. Attention is very demanding in memory and compute. FlashAttention-2 and similar methods optimize attention for modern hardware [10]. For example, implementations on NVIDIA Hopper use CUDA kernel fusion and hardware-aware optimization to get large speedups. Scaling LLMs is therefore not only an algorithmic challenge but also a systems and hardware engineering problem [10].\n6. What Happens When a User Asks a Question? Understanding training is important. What matters in practice is what happens at inference time, when a user sends a question.\nExample: \u0026ldquo;What are the symptoms of dengue fever?\u0026rdquo;\nHere is what happens inside the LLM (Large Language Model).\nFigure: LLM inference flow step by step (tokenization, self-attention, next-token prediction, generation, knowledge source, RAG).\nHow it works in simple terms. The diagram above shows the path from the user\u0026rsquo;s question to the answer:\nUser input and tokenization. The question (e.g. \u0026ldquo;What are the symptoms of dengue fever?\u0026rdquo;) is turned into tokens and then into vectors. Each word or punctuation becomes a token; each token becomes a list of numbers the model can process. There is no internet search or database lookup here; only text becomes numbers [1]. Self-attention processing. The model builds context between the words. It uses Query (Q), Key (K), and Value (V) to see how tokens relate (e.g. \u0026ldquo;symptoms\u0026rdquo; to \u0026ldquo;dengue\u0026rdquo; and \u0026ldquo;fever\u0026rdquo;). This step uses only the prompt; again, no search and no lookup [1]. Probability calculation. The model computes P(next token | previous tokens). It assigns probabilities to possible next words (e.g. [Common] 0.42, [Symptoms] 0.33, [Include] 0.18) and picks one. The response starts here [1]. Token-by-token generation. Each new token is added to the context (e.g. \u0026ldquo;Common symptoms include\u0026hellip;\u0026rdquo;). The model updates the context, recomputes attention, and predicts the next token again. It always generates one token at a time until a stop condition [1]. Where does knowledge come from? The model uses patterns learned from training data (e.g. \u0026ldquo;dengue fever\u0026rdquo;, \u0026ldquo;mosquito-borne\u0026rdquo;, \u0026ldquo;high fever\u0026rdquo;, \u0026ldquo;joint pain\u0026rdquo;, \u0026ldquo;rash\u0026rdquo;). It predicts patterns, not verified facts; it does not check a database or guarantee correctness [3]. With RAG. If the system uses RAG (Retrieval-Augmented Generation) [9], the question is used for a vector search, external documents are retrieved, and the LLM gets both the question and those documents. So the answer can use outside knowledge and reduce reliance on training data alone. Step 1: Tokenization. The text is split into tokens, for example: [What] [are] [the] [symptoms] [of] [dengue] [fever] [?]. Each token is then converted into an embedding vector. At this stage the model is not searching the internet or a database; it only turns text into numbers.\nStep 2: Context processing with self-attention. The embeddings go through multiple Transformer layers. In each layer, self-attention computes relationships between tokens (e.g. that \u0026ldquo;symptoms\u0026rdquo; relates to \u0026ldquo;dengue fever\u0026rdquo;) and builds a contextual representation of the question. That representation is a high-dimensional vector that encodes the meaning of the whole prompt. The model is not retrieving a stored paragraph about dengue; it is computing a probability distribution over possible next tokens.\nStep 3: Probability calculation. After processing the input, the model predicts the most likely next token. It computes:\nP(next token | previous tokens)\nIt may assign high probability to tokens such as [Common], [Symptoms], [Dengue], [Fever], [Include], and then picks one token according to its sampling strategy. This is when the response actually starts.\nStep 4: Autoregressive generation. After the first token is generated, the process repeats. Each new token is added to the context (e.g. \u0026ldquo;What are the symptoms of dengue fever? Common symptoms include\u0026hellip;\u0026rdquo;). For each new token the model updates the context, recomputes attention, and predicts the next token. This continues until a stop condition is met. The model never produces the full answer in one go; it generates one token at a time.\nStep 5: Where does the information come from? The model does not \u0026ldquo;look up\u0026rdquo; dengue fever. The knowledge comes from patterns learned during pre-training. If the training data included medical texts and web content about dengue, the model learned statistical links between \u0026ldquo;dengue fever,\u0026rdquo; \u0026ldquo;mosquito-borne disease,\u0026rdquo; \u0026ldquo;high fever,\u0026rdquo; \u0026ldquo;joint pain,\u0026rdquo; \u0026ldquo;rash,\u0026rdquo; and similar phrases. At inference it recombines these patterns. It does not check facts, query medical databases, or guarantee correctness. It predicts what is statistically likely to follow.\nStep 6: What changes with RAG? If the system uses RAG (Retrieval-Augmented Generation) [9], the flow changes. Before generation, the question is turned into an embedding, a vector database is queried, and relevant documents (e.g. medical) are retrieved and added to the prompt. The model then generates text conditioned on both the question and the retrieved documents. So the system can use external knowledge. Without RAG, the model relies only on what was learned in training.\nImportant clarification. An LLM does not store structured medical knowledge like a database. It stores distributed representations in its weights. When it answers about a disease, it is activating learned patterns across billions of parameters. This is pattern completion, not structured reasoning or medical diagnosis. That distinction is important for safety, hallucination, and reliability.\nFinal Consideration A Large Language Model (LLM) is a large probabilistic system trained to predict tokens. Its capabilities come from:\nThe Transformer architecture [1] Massive scale (parameters, data, compute) and scaling laws [4], [5] Alignment (SFT, RLHF, RLAIF) [6], [7] Efficiency techniques (MoE, RAG, FlashAttention) [8], [9], [10] It does not have symbolic reasoning, persistent memory across sessions, or guaranteed correctness. Its behavior is statistical and learned from data.\nReferences [1] Vaswani, A., et al. (2017). Attention Is All You Need. Advances in Neural Information Processing Systems (NIPS 30).\n[2] Reflections. (2024, June 10). Mathematical details behind self-attention.\n[3] Brown, T. B., et al. (2020). Language Models are Few-Shot Learners. NeurIPS 33, 1877–1901.\n[4] Kaplan, J., et al. (2020). Scaling Laws for Neural Language Models. arXiv:2001.08361.\n[5] Muennighoff, N., et al. (2025). Scaling Data-Constrained Language Models. Journal of Machine Learning Research, 26, 1–66.\n[6] Ouyang, L., et al. (2022). Training language models to follow instructions with human feedback. NeurIPS 36.\n[7] Bai, Y., et al. (2022). Constitutional AI: Harmlessness from AI Feedback. arXiv preprint.\n[8] Fedus, W., Zoph, B., \u0026amp; Shazeer, N. (2022). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. Journal of Machine Learning Research, 23.\n[9] Lewis, P., et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. NeurIPS 33.\n[10] Bikshandi, G., \u0026amp; Shah, J. (2023). A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on NVIDIA Hopper Architecture using the CUTLASS Library. Colfax Research.\n","permalink":"https://digenaldo.com/posts/how-a-large-language-model-llm-works/","summary":"\u003ch2 id=\"1-fundamental-architecture-the-transformer\"\u003e1. Fundamental Architecture: The Transformer\u003c/h2\u003e\n\u003cp\u003eThe foundation of modern LLMs (Large Language Models) is the \u003cstrong\u003eTransformer\u003c/strong\u003e architecture [1].\u003c/p\u003e\n\u003cp\u003eUnlike RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term Memory networks), which process text one step after another, the Transformer processes the \u003cstrong\u003eentire sequence in parallel\u003c/strong\u003e. This allows better modeling of long-range dependencies and faster training [1].\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Transformer architecture flow\" loading=\"lazy\" src=\"/images/llm-works-flow-topic1.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eFigure: Flow of the Transformer architecture (attention, encoder/decoder, feed-forward). Source: Vaswani et al. [1].\u003c/em\u003e\u003c/p\u003e","title":"How a Large Language Model (LLM) Works"},{"content":"Introduction Security professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\nWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\nThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\nWhat is ArgusScan? ArgusScan is a professional pentest automation tool that uses Shodan API to search for vulnerable hosts and exposed services. It\u0026rsquo;s designed for security professionals who need to perform authorized security assessments quickly and generate professional reports.\nThe tool is built with Python 3.8+ and provides a rich command-line interface that makes it easy to search for specific services, filter results, and export data in multiple formats.\nKey Characteristics:\nProfessional CLI with colorful output using the Rich library Multiple export formats: Markdown, JSON, CSV Advanced filtering options Professional report generation following PTES/OWASP standards Rate limiting to respect API limits Integration with popular security tools Key Features Shodan API Integration ArgusScan fully integrates with Shodan API, allowing you to search for vulnerable hosts using Shodan search queries (also known as \u0026ldquo;dorks\u0026rdquo;). You can search by service, port, country, organization, and many other criteria.\nThe tool handles API authentication, rate limiting, and error handling automatically, so you can focus on your security assessment.\nProfessional CLI Interface The command-line interface uses the Rich library to provide colorful, organized output. This makes it easy to read results and understand what the tool is doing. The interface shows progress, results, and any errors clearly.\nMultiple Export Formats ArgusScan supports three export formats:\nMarkdown: Perfect for documentation and reports JSON: Ideal for integration with other tools and automation CSV: Great for spreadsheet analysis and data processing This flexibility allows you to use ArgusScan results in different workflows and integrate them with other security tools.\nAdvanced Filtering You can filter Shodan results by:\nCountry code (e.g., country:BR for Brazil) Port number (e.g., port:8080) Organization name And many other Shodan search parameters This helps you narrow down results to specific targets or regions, making your assessment more focused and efficient.\nProfessional Reports ArgusScan generates reports that follow industry standards like PTES (Penetration Testing Execution Standard) and OWASP methodologies. These reports are suitable for professional pentest engagements and can be included in your final deliverables.\nRate Limiting The tool respects Shodan API rate limits (1 request per second) automatically. This prevents API errors and ensures you stay within your plan\u0026rsquo;s limits, whether you\u0026rsquo;re using a free or paid Shodan account.\nTool Integration ArgusScan is designed to work with popular security tools:\nNuclei: Use results to run vulnerability scans OWASP ZAP: Import targets for automated security testing Metasploit: Use discovered services as targets for exploitation The JSON export format makes it easy to integrate with these and other tools.\nInstallation and Setup Installing ArgusScan is straightforward:\n# Clone the repository git clone https://github.com/digenaldo/argusscan.git cd argusscan # Install dependencies pip install -r requirements.txt # Run the tool python argus_scan.py --help You\u0026rsquo;ll need a Shodan API key, which you can get from Shodan.io. Free accounts have some limitations, but they work with ArgusScan for basic searches.\nUsage Examples Here are practical examples of how to use ArgusScan:\nSearch for Jenkins Instances python argus_scan.py \u0026#34;jenkins port:8080\u0026#34; --token YOUR_API_KEY This searches for Jenkins instances running on port 8080. Jenkins is often misconfigured and can be a security risk if exposed publicly.\nThe output shows detailed information about discovered hosts, including IP addresses, ports, organizations, products, and identified vulnerabilities (CVEs). This makes it easy to identify potential targets and understand their security posture.\nSearch Apache Servers in Brazil python argus_scan.py \u0026#34;apache country:BR\u0026#34; --token YOUR_API_KEY This finds Apache web servers located in Brazil. You can use country filters to focus on specific regions.\nExport Results to JSON python argus_scan.py \u0026#34;jenkins\u0026#34; --token YOUR_API_KEY --export json This exports results in JSON format, which you can use with other tools or scripts for further analysis.\nGenerate Professional Report python argus_scan.py \u0026#34;apache\u0026#34; --token YOUR_API_KEY --export markdown --report This generates a professional markdown report following PTES/OWASP standards, suitable for client deliverables.\nTechnical Excellence ArgusScan is built with quality in mind:\nComprehensive Testing:\n25 unit tests covering all major functionality 96% code coverage ensuring reliability Automated testing with GitHub Actions CI/CD pipeline Code Quality:\nPre-commit hooks for quality assurance Modern Python 3.8+ implementation Clean, maintainable code structure Professional Development Practices:\nVersion control with Git Issue tracking and pull request workflow Documentation and examples This level of quality ensures the tool is reliable and suitable for professional use.\nIntegration with Pentest Workflows ArgusScan fits naturally into professional pentest workflows:\nReconnaissance Phase: Use ArgusScan to identify exposed services and potential targets before starting detailed testing.\nTool Integration: Export results to JSON and import them into Nuclei, OWASP ZAP, or Metasploit for automated vulnerability scanning and exploitation.\nReporting: Generate professional reports that can be included in your final pentest deliverables, following PTES and OWASP standards.\nDocumentation: Use Markdown exports to document findings and create detailed reports for clients.\nLegal and Ethical Considerations This is extremely important: ArgusScan should only be used with proper authorization.\nMandatory Requirements:\nWritten authorization (Rules of Engagement - RoE) before any scanning Only scan systems you own or have explicit permission to test Follow responsible disclosure practices Comply with all applicable laws and regulations Authorized Use Cases:\nBug bounty programs with written authorization Professional pentest engagements with signed RoE Academic security research with proper approvals Defensive security assessments of your own infrastructure Red team exercises with proper authorization Unauthorized Use:\nNever scan systems without permission Never use for malicious purposes Never violate terms of service Never break laws or regulations Security tools are powerful, and with that power comes responsibility. Always use ArgusScan ethically and legally.\nUse Cases ArgusScan is useful in several scenarios:\nAuthorized Bug Bounty Programs: When you have written authorization to test specific targets, ArgusScan helps you find exposed services quickly.\nProfessional Pentest Engagements: Use it during the reconnaissance phase to identify potential targets and attack surfaces before detailed testing.\nAcademic Security Research: Researchers can use it to study internet exposure and security trends, always with proper authorization.\nDefensive Security Assessments: Organizations can use it to assess their own infrastructure and identify misconfigurations or exposed services.\nRed Team Exercises: Red teams can use it to find targets within authorized scope during exercises.\nFuture Roadmap The project is actively maintained, and future enhancements may include:\nSupport for additional export formats Integration with more security tools Enhanced filtering options Web interface for easier use Additional report templates Performance improvements Contributions and feedback are welcome on the GitHub repository.\nConclusion ArgusScan is a powerful tool for ethical security professionals who need to automate reconnaissance during authorized pentest engagements. With its professional CLI, multiple export formats, and high code quality, it\u0026rsquo;s designed to make security assessments more efficient while maintaining professional standards.\nThe tool demonstrates that open-source security tools can be both powerful and well-built, with comprehensive testing and professional development practices.\nKey Takeaways:\nAutomates Shodan searches for pentest reconnaissance Professional reports following PTES/OWASP standards 96% test coverage and CI/CD pipeline Integrates with popular security tools Always use with proper authorization If you\u0026rsquo;re a security professional looking to improve your pentest workflow, consider trying ArgusScan. Star the repository on GitHub, try it out (with proper authorization), and contribute improvements if you find it useful.\nRemember: Security tools are powerful, but they must be used responsibly and ethically. Always get written authorization before scanning any systems.\nGet Started:\nGitHub Repository: https://github.com/digenaldo/argusscan Read the documentation Try it with your Shodan API key Contribute and share feedback Use responsibly, and happy ethical hacking!\n","permalink":"https://digenaldo.com/posts/argusscan-ethical-pentest-automation-with-shodan/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eSecurity professionals know that reconnaissance is one of the most important steps in a penetration test. Finding vulnerable systems, understanding network exposure, and identifying potential attack surfaces takes time and requires using multiple tools.\u003c/p\u003e\n\u003cp\u003eWhat if you could automate this process while maintaining professional standards and generating reports that follow industry methodologies like PTES and OWASP?\u003c/p\u003e\n\u003cp\u003eThat\u0026rsquo;s where ArgusScan comes in. It\u0026rsquo;s an open-source Python CLI tool that integrates with Shodan API to automate ethical pentest reconnaissance, making security assessments faster and more efficient.\u003c/p\u003e","title":"ArgusScan: Automating Ethical Pentest with Shodan API"},{"content":"Introduction Observability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\nI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\nThis article focuses on the actual results and what the metrics tell us about the performance differences between these two technologies.\nThe Setup The Monitoring Lab includes two backend applications running the same workload:\nA Go application that performs MongoDB operations every 5 seconds A Spring Boot application performing the same operations at the same interval Prometheus collecting metrics from both applications Grafana dashboards showing real-time performance data Both applications execute similar operations: they ping MongoDB, insert documents, and record metrics about their performance. The key difference is the technology stack each one uses.\nPerformance Metrics Analysis Go Application Performance The Go application shows consistent and efficient performance across all metrics:\nCPU Usage: The Go app maintains very low CPU usage at around 0.22%. This shows the application is lightweight and doesn\u0026rsquo;t require much processing power for these operations.\nMemory Usage: Memory consumption stays stable at approximately 21.6 MiB. This is a small amount of memory, showing that Go applications can be very memory-efficient.\nMongoDB Latency: The latency metrics show excellent performance:\nAverage latency stays between 0.5 ms and 1.2 ms P95 latency (95th percentile) remains consistently around 4.5 ms P99 latency (99th percentile) stays around 5 ms These numbers mean that most operations complete very quickly, and even the slowest operations are still fast.\nOperations Rate: The application maintains a steady rate of 0.3 operations per second, which matches the expected interval of one operation every 5 seconds.\nSpring Boot Application Performance The Spring Boot application shows different characteristics:\nCPU Usage: Similar to Go, CPU usage is low at around 0.28%. Both applications handle the workload without stressing the CPU.\nMemory Usage: This is where we see a significant difference. The Spring Boot application uses 112 MiB of memory, which is more than 5 times the memory used by the Go application. This higher memory usage is typical for Java applications due to the JVM overhead.\nMongoDB Latency: The latency metrics show more variation:\nAverage latency fluctuates between 10 ms and 15 ms P95 latency follows similar patterns to the average P99 latency ranges from 15 ms to 25 ms, with spikes reaching nearly 30 ms The latency is consistently higher than the Go application, which suggests the Spring Boot application takes longer to complete the same operations.\nOperations Rate: The application maintains 0.4 operations per second, slightly higher than the Go application.\nJVM Heap Memory: The dashboard shows how the Java Virtual Machine manages memory:\nG1 Eden Space fills up and gets cleared during garbage collection cycles G1 Old Gen gradually increases over time as objects are promoted G1 Survivor Space remains minimal This memory management pattern is normal for Java applications but adds overhead that Go applications don\u0026rsquo;t have.\nDirect Comparison The comparison dashboard makes the differences clear:\nLatency Comparison:\nGo maintains average latency below 2.5 ms throughout the entire period Spring Boot shows average latency ranging from 6 ms to 15 ms Go\u0026rsquo;s latency is consistently 3 to 6 times lower than Spring Boot\u0026rsquo;s Memory Comparison:\nGo uses between 16 MiB and 24 MiB consistently Spring Boot starts at 112 MiB and fluctuates between 80 MiB and 104 MiB Spring Boot uses approximately 4 to 7 times more memory than Go These results show that for this specific workload, the Go application performs better in terms of both latency and memory usage.\nKey Metrics Explained Understanding these metrics helps you make informed decisions about which technology to use:\nCPU Usage: Shows how much processing power the application needs. Lower is better, as it leaves more resources for other processes.\nMemory Usage: Indicates how much RAM the application consumes. Lower memory usage means you can run more instances on the same server.\nAverage Latency: The typical time it takes for an operation to complete. Lower latency means faster responses to users.\nP95 and P99 Latency: These percentiles show how the slowest operations perform. P95 means 95% of operations are faster than this value. P99 means 99% of operations are faster. These metrics help you understand worst-case performance.\nOperations Rate: How many operations the application completes per second. This helps you understand throughput.\nWhat These Results Mean The metrics show clear patterns:\nGo excels at low latency: The Go application consistently completes operations faster, with latency staying under 2.5 ms.\nGo uses less memory: With memory usage around 21 MiB, Go applications can run more instances on the same hardware.\nSpring Boot has higher overhead: The JVM and framework layers add overhead, resulting in higher latency and memory usage.\nBoth handle the workload: Neither application struggles with the workload, showing that both technologies can handle this type of operation effectively.\nIt\u0026rsquo;s important to remember that these results are specific to this test scenario. Different workloads, different operations, or different configurations might produce different results.\nReal-World Implications These metrics matter in production environments:\nLower latency means users get faster responses, which improves user experience. For applications handling many requests, even small latency differences can add up significantly.\nLower memory usage means you can run more application instances on the same server, which can reduce infrastructure costs. It also means applications start faster and use fewer resources.\nConsistent performance is important for reliability. When latency varies widely, it\u0026rsquo;s harder to predict how the application will behave under load.\nConclusion The Monitoring Lab provides real data about how Go and Spring Boot applications perform. The results show that for this specific workload:\nGo offers better latency performance and lower memory usage Spring Boot provides easier development through frameworks and tools, but with higher resource usage Both technologies can handle the workload effectively The choice between technologies depends on your specific needs. If low latency and low memory usage are priorities, Go might be a better fit. If rapid development and a rich ecosystem are more important, Spring Boot might be the right choice.\nObservability tools like Prometheus and Grafana make it possible to see these differences clearly. Without metrics, you\u0026rsquo;re making decisions based on assumptions. With metrics, you can make decisions based on data.\nThe code and dashboards are available on GitHub: https://github.com/digenaldo/monitoring-lab\n","permalink":"https://digenaldo.com/posts/monitoring-lab-complete-observability-with-go-spring-boot-prometheus/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eObservability is essential for understanding how applications perform in production. When you can see what your systems are doing, you can make better decisions about performance, resource usage, and reliability.\u003c/p\u003e\n\u003cp\u003eI built a Monitoring Lab to compare how Go and Spring Boot applications perform under similar conditions. Both applications connect to MongoDB and perform the same operations, while Prometheus collects metrics and Grafana visualizes the results.\u003c/p\u003e\n\u003cp\u003eThis article focuses on the actual results and what the metrics tell us about the performance differences between these two technologies.\u003c/p\u003e","title":"Monitoring Lab: A Complete Observability Lab with Go, Spring Boot and Prometheus"}]