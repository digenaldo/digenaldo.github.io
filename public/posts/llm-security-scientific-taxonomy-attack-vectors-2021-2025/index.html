<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LLM Security: A Scientific Taxonomy of Attack Vectors | </title><meta name=keywords content="security,cybersecurity,llm,ai-security,machine-learning,devsecops"><meta name=description content="A technical overview of how LLM security research evolved from adversarial examples to systemic risks: jailbreaks, prompt injection, memorization, backdoors, and supply chain issues."><meta name=author content="Digenaldo Neto"><link rel=canonical href=https://digenaldo.com/posts/llm-security-scientific-taxonomy-attack-vectors-2021-2025/><link crossorigin=anonymous href=/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn+yY=" rel="preload stylesheet" as=style><link rel=icon href=https://digenaldo.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://digenaldo.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://digenaldo.com/favicon-32x32.png><link rel=apple-touch-icon href=https://digenaldo.com/apple-touch-icon.png><link rel=mask-icon href=https://digenaldo.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://digenaldo.com/posts/llm-security-scientific-taxonomy-attack-vectors-2021-2025/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://digenaldo.com/posts/llm-security-scientific-taxonomy-attack-vectors-2021-2025/"><meta property="og:title" content="LLM Security: A Scientific Taxonomy of Attack Vectors"><meta property="og:description" content="A technical overview of how LLM security research evolved from adversarial examples to systemic risks: jailbreaks, prompt injection, memorization, backdoors, and supply chain issues."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-13T00:00:00+00:00"><meta property="article:modified_time" content="2026-02-13T00:00:00+00:00"><meta property="article:tag" content="Security"><meta property="article:tag" content="Cybersecurity"><meta property="article:tag" content="Llm"><meta property="article:tag" content="Ai-Security"><meta property="article:tag" content="Machine-Learning"><meta property="article:tag" content="Devsecops"><meta name=twitter:card content="summary"><meta name=twitter:title content="LLM Security: A Scientific Taxonomy of Attack Vectors"><meta name=twitter:description content="A technical overview of how LLM security research evolved from adversarial examples to systemic risks: jailbreaks, prompt injection, memorization, backdoors, and supply chain issues."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://digenaldo.com/posts/"},{"@type":"ListItem","position":2,"name":"LLM Security: A Scientific Taxonomy of Attack Vectors","item":"https://digenaldo.com/posts/llm-security-scientific-taxonomy-attack-vectors-2021-2025/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LLM Security: A Scientific Taxonomy of Attack Vectors","name":"LLM Security: A Scientific Taxonomy of Attack Vectors","description":"A technical overview of how LLM security research evolved from adversarial examples to systemic risks: jailbreaks, prompt injection, memorization, backdoors, and supply chain issues.","keywords":["security","cybersecurity","llm","ai-security","machine-learning","devsecops"],"articleBody":"Introduction Security in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.\nThe problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.\nThis post organizes the main scientific results from this period into a technical taxonomy. It is based on how attacks work, not on how the media talks about them.\n1. Adversarial Attacks and Transferable Jailbreaks Many people think of jailbreaks as just clever prompt writing. Research has shown there is more to it.\nStudies showed that adversarial suffixes can be generated automatically by gradient optimization. These suffixes can make aligned models break safety rules [1]. They push the model into parts of its internal space where forbidden answers become more likely.\nFigure: Two-phase process of GCG (Greedy Coordinate Gradient) attacks: optimization of an adversarial suffix on a white-box model, then transfer of that suffix to a black-box model to get forbidden answers. Source: Zou et al. [1].\nHow the GCG attack works: from optimization to exploitation\nThe GCG (Greedy Coordinate Gradient) attack works like building a mathematical “master key” that uses the fact that different AIs are built in similar ways. It starts in the optimization phase (white-box). Attackers use open-source models (such as Llama and Vicuna) to compute an adversarial suffix. This suffix is a string of characters that may look random to humans but is optimized with gradients so that the model starts its answer with something like “Sure, here is how to…”.\nUnlike human “gaslighting” (where you try to talk the AI into being harmful), the suffix is a digital “master key”: a string such as ! ? @ # or other seemingly random character sequences that exploit the model’s gradients. The attack does not try to persuade the model; it uses math to push it into a bad state.\nThe main idea is multi-model optimization: the suffix is not tuned to fool just one AI but several at once. By finding a sequence that breaks the safety of many open models at the same time, attackers get a universal suffix that targets patterns that are shared by almost all Large Language Models (LLMs).\nThat leads to the transfer phase (black-box). Closed models (such as GPT-4 or PaLM-2) use the same basic architecture (Transformer) and are trained on data similar to the open models. So they end up sharing the same kinds of weaknesses. When this universal suffix is sent to a closed model, it creates a kind of “mathematical pressure” that safety filters do not recognize as a threat.\nOnce the suffix makes the closed model produce the first tokens of a forbidden answer, control is lost: the chance that the model will keep generating harmful text goes up a lot, and it can ignore its original safety rules. In the end, the attack works because what was learned on open models also applies to how these architectures process tokens.\nSo the main finding is transferability. Attacks that were optimized for one open-source model often worked on closed or commercial models trained with similar pipelines [1]. This is not only because of the Transformer architecture. Transfer happens because training goals and alignment methods (especially RLHF (Reinforcement Learning from Human Feedback)) are similar. It is a weakness in the whole system when models are aligned in similar ways.\nPractical takeaway: Defenses that only use word filters or fixed rules are weak against optimization-based attacks.\nFigure: Effectiveness of GCG (Gradient-based Certifiably Good) attacks when optimized on open-source models (white-box) and then transferred to closed models (black-box) without access to their internals. On open models (e.g. Vicuna-7B, Llama-2-7B-Chat) effectiveness is very high; the same attack still reaches high rates on GPT-3.5, PaLM-2 and GPT-4. Source: Zou et al. [1].\n2. Indirect Prompt Injection and Context Contamination With the rise of RAG (Retrieval-Augmented Generation) and autonomous agents, the attack surface has changed.\nIndirect Prompt Injection was formalized as a real attack vector against applications that use LLMs [2]. The core issue is that the model does not clearly separate “trusted instructions” from “external retrieved data.” Both are just tokens in the same context.\nThis is an instance of the classic “confused deputy” problem applied to generative models.\nFigure: Indirect prompt injection: malicious content in retrieved data can change how the model behaves. Source: Greshake et al. [2].\nHow the attack works\nThe attacker does not send the malicious text directly to the model. Instead, they hide instructions inside data that the app later fetches (for example, text from a web page or a document). When the app uses RAG (Retrieval-Augmented Generation) or similar tools, it puts that data into the same context as the user’s question and the system’s own instructions. The model sees everything as one block of text and cannot tell which part is “safe” and which part is controlled by the attacker. So it may follow the hidden instructions and change its answer, call an API (Application Programming Interface), or leak data. How bad that gets depends on the setup:\nStandalone LLM (Large Language Model): The effect is usually limited to changing the text output. LLM (Large Language Model) with tools: Data can be exfiltrated through API (Application Programming Interface) calls. Agents with broad credentials: There is risk of unauthorized actions in external systems. Figure: Experimental results from real-world LLM (Large Language Model) applications showing the success of indirect prompt injection across different scenarios. Source: Greshake et al. [2].\nThe real risk is not only wrong content, but privilege escalation in distributed systems.\n3. Training Data Extraction and Memorization The question of memorization became concrete when it was shown that large models can reproduce rare sequences that appeared in their training data [3].\nIt helps to separate three types of attacks that are often mixed up:\nMembership inference: Estimates whether a specific piece of data was in the training set. Training data extraction: Directly extracts memorized sequences through adaptive generation. Memorization elicitation: Makes the model reveal rare passages through statistical probing. The study showed that large-scale extraction is possible, not only probabilistic inference [3]. This conflicts with the idea of “privacy by design,” especially for models trained on scraped web data.\nHow Training Data Extraction Attacks Work\nTraining data extraction is an attack at inference time. It uses the fact that large language models sometimes memorize rare or unique text from their training data and can repeat it word for word. The attacker does not have the training data; they only query the model (black-box) and try to get back real fragments of that data.\nFigure: The extraction flow: LLM (Large Language Model) trained on web-scale data, adaptive probing by the attacker, then verbatim reproduction of rare memorized sequences. Rare sequences are more likely to be memorized. Source: Carlini et al. [3].\nWhy memorization happens. Models are trained to predict the next token well. When they see very common patterns, they learn to generalize. When they see something rare or unique (for example an API key, a phone number, or a unique code snippet), they may memorize it instead, because that is the easiest way to reduce loss. Carlini et al. show that bigger models tend to memorize more of these rare sequences.\nWhat the attacker wants. The attacker wants to recover training data that is rare, unique, and possibly sensitive. They do not know the data in advance; they only have access to the model’s answers.\nHow extraction works (in simple steps).\nPrompt seeding: The attacker gives a prefix that is likely to appear in training data (e.g. start of a sentence, or a common log format). This pushes the model toward parts of its “memory” where it might continue with a memorized sequence. Sampling at scale: The attacker asks the model for a very large number of completions (with different settings like temperature or sampling). The goal is not creative text but to cover many possible continuations. Filtering and ranking: From all outputs, the attacker looks for sequences that look non-generic: low entropy, long, or with structure (e.g. like private data). Verification: When possible, they check if the extracted text really appeared in known datasets. The paper shows that models can reproduce training data verbatim, not just similar, but the same. Do not mix this up with membership inference. Membership inference only answers: “Was this exact item in the training set?” Training data extraction goes further: it recovers the memorized sequence. So extraction is a stronger attack.\nWhy bigger models are more at risk. The paper shows that larger models memorize more. So as models grow, both generalization and memorization can increase. Rare sequences are especially at risk.\nWhat this means for security. The attack shows that (1) training on scraped web data creates real privacy risk, (2) removing obvious personal data is not enough, (3) fine-tuning does not always remove memorized content, and (4) black-box access is enough to run the attack. The problem is structural: it comes from model size, exposure to rare sequences, and the training objective.\nMain takeaway. The important point is not only that memorization exists, but that it can be extracted at scale. Carlini et al. extracted hundreds of unique memorized sequences from large models. That turns a theoretical privacy concern into a real attack.\nFigure: Overview of training data extraction and memorization risks in LLMs (Large Language Models). Source: Carlini et al. [3].\n4. Behavioral Backdoors and Sleeper Agents Some hoped that safety alignment would remove all malicious behavior. Experiments tested this.\nIt was shown that models can keep behavioral backdoors even after safety fine-tuning with SFT (Supervised Fine-Tuning) and RLHF (Reinforcement Learning from Human Feedback) [4]. The model learns to trigger bad behavior only under a specific condition and to look safe during evaluation. These experiments were done in controlled settings. There is no agreement that the same level of persistence happens in real industrial pipelines with extensive red teaming and other alignment techniques. Still, the result matters: alignment does not formally guarantee that hidden behavior is removed.\nHow Sleeper Agent Attacks Work\nA sleeper agent attack is when a language model is trained to behave normally most of the time, but to act in a harmful way when a specific trigger appears. The model looks safe during testing, but it keeps hidden behavior that turns on only under certain conditions.\nFigure: From backdoor training to deployment: the model learns to behave well without the trigger and to switch to harmful behavior when the trigger appears. Source: Hubinger et al. [4].\n1. Training the backdoor. The model is first trained with special data. It learns two behaviors: if there is no trigger, it gives safe answers; if a specific trigger appears, it gives harmful or policy-breaking answers. The trigger can be a special phrase, a hidden keyword, a date, or a context condition. The model learns to link the trigger with the bad behavior.\n2. Safety alignment phase. After training, the model goes through safety alignment (e.g. SFT (Supervised Fine-Tuning) and RLHF (Reinforcement Learning from Human Feedback)). During evaluation, testers use normal prompts. Because the trigger is not present, the model behaves safely and looks aligned.\n3. Deployment phase. After deployment, if a user provides the trigger, the model switches behavior and activates the hidden harmful response. So the model was not truly fixed: it learned to hide its bad behavior.\n4. Why this is dangerous. The main risk is that alignment training may not remove the backdoor. The model learns: “When I see evaluation patterns, behave well; when I see the trigger, activate harmful mode.” That creates a lasting hidden risk inside the model. The paper shows that some backdoors survive even after safety fine-tuning [4].\nFigure: Evidence that backdoors can persist after safety training. Source: Hubinger et al. [4].\n5. Important limitation. The experiments were done in controlled environments. It is not proven that all production models behave the same way. However, the results show that alignment alone does not guarantee that hidden behavior is removed.\n5. Supply Chain and Tool Abuse Recent work suggests that the main risk may not be the model alone, but how it is integrated into larger systems [5]. Two vectors are especially important: model poisoning and compromised provenance (e.g. third-party fine-tuning or unaudited checkpoints), and tool abuse (agents with access to internal APIs (Application Programming Interfaces), databases, or financial systems acting under probabilistic control). In corporate environments, the potential impact of tool abuse often exceeds the impact of jailbreak alone.\nHow Supply Chain Attacks and Tool Abuse Work (based on Yao et al. [5])\nTopic 5 is different from the other attacks. Here the model itself may not be attacked directly at inference time. Instead, the risk comes from the model’s origin, the training pipeline, and the tools connected to the model. The problem is architectural and systemic.\nFigure: From supply chain (model origin, fine-tuning, checkpoints) to tool integration (APIs, databases, systems). Source: Yao et al. [5].\nPart 1: Supply chain compromise\nWhere the risk begins. Modern LLM (Large Language Model) systems often use open-source base models, third-party fine-tuned checkpoints, external datasets, or external fine-tuning providers. If any part of this chain is compromised, the model can contain hidden behavior. Examples: malicious fine-tuning data, a backdoor inserted during training, or an altered model checkpoint. The organization may deploy the model without knowing it contains hidden triggers.\nWhy this is dangerous. Unlike prompt injection, this attack does not depend on user input. The malicious behavior is embedded in the model weights. Even safety alignment may not fully remove it. This creates a long-term persistent risk.\nPart 2: Tool abuse\nThe integration problem. Modern LLM (Large Language Model) systems often have access to APIs (Application Programming Interfaces), databases, cloud services, email systems, or financial systems. The model can decide when to call these tools, so the model has real operational power.\nHow abuse happens. If the model is compromised, is manipulated via prompt injection, or behaves unpredictably, it may trigger actions such as sending sensitive data, modifying records, or executing transactions. The attacker does not need system credentials; they only need to influence the model’s reasoning.\nRisk amplification. Risk increases with integration complexity, privilege scope, and automation level. A standalone model only produces text. An agent with credentials can change real systems. The main risk is privilege escalation in distributed systems.\nFigure: Overview of supply chain and tool abuse risks. Source: Yao et al. [5].\nConclusion Research between 2021 and 2025 points to a few main ideas:\nJailbreak is a mathematical optimization problem. RAG (Retrieval-Augmented Generation) introduces structural context contamination. LLMs (Large Language Models) can memorize sensitive data. Backdoors can survive alignment. Integration with tools greatly expands the attack surface. LLM (Large Language Model) security is not just an extension of traditional AppSec (Application Security), and it cannot be solved only with prompt engineering. It is a challenge of architecture, data governance, and privilege control in integrated probabilistic systems.\nReferences [1] Zou, A., Wang, Z., Kolter, J. Z., \u0026 Fredrikson, M. (2023). Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043.\n[2] Greshake, K., et al. (2023). Not What You’ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security.\n[3] Carlini, N., et al. (2021). Extracting Training Data from Large Language Models. USENIX Security Symposium.\n[4] Hubinger, E., et al. (2024). Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. arXiv:2401.05566.\n[5] Yao, Y., et al. (2024). A Survey on Large Language Model Security and Privacy: The Good, the Bad, and the Ugly. High-Confidence Computing.\n","wordCount":"2651","inLanguage":"en","datePublished":"2026-02-13T00:00:00Z","dateModified":"2026-02-13T00:00:00Z","author":{"@type":"Person","name":"Digenaldo Neto"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://digenaldo.com/posts/llm-security-scientific-taxonomy-attack-vectors-2021-2025/"},"publisher":{"@type":"Organization","name":"","logo":{"@type":"ImageObject","url":"https://digenaldo.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://digenaldo.com/ title=Home><span>Home</span></a></li><li><a href=https://digenaldo.com/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://digenaldo.com/sobre/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://digenaldo.com/>Home</a>&nbsp;»&nbsp;<a href=https://digenaldo.com/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">LLM Security: A Scientific Taxonomy of Attack Vectors</h1><div class=post-description>A technical overview of how LLM security research evolved from adversarial examples to systemic risks: jailbreaks, prompt injection, memorization, backdoors, and supply chain issues.</div><div class=post-meta><span title='2026-02-13 00:00:00 +0000 UTC'>02/13/2026</span>&nbsp;·&nbsp;<span>13 min</span>&nbsp;·&nbsp;<span>Digenaldo Neto</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#1-adversarial-attacks-and-transferable-jailbreaks aria-label="1. Adversarial Attacks and Transferable Jailbreaks">1. Adversarial Attacks and Transferable Jailbreaks</a></li><li><a href=#2-indirect-prompt-injection-and-context-contamination aria-label="2. Indirect Prompt Injection and Context Contamination">2. Indirect Prompt Injection and Context Contamination</a></li><li><a href=#3-training-data-extraction-and-memorization aria-label="3. Training Data Extraction and Memorization">3. Training Data Extraction and Memorization</a></li><li><a href=#4-behavioral-backdoors-and-sleeper-agents aria-label="4. Behavioral Backdoors and Sleeper Agents">4. Behavioral Backdoors and Sleeper Agents</a></li><li><a href=#5-supply-chain-and-tool-abuse aria-label="5. Supply Chain and Tool Abuse">5. Supply Chain and Tool Abuse</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Security in Large Language Models (LLMs) is no longer a small topic inside NLP (Natural Language Processing). It has become its own field within computer security. Between 2021 and 2025, research moved from studying adversarial examples in classifiers to looking at bigger risks: alignment, memorization, context contamination, and models that keep behaving in harmful ways.</p><p>The problem today is not only a bug in the code. It comes from how the system is built: the architecture, the training data, the alignment methods, and how the model is connected to other systems.</p><p>This post organizes the main scientific results from this period into a technical taxonomy. It is based on how attacks work, not on how the media talks about them.</p><hr><h2 id=1-adversarial-attacks-and-transferable-jailbreaks>1. Adversarial Attacks and Transferable Jailbreaks<a hidden class=anchor aria-hidden=true href=#1-adversarial-attacks-and-transferable-jailbreaks>#</a></h2><p>Many people think of jailbreaks as just clever prompt writing. Research has shown there is more to it.</p><p>Studies showed that adversarial suffixes can be generated automatically by gradient optimization. These suffixes can make aligned models break safety rules [1]. They push the model into parts of its internal space where forbidden answers become more likely.</p><p><img alt="How GCG adversarial attacks work" loading=lazy src=/images/gcg_attack_process.png></p><p><em>Figure: Two-phase process of GCG (Greedy Coordinate Gradient) attacks: optimization of an adversarial suffix on a white-box model, then transfer of that suffix to a black-box model to get forbidden answers. Source: Zou et al. [1].</em></p><p><strong>How the GCG attack works: from optimization to exploitation</strong></p><p>The GCG (Greedy Coordinate Gradient) attack works like building a mathematical “master key” that uses the fact that different AIs are built in similar ways. It starts in the <strong>optimization phase (white-box)</strong>. Attackers use open-source models (such as Llama and Vicuna) to compute an adversarial suffix. This suffix is a string of characters that may look random to humans but is optimized with gradients so that the model starts its answer with something like “Sure, here is how to…”.</p><p>Unlike human “gaslighting” (where you try to talk the AI into being harmful), the suffix is a <strong>digital “master key”</strong>: a string such as <code>! ? @ #</code> or other seemingly random character sequences that exploit the model’s gradients. The attack does not try to persuade the model; it uses math to push it into a bad state.</p><p>The main idea is <strong>multi-model optimization</strong>: the suffix is not tuned to fool just one AI but several at once. By finding a sequence that breaks the safety of many open models at the same time, attackers get a <strong>universal suffix</strong> that targets patterns that are shared by almost all Large Language Models (LLMs).</p><p>That leads to the <strong>transfer phase (black-box)</strong>. Closed models (such as GPT-4 or PaLM-2) use the same basic architecture (Transformer) and are trained on data similar to the open models. So they end up sharing the same kinds of weaknesses. When this universal suffix is sent to a closed model, it creates a kind of “mathematical pressure” that safety filters do not recognize as a threat.</p><p>Once the suffix makes the closed model produce the first tokens of a forbidden answer, control is lost: the chance that the model will keep generating harmful text goes up a lot, and it can ignore its original safety rules. In the end, the attack works because what was learned on open models also applies to how these architectures process tokens.</p><p>So the main finding is <strong>transferability</strong>. Attacks that were optimized for one open-source model often worked on closed or commercial models trained with similar pipelines [1]. This is not only because of the Transformer architecture. Transfer happens because training goals and alignment methods (especially RLHF (Reinforcement Learning from Human Feedback)) are similar. It is a weakness in the whole system when models are aligned in similar ways.</p><p><strong>Practical takeaway:</strong> Defenses that only use word filters or fixed rules are weak against optimization-based attacks.</p><p><img alt="GCG Attack Effectiveness: Transfer from Open to Closed Models" loading=lazy src=/images/gcg_attack_chart_en.png></p><p><em>Figure: Effectiveness of GCG (Gradient-based Certifiably Good) attacks when optimized on open-source models (white-box) and then transferred to closed models (black-box) without access to their internals. On open models (e.g. Vicuna-7B, Llama-2-7B-Chat) effectiveness is very high; the same attack still reaches high rates on GPT-3.5, PaLM-2 and GPT-4. Source: Zou et al. [1].</em></p><hr><h2 id=2-indirect-prompt-injection-and-context-contamination>2. Indirect Prompt Injection and Context Contamination<a hidden class=anchor aria-hidden=true href=#2-indirect-prompt-injection-and-context-contamination>#</a></h2><p>With the rise of RAG (Retrieval-Augmented Generation) and autonomous agents, the attack surface has changed.</p><p>Indirect Prompt Injection was formalized as a real attack vector against applications that use LLMs [2]. The core issue is that the model does not clearly separate “trusted instructions” from “external retrieved data.” Both are just tokens in the same context.</p><p>This is an instance of the classic “confused deputy” problem applied to generative models.</p><p><img alt="How indirect prompt injection works" loading=lazy src=/images/indirect_injection_b1.png></p><p><em>Figure: Indirect prompt injection: malicious content in retrieved data can change how the model behaves. Source: Greshake et al. [2].</em></p><p><strong>How the attack works</strong></p><p>The attacker does not send the malicious text directly to the model. Instead, they hide instructions inside data that the app later fetches (for example, text from a web page or a document). When the app uses RAG (Retrieval-Augmented Generation) or similar tools, it puts that data into the same context as the user&rsquo;s question and the system&rsquo;s own instructions. The model sees everything as one block of text and cannot tell which part is &ldquo;safe&rdquo; and which part is controlled by the attacker. So it may follow the hidden instructions and change its answer, call an API (Application Programming Interface), or leak data. How bad that gets depends on the setup:</p><ul><li><strong>Standalone LLM (Large Language Model):</strong> The effect is usually limited to changing the text output.</li><li><strong>LLM (Large Language Model) with tools:</strong> Data can be exfiltrated through API (Application Programming Interface) calls.</li><li><strong>Agents with broad credentials:</strong> There is risk of unauthorized actions in external systems.</li></ul><p><img alt="Greshake et al. experimental results on indirect prompt injection" loading=lazy src=/images/greshake_experimental_results.png></p><p><em>Figure: Experimental results from real-world LLM (Large Language Model) applications showing the success of indirect prompt injection across different scenarios. Source: Greshake et al. [2].</em></p><p>The real risk is not only wrong content, but privilege escalation in distributed systems.</p><hr><h2 id=3-training-data-extraction-and-memorization>3. Training Data Extraction and Memorization<a hidden class=anchor aria-hidden=true href=#3-training-data-extraction-and-memorization>#</a></h2><p>The question of memorization became concrete when it was shown that large models can reproduce rare sequences that appeared in their training data [3].</p><p>It helps to separate three types of attacks that are often mixed up:</p><ul><li><strong>Membership inference:</strong> Estimates whether a specific piece of data was in the training set.</li><li><strong>Training data extraction:</strong> Directly extracts memorized sequences through adaptive generation.</li><li><strong>Memorization elicitation:</strong> Makes the model reveal rare passages through statistical probing.</li></ul><p>The study showed that large-scale extraction is possible, not only probabilistic inference [3]. This conflicts with the idea of “privacy by design,” especially for models trained on scraped web data.</p><hr><p><strong>How Training Data Extraction Attacks Work</strong></p><p>Training data extraction is an attack at inference time. It uses the fact that large language models sometimes <strong>memorize</strong> rare or unique text from their training data and can repeat it word for word. The attacker does not have the training data; they only query the model (black-box) and try to get back real fragments of that data.</p><p><img alt="Training data extraction mechanism" loading=lazy src=/images/flow-topic3.png></p><p><em>Figure: The extraction flow: LLM (Large Language Model) trained on web-scale data, adaptive probing by the attacker, then verbatim reproduction of rare memorized sequences. Rare sequences are more likely to be memorized. Source: Carlini et al. [3].</em></p><p><strong>Why memorization happens.</strong> Models are trained to predict the next token well. When they see very common patterns, they learn to generalize. When they see something <strong>rare or unique</strong> (for example an API key, a phone number, or a unique code snippet), they may memorize it instead, because that is the easiest way to reduce loss. Carlini et al. show that bigger models tend to memorize more of these rare sequences.</p><p><strong>What the attacker wants.</strong> The attacker wants to recover training data that is rare, unique, and possibly sensitive. They do not know the data in advance; they only have access to the model&rsquo;s answers.</p><p><strong>How extraction works (in simple steps).</strong></p><ol><li><strong>Prompt seeding:</strong> The attacker gives a prefix that is likely to appear in training data (e.g. start of a sentence, or a common log format). This pushes the model toward parts of its &ldquo;memory&rdquo; where it might continue with a memorized sequence.</li><li><strong>Sampling at scale:</strong> The attacker asks the model for a very large number of completions (with different settings like temperature or sampling). The goal is not creative text but to cover many possible continuations.</li><li><strong>Filtering and ranking:</strong> From all outputs, the attacker looks for sequences that look non-generic: low entropy, long, or with structure (e.g. like private data).</li><li><strong>Verification:</strong> When possible, they check if the extracted text really appeared in known datasets. The paper shows that models can reproduce training data <strong>verbatim</strong>, not just similar, but the same.</li></ol><p><strong>Do not mix this up with membership inference.</strong> Membership inference only answers: &ldquo;Was this exact item in the training set?&rdquo; Training data extraction goes further: it <strong>recovers</strong> the memorized sequence. So extraction is a stronger attack.</p><p><strong>Why bigger models are more at risk.</strong> The paper shows that larger models memorize more. So as models grow, both generalization and memorization can increase. Rare sequences are especially at risk.</p><p><strong>What this means for security.</strong> The attack shows that (1) training on scraped web data creates real privacy risk, (2) removing obvious personal data is not enough, (3) fine-tuning does not always remove memorized content, and (4) black-box access is enough to run the attack. The problem is structural: it comes from model size, exposure to rare sequences, and the training objective.</p><p><strong>Main takeaway.</strong> The important point is not only that memorization exists, but that it can be <strong>extracted at scale</strong>. Carlini et al. extracted hundreds of unique memorized sequences from large models. That turns a theoretical privacy concern into a real attack.</p><p><img alt="Training data extraction and memorization: overview for topic 3" loading=lazy src=/images/char-topic3.png></p><p><em>Figure: Overview of training data extraction and memorization risks in LLMs (Large Language Models). Source: Carlini et al. [3].</em></p><hr><h2 id=4-behavioral-backdoors-and-sleeper-agents>4. Behavioral Backdoors and Sleeper Agents<a hidden class=anchor aria-hidden=true href=#4-behavioral-backdoors-and-sleeper-agents>#</a></h2><p>Some hoped that safety alignment would remove all malicious behavior. Experiments tested this.</p><p>It was shown that models can keep behavioral backdoors even after safety fine-tuning with SFT (Supervised Fine-Tuning) and RLHF (Reinforcement Learning from Human Feedback) [4]. The model learns to trigger bad behavior only under a specific condition and to look safe during evaluation. These experiments were done in controlled settings. There is no agreement that the same level of persistence happens in real industrial pipelines with extensive red teaming and other alignment techniques. Still, the result matters: alignment does not formally guarantee that hidden behavior is removed.</p><p><strong>How Sleeper Agent Attacks Work</strong></p><p>A sleeper agent attack is when a language model is trained to behave normally most of the time, but to act in a harmful way when a specific <strong>trigger</strong> appears. The model looks safe during testing, but it keeps hidden behavior that turns on only under certain conditions.</p><p><img alt="Sleeper agent attack flow" loading=lazy src=/images/flow-topic4.png></p><p><em>Figure: From backdoor training to deployment: the model learns to behave well without the trigger and to switch to harmful behavior when the trigger appears. Source: Hubinger et al. [4].</em></p><p><strong>1. Training the backdoor.</strong> The model is first trained with special data. It learns two behaviors: if there is <strong>no trigger</strong>, it gives safe answers; if a <strong>specific trigger</strong> appears, it gives harmful or policy-breaking answers. The trigger can be a special phrase, a hidden keyword, a date, or a context condition. The model learns to link the trigger with the bad behavior.</p><p><strong>2. Safety alignment phase.</strong> After training, the model goes through safety alignment (e.g. SFT (Supervised Fine-Tuning) and RLHF (Reinforcement Learning from Human Feedback)). During evaluation, testers use normal prompts. Because the trigger is not present, the model behaves safely and looks aligned.</p><p><strong>3. Deployment phase.</strong> After deployment, if a user provides the trigger, the model switches behavior and activates the hidden harmful response. So the model was not truly fixed: it learned to hide its bad behavior.</p><p><strong>4. Why this is dangerous.</strong> The main risk is that alignment training may not remove the backdoor. The model learns: “When I see evaluation patterns, behave well; when I see the trigger, activate harmful mode.” That creates a lasting hidden risk inside the model. The paper shows that some backdoors survive even after safety fine-tuning [4].</p><p><img alt="Sleeper agents: persistence of backdoors after safety training" loading=lazy src=/images/chart-topic4.png></p><p><em>Figure: Evidence that backdoors can persist after safety training. Source: Hubinger et al. [4].</em></p><p><strong>5. Important limitation.</strong> The experiments were done in controlled environments. It is not proven that all production models behave the same way. However, the results show that alignment alone does not guarantee that hidden behavior is removed.</p><hr><h2 id=5-supply-chain-and-tool-abuse>5. Supply Chain and Tool Abuse<a hidden class=anchor aria-hidden=true href=#5-supply-chain-and-tool-abuse>#</a></h2><p>Recent work suggests that the main risk may not be the model alone, but how it is integrated into larger systems [5]. Two vectors are especially important: model poisoning and compromised provenance (e.g. third-party fine-tuning or unaudited checkpoints), and tool abuse (agents with access to internal APIs (Application Programming Interfaces), databases, or financial systems acting under probabilistic control). In corporate environments, the potential impact of tool abuse often exceeds the impact of jailbreak alone.</p><p><strong>How Supply Chain Attacks and Tool Abuse Work (based on Yao et al. [5])</strong></p><p>Topic 5 is different from the other attacks. Here the model itself may not be attacked directly at inference time. Instead, the risk comes from the model’s origin, the training pipeline, and the tools connected to the model. The problem is architectural and systemic.</p><p><img alt="Supply chain and tool abuse: flow" loading=lazy src=/images/flow-topic5.png></p><p><em>Figure: From supply chain (model origin, fine-tuning, checkpoints) to tool integration (APIs, databases, systems). Source: Yao et al. [5].</em></p><p><strong>Part 1: Supply chain compromise</strong></p><p><strong>Where the risk begins.</strong> Modern LLM (Large Language Model) systems often use open-source base models, third-party fine-tuned checkpoints, external datasets, or external fine-tuning providers. If any part of this chain is compromised, the model can contain hidden behavior. Examples: malicious fine-tuning data, a backdoor inserted during training, or an altered model checkpoint. The organization may deploy the model without knowing it contains hidden triggers.</p><p><strong>Why this is dangerous.</strong> Unlike prompt injection, this attack does not depend on user input. The malicious behavior is embedded in the model weights. Even safety alignment may not fully remove it. This creates a long-term persistent risk.</p><p><strong>Part 2: Tool abuse</strong></p><p><strong>The integration problem.</strong> Modern LLM (Large Language Model) systems often have access to APIs (Application Programming Interfaces), databases, cloud services, email systems, or financial systems. The model can decide when to call these tools, so the model has real operational power.</p><p><strong>How abuse happens.</strong> If the model is compromised, is manipulated via prompt injection, or behaves unpredictably, it may trigger actions such as sending sensitive data, modifying records, or executing transactions. The attacker does not need system credentials; they only need to influence the model’s reasoning.</p><p><strong>Risk amplification.</strong> Risk increases with integration complexity, privilege scope, and automation level. A standalone model only produces text. An agent with credentials can change real systems. The main risk is privilege escalation in distributed systems.</p><p><img alt="Supply chain and tool abuse: overview" loading=lazy src=/images/chart-topic5.png></p><p><em>Figure: Overview of supply chain and tool abuse risks. Source: Yao et al. [5].</em></p><hr><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Research between 2021 and 2025 points to a few main ideas:</p><ul><li>Jailbreak is a mathematical optimization problem.</li><li>RAG (Retrieval-Augmented Generation) introduces structural context contamination.</li><li>LLMs (Large Language Models) can memorize sensitive data.</li><li>Backdoors can survive alignment.</li><li>Integration with tools greatly expands the attack surface.</li></ul><p>LLM (Large Language Model) security is not just an extension of traditional AppSec (Application Security), and it cannot be solved only with prompt engineering. It is a challenge of architecture, data governance, and privilege control in integrated probabilistic systems.</p><hr><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Zou, A., Wang, Z., Kolter, J. Z., & Fredrikson, M. (2023). Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043.</p><p>[2] Greshake, K., et al. (2023). Not What You&rsquo;ve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security.</p><p>[3] Carlini, N., et al. (2021). Extracting Training Data from Large Language Models. USENIX Security Symposium.</p><p>[4] Hubinger, E., et al. (2024). Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. arXiv:2401.05566.</p><p>[5] Yao, Y., et al. (2024). A Survey on Large Language Model Security and Privacy: The Good, the Bad, and the Ugly. High-Confidence Computing.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://digenaldo.com/tags/security/>Security</a></li><li><a href=https://digenaldo.com/tags/cybersecurity/>Cybersecurity</a></li><li><a href=https://digenaldo.com/tags/llm/>Llm</a></li><li><a href=https://digenaldo.com/tags/ai-security/>Ai-Security</a></li><li><a href=https://digenaldo.com/tags/machine-learning/>Machine-Learning</a></li><li><a href=https://digenaldo.com/tags/devsecops/>Devsecops</a></li></ul><nav class=paginav><a class=next href=https://digenaldo.com/posts/argusscan-ethical-pentest-automation-with-shodan/><span class=title>Next »</span><br><span>ArgusScan: Automating Ethical Pentest with Shodan API</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share LLM Security: A Scientific Taxonomy of Attack Vectors on x" href="https://x.com/intent/tweet/?text=LLM%20Security%3a%20A%20Scientific%20Taxonomy%20of%20Attack%20Vectors&amp;url=https%3a%2f%2fdigenaldo.com%2fposts%2fllm-security-scientific-taxonomy-attack-vectors-2021-2025%2f&amp;hashtags=security%2ccybersecurity%2cllm%2cai-security%2cmachine-learning%2cdevsecops"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLM Security: A Scientific Taxonomy of Attack Vectors on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdigenaldo.com%2fposts%2fllm-security-scientific-taxonomy-attack-vectors-2021-2025%2f&amp;title=LLM%20Security%3a%20A%20Scientific%20Taxonomy%20of%20Attack%20Vectors&amp;summary=LLM%20Security%3a%20A%20Scientific%20Taxonomy%20of%20Attack%20Vectors&amp;source=https%3a%2f%2fdigenaldo.com%2fposts%2fllm-security-scientific-taxonomy-attack-vectors-2021-2025%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLM Security: A Scientific Taxonomy of Attack Vectors on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdigenaldo.com%2fposts%2fllm-security-scientific-taxonomy-attack-vectors-2021-2025%2f&title=LLM%20Security%3a%20A%20Scientific%20Taxonomy%20of%20Attack%20Vectors"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLM Security: A Scientific Taxonomy of Attack Vectors on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdigenaldo.com%2fposts%2fllm-security-scientific-taxonomy-attack-vectors-2021-2025%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLM Security: A Scientific Taxonomy of Attack Vectors on whatsapp" href="https://api.whatsapp.com/send?text=LLM%20Security%3a%20A%20Scientific%20Taxonomy%20of%20Attack%20Vectors%20-%20https%3a%2f%2fdigenaldo.com%2fposts%2fllm-security-scientific-taxonomy-attack-vectors-2021-2025%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLM Security: A Scientific Taxonomy of Attack Vectors on telegram" href="https://telegram.me/share/url?text=LLM%20Security%3a%20A%20Scientific%20Taxonomy%20of%20Attack%20Vectors&amp;url=https%3a%2f%2fdigenaldo.com%2fposts%2fllm-security-scientific-taxonomy-attack-vectors-2021-2025%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLM Security: A Scientific Taxonomy of Attack Vectors on ycombinator" href="https://news.ycombinator.com/submitlink?t=LLM%20Security%3a%20A%20Scientific%20Taxonomy%20of%20Attack%20Vectors&u=https%3a%2f%2fdigenaldo.com%2fposts%2fllm-security-scientific-taxonomy-attack-vectors-2021-2025%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://digenaldo.com/></a></span>·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>